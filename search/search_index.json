{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Index","text":""},{"location":"#skorecard","title":"skorecard","text":"<p><code>skorecard</code> is a scikit-learn compatible python package that helps streamline the development of credit risk acceptance models (scorecards).</p> <p>Scorecards are \u2018traditional\u2019 models used by banks in the credit decision process. Internally, scorecards are Logistic Regression models that make use of features that are binned into different groups. The process of binning is usually done manually by experts, and <code>skorecard</code> provides tools to makes this process easier. <code>skorecard</code> is built on top of scikit-learn as well as other excellent open source projects like optbinning, dash and plotly.</p>"},{"location":"#features","title":"Features \u2b50","text":"<ul> <li>Automate bucketing of features inside scikit-learn pipelines.</li> <li>Dash webapp to help manually tweak bucketing of features with business knowledge</li> <li>Extension to <code>sklearn.linear_model.LogisticRegression</code> that is also able to report p-values</li> <li>Plots and reports to speed up analysis and writing technical documentation.</li> </ul>"},{"location":"#quick-demo","title":"Quick demo","text":"<p><code>skorecard</code> offers a range of bucketers:</p> <pre><code>import pandas as pd\nfrom skorecard.bucketers import EqualWidthBucketer\n\ndf = pd.DataFrame({'column' : range(100)})\n\newb = EqualWidthBucketer(n_bins=5)\newb.fit_transform(df)\n\newb.bucket_table('column')\n#&gt;    bucket                       label  Count  Count (%)\n#&gt; 0      -1                     Missing    0.0        0.0\n#&gt; 1       0                (-inf, 19.8]   20.0       20.0\n#&gt; 2       1                (19.8, 39.6]   20.0       20.0\n#&gt; 3       2  (39.6, 59.400000000000006]   20.0       20.0\n#&gt; 4       3  (59.400000000000006, 79.2]   20.0       20.0\n#&gt; 5       4                 (79.2, inf]   20.0       20.0\n</code></pre> <p>That also support a dash app to explore and update bucket boundaries:</p> <pre><code>ewb.fit_interactive(df)\n#&gt; Dash app running on http://127.0.0.1:8050/\n</code></pre> <p></p>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip3 install skorecard\n</code></pre>"},{"location":"#documentation","title":"Documentation","text":"<p>See ing-bank.github.io/skorecard/.</p>"},{"location":"#presentations","title":"Presentations","text":"Title Host Date Speaker(s) Skorecard: Making logistic regressions great again ING Data Science Meetup 10 June 2021 Daniel Timbrell, Sandro Bjelogrlic, Tim Vink"},{"location":"contributing/","title":"Contributing guidelines","text":"<p>Make sure to discuss any changes you would like to make in the issue board, before putting in any work.</p>"},{"location":"contributing/#setup","title":"Setup","text":"<p>Development install:</p> <pre><code>pip install -e '.[all]'\n</code></pre> <p>Unit testing:</p> <pre><code>pytest\n</code></pre> <p>We use pre-commit hooks to ensure code styling. Install with:</p> <pre><code>pre-commit install\n</code></pre> <p>Now if you install it (which you are encouraged to do), you are encouraged to do the following command before committing your work:</p> <pre><code>pre-commit run --all-files\n</code></pre> <p>This will allow you to quickly see if the work you made contains some adaptions that you still might need to make before a pull request is accepted.</p>"},{"location":"contributing/#documentation","title":"Documentation","text":"<p>We use mkdocs with mkdocs-material theme. The docs are structured using the divio documentation system. To view the docs locally:</p> <pre><code>pip install mkdocs-material\nmkdocs serve\n</code></pre>"},{"location":"contributing/#releases-and-versioning","title":"Releases and versioning","text":"<p>We use semver for versioning. When we are ready for a release, the maintainer runs:</p> <pre><code>git tag -a v0.1 -m \"skorecard v0.1\" &amp;&amp; git push origin v0.1\n</code></pre> <p>When we create a new github release a github action is triggered that:</p> <ul> <li>a new version will be deployed to pypi</li> <li>the docs will be re-built and deployed</li> </ul>"},{"location":"contributing/#logo","title":"Logo","text":"<ul> <li>We adapted the 'scores' noun</li> <li>We used this color scheme from coolors.co</li> <li>We edited the logo using https://boxy-svg.com/app</li> </ul>"},{"location":"contributing/#terminology","title":"Terminology","text":"<ul> <li><code>BucketMapping</code> is a custom class that stores all the information needed for bucketing, including the map itself (either boundaries for binning, or a list of lists for categoricals)</li> <li><code>FeaturesBucketMapping</code> is simply a collection of <code>BucketMapping</code>s, and is used to store all info for bucketing transformations for a dataset.</li> </ul>"},{"location":"api/bucketers/AgglomerativeClusteringBucketer/","title":"AgglomerativeClusteringBucketer","text":"<p>               Bases: <code>BaseBucketer</code></p> <p>The <code>AgglomerativeClusteringBucketer</code> transformer creates buckets using sklearn.AgglomerativeClustering.</p> <p>Support  </p> <p>Example:</p> <pre><code>from skorecard import datasets\nfrom skorecard.bucketers import AgglomerativeClusteringBucketer\n\nspecials = {\"LIMIT_BAL\": {\"=50000\": [50000], \"in [20001,30000]\": [20000, 30000]}}\n\nX, y = datasets.load_uci_credit_card(return_X_y=True)\nbucketer = AgglomerativeClusteringBucketer(n_bins = 10, variables=['LIMIT_BAL'], specials=specials)\nbucketer.fit_transform(X)\nbucketer.fit_transform(X)['LIMIT_BAL'].value_counts()\n</code></pre> Source code in <code>skorecard/bucketers/bucketers.py</code> <pre><code>class AgglomerativeClusteringBucketer(BaseBucketer):\n    \"\"\"\n    The `AgglomerativeClusteringBucketer` transformer creates buckets using [sklearn.AgglomerativeClustering](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html).\n\n    Support ![badge](https://img.shields.io/badge/numerical-true-green) ![badge](https://img.shields.io/badge/categorical-false-red) ![badge](https://img.shields.io/badge/supervised-false-red)\n\n    Example:\n\n    ```python\n    from skorecard import datasets\n    from skorecard.bucketers import AgglomerativeClusteringBucketer\n\n    specials = {\"LIMIT_BAL\": {\"=50000\": [50000], \"in [20001,30000]\": [20000, 30000]}}\n\n    X, y = datasets.load_uci_credit_card(return_X_y=True)\n    bucketer = AgglomerativeClusteringBucketer(n_bins = 10, variables=['LIMIT_BAL'], specials=specials)\n    bucketer.fit_transform(X)\n    bucketer.fit_transform(X)['LIMIT_BAL'].value_counts()\n    ```\n    \"\"\"  # noqa\n\n    def __init__(\n        self,\n        n_bins=5,\n        variables=[],\n        specials={},\n        missing_treatment=\"separate\",\n        remainder=\"passthrough\",\n        get_statistics=True,\n        **kwargs,\n    ):\n        \"\"\"Init the class.\n\n        Args:\n            n_bins (int): Number of bins to create.\n            variables (list): The features to bucket. Uses all features if not defined.\n            specials: (dict) of special values that require their own binning.\n                The dictionary has the following format:\n                 {\"&lt;column name&gt;\" : {\"name of special bucket\" : &lt;list with 1 or more values&gt;}}\n                For every feature that needs a special value, a dictionary must be passed as value.\n                This dictionary contains a name of a bucket (key) and an array of unique values that should be put\n                in that bucket.\n                When special values are defined, they are not considered in the fitting procedure.\n            missing_treatment: Defines how we treat the missing values present in the data.\n                If a string, it must be one of the following options:\n                    separate: Missing values get put in a separate 'Other' bucket: `-1`\n                    most_risky: Missing values are put into the bucket containing the largest percentage of Class 1.\n                    least_risky: Missing values are put into the bucket containing the largest percentage of Class 0.\n                    most_frequent: Missing values are put into the most common bucket.\n                    neutral: Missing values are put into the bucket with WoE closest to 0.\n                    similar: Missing values are put into the bucket with WoE closest to the bucket with only missing values.\n                    passthrough: Leaves missing values untouched.\n                If a dict, it must be of the following format:\n                    {\"&lt;column name&gt;\": &lt;bucket_number&gt;}\n                    This bucket number is where we will put the missing values.\n            remainder: How we want the non-specified columns to be transformed. It must be in [\"passthrough\", \"drop\"].\n                passthrough (Default): all columns that were not specified in \"variables\" will be passed through.\n                drop: all remaining columns that were not specified in \"variables\" will be dropped.\n            kwargs: Other parameters passed to AgglomerativeBucketer\n        \"\"\"  # noqa\n        self.variables = variables\n        self.n_bins = n_bins\n        self.specials = specials\n        self.missing_treatment = missing_treatment\n        self.remainder = remainder\n        self.get_statistics = get_statistics\n        self.kwargs = kwargs\n\n    @property\n    def variables_type(self):\n        \"\"\"\n        Signals variables type supported by this bucketer.\n        \"\"\"\n        return \"numerical\"\n\n    def _get_feature_splits(self, feature, X, y, X_unfiltered=None):\n        \"\"\"\n        Finds the splits for a single feature.\n\n        X and y have already been preprocessed, and have specials removed.\n\n        Args:\n            feature (str): Name of the feature.\n            X (pd.Series): df with single column of feature to bucket\n            y (np.ndarray): array with target\n            X_unfiltered (pd.Series): df with single column of feature to bucket before any filtering was applied\n\n        Returns:\n            splits, right (tuple): The splits (dict or array), and whether right=True or False.\n        \"\"\"\n        # Fit the estimator\n        ab = AgglomerativeClustering(n_clusters=self.n_bins, **self.kwargs)\n        ab.fit(X.values.reshape(-1, 1), y=None)\n\n        # Find the boundaries\n        df = pd.DataFrame({\"x\": X.values, \"label\": ab.labels_}).sort_values(by=\"x\")\n        cluster_minimum_values = df.groupby(\"label\")[\"x\"].min().sort_values().tolist()\n        cluster_maximum_values = df.groupby(\"label\")[\"x\"].max().sort_values().tolist()\n        # take the mean of the upper boundary of a cluster and the lower boundary of the next cluster\n        boundaries = [\n            # Assures numbers are float and not np.float - necessary for serialization\n            float(np.mean([cluster_minimum_values[i + 1], cluster_maximum_values[i]]))\n            for i in range(len(cluster_minimum_values) - 1)\n        ]\n\n        if isinstance(boundaries, np.ndarray):\n            boundaries = boundaries.tolist()\n\n        return (boundaries, True)\n</code></pre>"},{"location":"api/bucketers/AgglomerativeClusteringBucketer/#skorecard.bucketers.AgglomerativeClusteringBucketer.variables_type","title":"<code>variables_type</code>  <code>property</code>","text":"<p>Signals variables type supported by this bucketer.</p>"},{"location":"api/bucketers/AgglomerativeClusteringBucketer/#skorecard.bucketers.AgglomerativeClusteringBucketer.__init__","title":"<code>__init__(n_bins=5, variables=[], specials={}, missing_treatment='separate', remainder='passthrough', get_statistics=True, **kwargs)</code>","text":"<p>Init the class.</p> <p>Parameters:</p> Name Type Description Default <code>n_bins</code> <code>int</code> <p>Number of bins to create.</p> <code>5</code> <code>variables</code> <code>list</code> <p>The features to bucket. Uses all features if not defined.</p> <code>[]</code> <code>specials</code> <p>(dict) of special values that require their own binning. The dictionary has the following format:  {\"\" : {\"name of special bucket\" : }} For every feature that needs a special value, a dictionary must be passed as value. This dictionary contains a name of a bucket (key) and an array of unique values that should be put in that bucket. When special values are defined, they are not considered in the fitting procedure. <code>{}</code> <code>missing_treatment</code> <p>Defines how we treat the missing values present in the data. If a string, it must be one of the following options:     separate: Missing values get put in a separate 'Other' bucket: <code>-1</code>     most_risky: Missing values are put into the bucket containing the largest percentage of Class 1.     least_risky: Missing values are put into the bucket containing the largest percentage of Class 0.     most_frequent: Missing values are put into the most common bucket.     neutral: Missing values are put into the bucket with WoE closest to 0.     similar: Missing values are put into the bucket with WoE closest to the bucket with only missing values.     passthrough: Leaves missing values untouched. If a dict, it must be of the following format:     {\"\": }     This bucket number is where we will put the missing values. <code>'separate'</code> <code>remainder</code> <p>How we want the non-specified columns to be transformed. It must be in [\"passthrough\", \"drop\"]. passthrough (Default): all columns that were not specified in \"variables\" will be passed through. drop: all remaining columns that were not specified in \"variables\" will be dropped.</p> <code>'passthrough'</code> <code>kwargs</code> <p>Other parameters passed to AgglomerativeBucketer</p> <code>{}</code> Source code in <code>skorecard/bucketers/bucketers.py</code> <pre><code>def __init__(\n    self,\n    n_bins=5,\n    variables=[],\n    specials={},\n    missing_treatment=\"separate\",\n    remainder=\"passthrough\",\n    get_statistics=True,\n    **kwargs,\n):\n    \"\"\"Init the class.\n\n    Args:\n        n_bins (int): Number of bins to create.\n        variables (list): The features to bucket. Uses all features if not defined.\n        specials: (dict) of special values that require their own binning.\n            The dictionary has the following format:\n             {\"&lt;column name&gt;\" : {\"name of special bucket\" : &lt;list with 1 or more values&gt;}}\n            For every feature that needs a special value, a dictionary must be passed as value.\n            This dictionary contains a name of a bucket (key) and an array of unique values that should be put\n            in that bucket.\n            When special values are defined, they are not considered in the fitting procedure.\n        missing_treatment: Defines how we treat the missing values present in the data.\n            If a string, it must be one of the following options:\n                separate: Missing values get put in a separate 'Other' bucket: `-1`\n                most_risky: Missing values are put into the bucket containing the largest percentage of Class 1.\n                least_risky: Missing values are put into the bucket containing the largest percentage of Class 0.\n                most_frequent: Missing values are put into the most common bucket.\n                neutral: Missing values are put into the bucket with WoE closest to 0.\n                similar: Missing values are put into the bucket with WoE closest to the bucket with only missing values.\n                passthrough: Leaves missing values untouched.\n            If a dict, it must be of the following format:\n                {\"&lt;column name&gt;\": &lt;bucket_number&gt;}\n                This bucket number is where we will put the missing values.\n        remainder: How we want the non-specified columns to be transformed. It must be in [\"passthrough\", \"drop\"].\n            passthrough (Default): all columns that were not specified in \"variables\" will be passed through.\n            drop: all remaining columns that were not specified in \"variables\" will be dropped.\n        kwargs: Other parameters passed to AgglomerativeBucketer\n    \"\"\"  # noqa\n    self.variables = variables\n    self.n_bins = n_bins\n    self.specials = specials\n    self.missing_treatment = missing_treatment\n    self.remainder = remainder\n    self.get_statistics = get_statistics\n    self.kwargs = kwargs\n</code></pre>"},{"location":"api/bucketers/AsIsCategoricalBucketer/","title":"AsIsCategoricalBucketer","text":"<p>               Bases: <code>BaseBucketer</code></p> <p>The <code>AsIsCategoricalBucketer</code> treats unique values as categories.</p> <p>Support:  </p> <p>It will assign each a bucket number in the order of appearance. If new data contains new, unknown labels they will be replaced by 'Other'.</p> <p>This is bucketer is useful when you have data that is already sufficiented bucketed, but you would like to be able to bucket new data in the same way.</p> <p>Example:</p> <pre><code>from skorecard import datasets\nfrom skorecard.bucketers import AsIsCategoricalBucketer\n\nX, y = datasets.load_uci_credit_card(return_X_y=True)\nbucketer = AsIsCategoricalBucketer(variables=['EDUCATION'])\nbucketer.fit_transform(X)\n</code></pre> Source code in <code>skorecard/bucketers/bucketers.py</code> <pre><code>class AsIsCategoricalBucketer(BaseBucketer):\n    \"\"\"\n    The `AsIsCategoricalBucketer` treats unique values as categories.\n\n    Support: ![badge](https://img.shields.io/badge/numerical-false-red) ![badge](https://img.shields.io/badge/categorical-true-green) ![badge](https://img.shields.io/badge/supervised-false-blue)\n\n    It will assign each a bucket number in the order of appearance.\n    If new data contains new, unknown labels they will be replaced by 'Other'.\n\n    This is bucketer is useful when you have data that is already sufficiented bucketed,\n    but you would like to be able to bucket new data in the same way.\n\n    Example:\n\n    ```python\n    from skorecard import datasets\n    from skorecard.bucketers import AsIsCategoricalBucketer\n\n    X, y = datasets.load_uci_credit_card(return_X_y=True)\n    bucketer = AsIsCategoricalBucketer(variables=['EDUCATION'])\n    bucketer.fit_transform(X)\n    ```\n    \"\"\"  # noqa\n\n    def __init__(\n        self, variables=[], specials={}, missing_treatment=\"separate\", remainder=\"passthrough\", get_statistics=True\n    ):\n        \"\"\"Init the class.\n\n        Args:\n            variables (list): The features to bucket. Uses all features if not defined.\n            specials: (nested) dictionary of special values that require their own binning.\n                The dictionary has the following format:\n                 {\"&lt;column name&gt;\" : {\"name of special bucket\" : &lt;list with 1 or more values&gt;}}\n                For every feature that needs a special value, a dictionary must be passed as value.\n                This dictionary contains a name of a bucket (key) and an array of unique values that should be put\n                in that bucket.\n                When special values are defined, they are not considered in the fitting procedure.\n            missing_treatment: Defines how we treat the missing values present in the data.\n                If a string, it must be one of the following options:\n                    separate: Missing values get put in a separate 'Other' bucket: `-1`\n                    most_risky: Missing values are put into the bucket containing the largest percentage of Class 1.\n                    least_risky: Missing values are put into the bucket containing the largest percentage of Class 0.\n                    most_frequent: Missing values are put into the most common bucket.\n                    neutral: Missing values are put into the bucket with WoE closest to 0.\n                    similar: Missing values are put into the bucket with WoE closest to the bucket with only missing values.\n                    passthrough: Leaves missing values untouched.\n                If a dict, it must be of the following format:\n                    {\"&lt;column name&gt;\": &lt;bucket_number&gt;}\n                    This bucket number is where we will put the missing values.\n            remainder: How we want the non-specified columns to be transformed. It must be in [\"passthrough\", \"drop\"].\n                passthrough (Default): all columns that were not specified in \"variables\" will be passed through.\n                drop: all remaining columns that were not specified in \"variables\" will be dropped.\n        \"\"\"  # noqa\n        self.variables = variables\n        self.specials = specials\n        self.missing_treatment = missing_treatment\n        self.remainder = remainder\n        self.get_statistics = get_statistics\n\n    @property\n    def variables_type(self):\n        \"\"\"\n        Signals variables type supported by this bucketer.\n        \"\"\"\n        return \"categorical\"\n\n    def _get_feature_splits(self, feature, X, y, X_unfiltered=None):\n        \"\"\"\n        Finds the splits for a single feature.\n\n        X and y have already been preprocessed, and have specials removed.\n\n        Args:\n            feature (str): Name of the feature.\n            X (pd.Series): df with single column of feature to bucket\n            y (np.ndarray): array with target\n            X_unfiltered (pd.Series): df with single column of feature to bucket before any filtering was applied\n\n        Returns:\n            splits, right (tuple): The splits (dict or array), and whether right=True or False.\n        \"\"\"\n        unq = X.unique().tolist()\n        mapping = dict(zip(unq, range(0, len(unq))))\n\n        # Note that right is set to True, but this is not used at all for categoricals\n        return (mapping, True)\n</code></pre>"},{"location":"api/bucketers/AsIsCategoricalBucketer/#skorecard.bucketers.AsIsCategoricalBucketer.variables_type","title":"<code>variables_type</code>  <code>property</code>","text":"<p>Signals variables type supported by this bucketer.</p>"},{"location":"api/bucketers/AsIsCategoricalBucketer/#skorecard.bucketers.AsIsCategoricalBucketer.__init__","title":"<code>__init__(variables=[], specials={}, missing_treatment='separate', remainder='passthrough', get_statistics=True)</code>","text":"<p>Init the class.</p> <p>Parameters:</p> Name Type Description Default <code>variables</code> <code>list</code> <p>The features to bucket. Uses all features if not defined.</p> <code>[]</code> <code>specials</code> <p>(nested) dictionary of special values that require their own binning. The dictionary has the following format:  {\"\" : {\"name of special bucket\" : }} For every feature that needs a special value, a dictionary must be passed as value. This dictionary contains a name of a bucket (key) and an array of unique values that should be put in that bucket. When special values are defined, they are not considered in the fitting procedure. <code>{}</code> <code>missing_treatment</code> <p>Defines how we treat the missing values present in the data. If a string, it must be one of the following options:     separate: Missing values get put in a separate 'Other' bucket: <code>-1</code>     most_risky: Missing values are put into the bucket containing the largest percentage of Class 1.     least_risky: Missing values are put into the bucket containing the largest percentage of Class 0.     most_frequent: Missing values are put into the most common bucket.     neutral: Missing values are put into the bucket with WoE closest to 0.     similar: Missing values are put into the bucket with WoE closest to the bucket with only missing values.     passthrough: Leaves missing values untouched. If a dict, it must be of the following format:     {\"\": }     This bucket number is where we will put the missing values. <code>'separate'</code> <code>remainder</code> <p>How we want the non-specified columns to be transformed. It must be in [\"passthrough\", \"drop\"]. passthrough (Default): all columns that were not specified in \"variables\" will be passed through. drop: all remaining columns that were not specified in \"variables\" will be dropped.</p> <code>'passthrough'</code> Source code in <code>skorecard/bucketers/bucketers.py</code> <pre><code>def __init__(\n    self, variables=[], specials={}, missing_treatment=\"separate\", remainder=\"passthrough\", get_statistics=True\n):\n    \"\"\"Init the class.\n\n    Args:\n        variables (list): The features to bucket. Uses all features if not defined.\n        specials: (nested) dictionary of special values that require their own binning.\n            The dictionary has the following format:\n             {\"&lt;column name&gt;\" : {\"name of special bucket\" : &lt;list with 1 or more values&gt;}}\n            For every feature that needs a special value, a dictionary must be passed as value.\n            This dictionary contains a name of a bucket (key) and an array of unique values that should be put\n            in that bucket.\n            When special values are defined, they are not considered in the fitting procedure.\n        missing_treatment: Defines how we treat the missing values present in the data.\n            If a string, it must be one of the following options:\n                separate: Missing values get put in a separate 'Other' bucket: `-1`\n                most_risky: Missing values are put into the bucket containing the largest percentage of Class 1.\n                least_risky: Missing values are put into the bucket containing the largest percentage of Class 0.\n                most_frequent: Missing values are put into the most common bucket.\n                neutral: Missing values are put into the bucket with WoE closest to 0.\n                similar: Missing values are put into the bucket with WoE closest to the bucket with only missing values.\n                passthrough: Leaves missing values untouched.\n            If a dict, it must be of the following format:\n                {\"&lt;column name&gt;\": &lt;bucket_number&gt;}\n                This bucket number is where we will put the missing values.\n        remainder: How we want the non-specified columns to be transformed. It must be in [\"passthrough\", \"drop\"].\n            passthrough (Default): all columns that were not specified in \"variables\" will be passed through.\n            drop: all remaining columns that were not specified in \"variables\" will be dropped.\n    \"\"\"  # noqa\n    self.variables = variables\n    self.specials = specials\n    self.missing_treatment = missing_treatment\n    self.remainder = remainder\n    self.get_statistics = get_statistics\n</code></pre>"},{"location":"api/bucketers/AsIsNumericalBucketer/","title":"AsIsNumericalBucketer","text":"<p>               Bases: <code>BaseBucketer</code></p> <p>The <code>AsIsNumericalBucketer</code> transformer creates buckets by treating the existing unique values as boundaries.</p> <p>Support:  </p> <p>This is bucketer is useful when you have data that is already sufficiented bucketed, but you would like to be able to bucket new data in the same way.</p> <p>Example:</p> <pre><code>from skorecard import datasets\nfrom skorecard.bucketers import AsIsNumericalBucketer\n\nX, y = datasets.load_uci_credit_card(return_X_y=True)\nbucketer = AsIsNumericalBucketer(variables=['LIMIT_BAL'])\nbucketer.fit_transform(X)\n</code></pre> Source code in <code>skorecard/bucketers/bucketers.py</code> <pre><code>class AsIsNumericalBucketer(BaseBucketer):\n    \"\"\"\n    The `AsIsNumericalBucketer` transformer creates buckets by treating the existing unique values as boundaries.\n\n    Support: ![badge](https://img.shields.io/badge/numerical-true-green) ![badge](https://img.shields.io/badge/categorical-false-red) ![badge](https://img.shields.io/badge/supervised-false-blue)\n\n    This is bucketer is useful when you have data that is already sufficiented bucketed,\n    but you would like to be able to bucket new data in the same way.\n\n    Example:\n\n    ```python\n    from skorecard import datasets\n    from skorecard.bucketers import AsIsNumericalBucketer\n\n    X, y = datasets.load_uci_credit_card(return_X_y=True)\n    bucketer = AsIsNumericalBucketer(variables=['LIMIT_BAL'])\n    bucketer.fit_transform(X)\n    ```\n    \"\"\"  # noqa\n\n    def __init__(\n        self,\n        right=True,\n        variables=[],\n        specials={},\n        missing_treatment=\"separate\",\n        remainder=\"passthrough\",\n        get_statistics=True,\n    ):\n        \"\"\"\n        Init the class.\n\n        Args:\n            right (boolean): Is the right value included in a range (default) or is 'up to not but including'.\n                For example, if you have [5, 10], the ranges for right=True would be (-Inf, 5], (5, 10], (10, Inf]\n                or [-Inf, 5), [5, 10), [10, Inf) for right=False\n            variables (list): The features to bucket. Uses all features if not defined.\n            specials (dict): (nested) dictionary of special values that require their own binning.\n                The dictionary has the following format:\n                 {\"&lt;column name&gt;\" : {\"name of special bucket\" : &lt;list with 1 or more values&gt;}}\n                For every feature that needs a special value, a dictionary must be passed as value.\n                This dictionary contains a name of a bucket (key) and an array of unique values that should be put\n                in that bucket.\n                When special values are defined, they are not considered in the fitting procedure.\n            missing_treatment (str or dict): Defines how we treat the missing values present in the data.\n                If a string, it must be one of the following options:\n                    separate: Missing values get put in a separate 'Other' bucket: `-1`\n                    most_risky: Missing values are put into the bucket containing the largest percentage of Class 1.\n                    least_risky: Missing values are put into the bucket containing the largest percentage of Class 0.\n                    most_frequent: Missing values are put into the most common bucket.\n                    neutral: Missing values are put into the bucket with WoE closest to 0.\n                    similar: Missing values are put into the bucket with WoE closest to the bucket with only missing values.\n                    passthrough: Leaves missing values untouched.\n                If a dict, it must be of the following format:\n                    {\"&lt;column name&gt;\": &lt;bucket_number&gt;}\n                    This bucket number is where we will put the missing values..\n            remainder (str): How we want the non-specified columns to be transformed. It must be in [\"passthrough\", \"drop\"].\n                passthrough (Default): all columns that were not specified in \"variables\" will be passed through.\n                drop: all remaining columns that were not specified in \"variables\" will be dropped.\n        \"\"\"  # noqa\n        self.right = right\n        self.variables = variables\n        self.specials = specials\n        self.missing_treatment = missing_treatment\n        self.remainder = remainder\n        self.get_statistics = get_statistics\n\n    @property\n    def variables_type(self):\n        \"\"\"\n        Signals variables type supported by this bucketer.\n        \"\"\"\n        return \"numerical\"\n\n    def _get_feature_splits(self, feature, X, y, X_unfiltered=None):\n        \"\"\"\n        Finds the splits for a single feature.\n\n        X and y have already been preprocessed, and have specials removed.\n\n        Args:\n            feature (str): Name of the feature.\n            X (pd.Series): df with single column of feature to bucket\n            y (np.ndarray): array with target\n            X_unfiltered (pd.Series): df with single column of feature to bucket before any filtering was applied\n\n        Returns:\n            splits, right (tuple): The splits (dict or array), and whether right=True or False.\n        \"\"\"\n        boundaries = X.unique().tolist()\n        boundaries.sort()\n\n        if len(boundaries) &gt; 100:\n            msg = f\"The column '{feature}' has more than 100 unique values \"\n            msg += \"and cannot be used with the AsIsBucketer.\"\n            msg += \"Apply a different bucketer first.\"\n            raise NotPreBucketedError(msg)\n\n        return (boundaries, self.right)\n</code></pre>"},{"location":"api/bucketers/AsIsNumericalBucketer/#skorecard.bucketers.AsIsNumericalBucketer.variables_type","title":"<code>variables_type</code>  <code>property</code>","text":"<p>Signals variables type supported by this bucketer.</p>"},{"location":"api/bucketers/AsIsNumericalBucketer/#skorecard.bucketers.AsIsNumericalBucketer.__init__","title":"<code>__init__(right=True, variables=[], specials={}, missing_treatment='separate', remainder='passthrough', get_statistics=True)</code>","text":"<p>Init the class.</p> <p>Parameters:</p> Name Type Description Default <code>right</code> <code>boolean</code> <p>Is the right value included in a range (default) or is 'up to not but including'. For example, if you have [5, 10], the ranges for right=True would be (-Inf, 5], (5, 10], (10, Inf] or [-Inf, 5), [5, 10), [10, Inf) for right=False</p> <code>True</code> <code>variables</code> <code>list</code> <p>The features to bucket. Uses all features if not defined.</p> <code>[]</code> <code>specials</code> <code>dict</code> <p>(nested) dictionary of special values that require their own binning. The dictionary has the following format:  {\"\" : {\"name of special bucket\" : }} For every feature that needs a special value, a dictionary must be passed as value. This dictionary contains a name of a bucket (key) and an array of unique values that should be put in that bucket. When special values are defined, they are not considered in the fitting procedure. <code>{}</code> <code>missing_treatment</code> <code>str or dict</code> <p>Defines how we treat the missing values present in the data. If a string, it must be one of the following options:     separate: Missing values get put in a separate 'Other' bucket: <code>-1</code>     most_risky: Missing values are put into the bucket containing the largest percentage of Class 1.     least_risky: Missing values are put into the bucket containing the largest percentage of Class 0.     most_frequent: Missing values are put into the most common bucket.     neutral: Missing values are put into the bucket with WoE closest to 0.     similar: Missing values are put into the bucket with WoE closest to the bucket with only missing values.     passthrough: Leaves missing values untouched. If a dict, it must be of the following format:     {\"\": }     This bucket number is where we will put the missing values.. <code>'separate'</code> <code>remainder</code> <code>str</code> <p>How we want the non-specified columns to be transformed. It must be in [\"passthrough\", \"drop\"]. passthrough (Default): all columns that were not specified in \"variables\" will be passed through. drop: all remaining columns that were not specified in \"variables\" will be dropped.</p> <code>'passthrough'</code> Source code in <code>skorecard/bucketers/bucketers.py</code> <pre><code>def __init__(\n    self,\n    right=True,\n    variables=[],\n    specials={},\n    missing_treatment=\"separate\",\n    remainder=\"passthrough\",\n    get_statistics=True,\n):\n    \"\"\"\n    Init the class.\n\n    Args:\n        right (boolean): Is the right value included in a range (default) or is 'up to not but including'.\n            For example, if you have [5, 10], the ranges for right=True would be (-Inf, 5], (5, 10], (10, Inf]\n            or [-Inf, 5), [5, 10), [10, Inf) for right=False\n        variables (list): The features to bucket. Uses all features if not defined.\n        specials (dict): (nested) dictionary of special values that require their own binning.\n            The dictionary has the following format:\n             {\"&lt;column name&gt;\" : {\"name of special bucket\" : &lt;list with 1 or more values&gt;}}\n            For every feature that needs a special value, a dictionary must be passed as value.\n            This dictionary contains a name of a bucket (key) and an array of unique values that should be put\n            in that bucket.\n            When special values are defined, they are not considered in the fitting procedure.\n        missing_treatment (str or dict): Defines how we treat the missing values present in the data.\n            If a string, it must be one of the following options:\n                separate: Missing values get put in a separate 'Other' bucket: `-1`\n                most_risky: Missing values are put into the bucket containing the largest percentage of Class 1.\n                least_risky: Missing values are put into the bucket containing the largest percentage of Class 0.\n                most_frequent: Missing values are put into the most common bucket.\n                neutral: Missing values are put into the bucket with WoE closest to 0.\n                similar: Missing values are put into the bucket with WoE closest to the bucket with only missing values.\n                passthrough: Leaves missing values untouched.\n            If a dict, it must be of the following format:\n                {\"&lt;column name&gt;\": &lt;bucket_number&gt;}\n                This bucket number is where we will put the missing values..\n        remainder (str): How we want the non-specified columns to be transformed. It must be in [\"passthrough\", \"drop\"].\n            passthrough (Default): all columns that were not specified in \"variables\" will be passed through.\n            drop: all remaining columns that were not specified in \"variables\" will be dropped.\n    \"\"\"  # noqa\n    self.right = right\n    self.variables = variables\n    self.specials = specials\n    self.missing_treatment = missing_treatment\n    self.remainder = remainder\n    self.get_statistics = get_statistics\n</code></pre>"},{"location":"api/bucketers/DecisionTreeBucketer/","title":"DecisionTreeBucketer","text":"<p>               Bases: <code>BaseBucketer</code></p> <p>The <code>DecisionTreeBucketer</code> transformer creates buckets by training a decision tree.</p> <p>Support:  </p> <p>It uses sklearn.tree.DecisionTreeClassifier to find the splits.</p> <p>Example:</p> <pre><code>from skorecard import datasets\nfrom skorecard.bucketers import DecisionTreeBucketer\nX, y = datasets.load_uci_credit_card(return_X_y=True)\n\n# make sure that those cases\nspecials = {\n    \"LIMIT_BAL\":{\n        \"=50000\":[50000],\n        \"in [20001,30000]\":[20000,30000],\n        }\n}\n\ndt_bucketer = DecisionTreeBucketer(variables=['LIMIT_BAL'], specials = specials)\ndt_bucketer.fit(X, y)\n\ndt_bucketer.fit_transform(X, y)['LIMIT_BAL'].value_counts()\n</code></pre> Source code in <code>skorecard/bucketers/bucketers.py</code> <pre><code>class DecisionTreeBucketer(BaseBucketer):\n    \"\"\"\n    The `DecisionTreeBucketer` transformer creates buckets by training a decision tree.\n\n    Support: ![badge](https://img.shields.io/badge/numerical-true-green) ![badge](https://img.shields.io/badge/categorical-false-red) ![badge](https://img.shields.io/badge/supervised-true-green)\n\n    It uses [sklearn.tree.DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)\n    to find the splits.\n\n    Example:\n\n    ```python\n    from skorecard import datasets\n    from skorecard.bucketers import DecisionTreeBucketer\n    X, y = datasets.load_uci_credit_card(return_X_y=True)\n\n    # make sure that those cases\n    specials = {\n        \"LIMIT_BAL\":{\n            \"=50000\":[50000],\n            \"in [20001,30000]\":[20000,30000],\n            }\n    }\n\n    dt_bucketer = DecisionTreeBucketer(variables=['LIMIT_BAL'], specials = specials)\n    dt_bucketer.fit(X, y)\n\n    dt_bucketer.fit_transform(X, y)['LIMIT_BAL'].value_counts()\n    ```\n    \"\"\"  # noqa\n\n    def __init__(\n        self,\n        variables=[],\n        specials={},\n        max_n_bins=100,\n        missing_treatment=\"separate\",\n        min_bin_size=0.05,\n        random_state=None,\n        remainder=\"passthrough\",\n        get_statistics=True,\n        dt_kwargs={},\n    ) -&gt; None:\n        \"\"\"Init the class.\n\n        Args:\n            variables (list): The features to bucket. Uses all features if not defined.\n            specials (dict):  dictionary of special values that require their own binning.\n                The dictionary has the following format:\n                 {\"&lt;column name&gt;\" : {\"name of special bucket\" : &lt;list with 1 or more values&gt;}}\n                For every feature that needs a special value, a dictionary must be passed as value.\n                This dictionary contains a name of a bucket (key) and an array of unique values that should be put\n                in that bucket.\n                When special values are defined, they are not considered in the fitting procedure.\n            min_bin_size (float): Minimum fraction of observations in a bucket. Passed directly to min_samples_leaf.\n            max_n_bins (int): Maximum numbers of after the bucketing. Passed directly to max_leaf_nodes of the\n                DecisionTreeClassifier.\n                If specials are defined, max_leaf_nodes will be redefined to max_n_bins - (number of special bins).\n                The DecisionTreeClassifier requires max_leaf_nodes&gt;=2:\n                therefore, max_n_bins  must always be &gt;= (number of special bins + 2) if specials are defined,\n                otherwise must be &gt;=2.\n            missing_treatment (str or dict): Defines how we treat the missing values present in the data.\n                If a string, it must be one of the following options:\n                    separate: Missing values get put in a separate 'Other' bucket: `-1`\n                    most_risky: Missing values are put into the bucket containing the largest percentage of Class 1.\n                    least_risky: Missing values are put into the bucket containing the largest percentage of Class 0.\n                    most_frequent: Missing values are put into the most common bucket.\n                    neutral: Missing values are put into the bucket with WoE closest to 0.\n                    similar: Missing values are put into the bucket with WoE closest to the bucket with only missing values.\n                    passthrough: Leaves missing values untouched.\n                If a dict, it must be of the following format:\n                    {\"&lt;column name&gt;\": &lt;bucket_number&gt;}\n                    This bucket number is where we will put the missing values.\n            random_state (int): The random state, Passed directly to DecisionTreeClassifier\n            remainder (str): How we want the non-specified columns to be transformed. It must be in [\"passthrough\", \"drop\"].\n                passthrough (Default): all columns that were not specified in \"variables\" will be passed through.\n                drop: all remaining columns that were not specified in \"variables\" will be dropped.\n            dt_kwargs: Other parameters passed to DecisionTreeClassifier\n        \"\"\"  # noqa\n        self.variables = variables\n        self.specials = specials\n        self.dt_kwargs = dt_kwargs\n        self.max_n_bins = max_n_bins\n        self.missing_treatment = missing_treatment\n        self.min_bin_size = min_bin_size\n        self.random_state = random_state\n        self.remainder = remainder\n        self.get_statistics = get_statistics\n        self.dt_kwargs.update({\"random_state\": self.random_state})\n\n        check_args(dt_kwargs, DecisionTreeClassifier)\n\n    @property\n    def variables_type(self):\n        \"\"\"\n        Signals variables type supported by this bucketer.\n        \"\"\"\n        return \"numerical\"\n\n    def _get_feature_splits(self, feature, X, y, X_unfiltered=None):\n        \"\"\"\n        Finds the splits for a single feature.\n\n        X and y have already been preprocessed, and have specials removed.\n\n        Args:\n            feature (str): Name of the feature.\n            X (pd.Series): df with single column of feature to bucket\n            y (np.ndarray): array with target\n            X_unfiltered (pd.Series): df with single column of feature to bucket before any filtering was applied\n\n        Returns:\n            splits, right (tuple): The splits (dict or array), and whether right=True or False.\n        \"\"\"\n        # Make sure max_n_bins settings is correct\n        n_special_bins = 0\n        if feature in self.specials.keys():\n            n_special_bins = len(self.specials[feature])\n            if (self.max_n_bins - n_special_bins) &lt;= 1:\n                raise ValueError(\n                    f\"max_n_bins must be at least = the number of special bins + 2: set a value \"\n                    f\"max_n_bins&gt;= {n_special_bins+2} (currently max_n_bins={self.max_n_bins})\"\n                )\n\n        # If the data contains only specials,\n        # Then don't use any splits\n        if X.shape[0] == 0:\n            splits = []\n        else:\n            # If the specials are excluded, make sure that the bin size is rescaled.\n            frac_left = X.shape[0] / X_unfiltered.shape[0]\n            min_bin_size = self.min_bin_size / frac_left\n\n            if min_bin_size &gt; 0.5:\n                min_bin_size = 0.5\n\n            binner = DecisionTreeClassifier(\n                max_leaf_nodes=(self.max_n_bins - n_special_bins),\n                min_samples_leaf=min_bin_size,\n                **self.dt_kwargs,\n            )\n            binner.fit(X.values.reshape(-1, 1), y)\n\n            # Extract fitted boundaries\n            splits = np.unique(binner.tree_.threshold[binner.tree_.feature != _tree.TREE_UNDEFINED])\n\n        # Note for trees we use right=False\n        return (splits, False)\n</code></pre>"},{"location":"api/bucketers/DecisionTreeBucketer/#skorecard.bucketers.DecisionTreeBucketer.variables_type","title":"<code>variables_type</code>  <code>property</code>","text":"<p>Signals variables type supported by this bucketer.</p>"},{"location":"api/bucketers/DecisionTreeBucketer/#skorecard.bucketers.DecisionTreeBucketer.__init__","title":"<code>__init__(variables=[], specials={}, max_n_bins=100, missing_treatment='separate', min_bin_size=0.05, random_state=None, remainder='passthrough', get_statistics=True, dt_kwargs={})</code>","text":"<p>Init the class.</p> <p>Parameters:</p> Name Type Description Default <code>variables</code> <code>list</code> <p>The features to bucket. Uses all features if not defined.</p> <code>[]</code> <code>specials</code> <code>dict</code> <p>dictionary of special values that require their own binning. The dictionary has the following format:  {\"\" : {\"name of special bucket\" : }} For every feature that needs a special value, a dictionary must be passed as value. This dictionary contains a name of a bucket (key) and an array of unique values that should be put in that bucket. When special values are defined, they are not considered in the fitting procedure. <code>{}</code> <code>min_bin_size</code> <code>float</code> <p>Minimum fraction of observations in a bucket. Passed directly to min_samples_leaf.</p> <code>0.05</code> <code>max_n_bins</code> <code>int</code> <p>Maximum numbers of after the bucketing. Passed directly to max_leaf_nodes of the DecisionTreeClassifier. If specials are defined, max_leaf_nodes will be redefined to max_n_bins - (number of special bins). The DecisionTreeClassifier requires max_leaf_nodes&gt;=2: therefore, max_n_bins  must always be &gt;= (number of special bins + 2) if specials are defined, otherwise must be &gt;=2.</p> <code>100</code> <code>missing_treatment</code> <code>str or dict</code> <p>Defines how we treat the missing values present in the data. If a string, it must be one of the following options:     separate: Missing values get put in a separate 'Other' bucket: <code>-1</code>     most_risky: Missing values are put into the bucket containing the largest percentage of Class 1.     least_risky: Missing values are put into the bucket containing the largest percentage of Class 0.     most_frequent: Missing values are put into the most common bucket.     neutral: Missing values are put into the bucket with WoE closest to 0.     similar: Missing values are put into the bucket with WoE closest to the bucket with only missing values.     passthrough: Leaves missing values untouched. If a dict, it must be of the following format:     {\"\": }     This bucket number is where we will put the missing values. <code>'separate'</code> <code>random_state</code> <code>int</code> <p>The random state, Passed directly to DecisionTreeClassifier</p> <code>None</code> <code>remainder</code> <code>str</code> <p>How we want the non-specified columns to be transformed. It must be in [\"passthrough\", \"drop\"]. passthrough (Default): all columns that were not specified in \"variables\" will be passed through. drop: all remaining columns that were not specified in \"variables\" will be dropped.</p> <code>'passthrough'</code> <code>dt_kwargs</code> <p>Other parameters passed to DecisionTreeClassifier</p> <code>{}</code> Source code in <code>skorecard/bucketers/bucketers.py</code> <pre><code>def __init__(\n    self,\n    variables=[],\n    specials={},\n    max_n_bins=100,\n    missing_treatment=\"separate\",\n    min_bin_size=0.05,\n    random_state=None,\n    remainder=\"passthrough\",\n    get_statistics=True,\n    dt_kwargs={},\n) -&gt; None:\n    \"\"\"Init the class.\n\n    Args:\n        variables (list): The features to bucket. Uses all features if not defined.\n        specials (dict):  dictionary of special values that require their own binning.\n            The dictionary has the following format:\n             {\"&lt;column name&gt;\" : {\"name of special bucket\" : &lt;list with 1 or more values&gt;}}\n            For every feature that needs a special value, a dictionary must be passed as value.\n            This dictionary contains a name of a bucket (key) and an array of unique values that should be put\n            in that bucket.\n            When special values are defined, they are not considered in the fitting procedure.\n        min_bin_size (float): Minimum fraction of observations in a bucket. Passed directly to min_samples_leaf.\n        max_n_bins (int): Maximum numbers of after the bucketing. Passed directly to max_leaf_nodes of the\n            DecisionTreeClassifier.\n            If specials are defined, max_leaf_nodes will be redefined to max_n_bins - (number of special bins).\n            The DecisionTreeClassifier requires max_leaf_nodes&gt;=2:\n            therefore, max_n_bins  must always be &gt;= (number of special bins + 2) if specials are defined,\n            otherwise must be &gt;=2.\n        missing_treatment (str or dict): Defines how we treat the missing values present in the data.\n            If a string, it must be one of the following options:\n                separate: Missing values get put in a separate 'Other' bucket: `-1`\n                most_risky: Missing values are put into the bucket containing the largest percentage of Class 1.\n                least_risky: Missing values are put into the bucket containing the largest percentage of Class 0.\n                most_frequent: Missing values are put into the most common bucket.\n                neutral: Missing values are put into the bucket with WoE closest to 0.\n                similar: Missing values are put into the bucket with WoE closest to the bucket with only missing values.\n                passthrough: Leaves missing values untouched.\n            If a dict, it must be of the following format:\n                {\"&lt;column name&gt;\": &lt;bucket_number&gt;}\n                This bucket number is where we will put the missing values.\n        random_state (int): The random state, Passed directly to DecisionTreeClassifier\n        remainder (str): How we want the non-specified columns to be transformed. It must be in [\"passthrough\", \"drop\"].\n            passthrough (Default): all columns that were not specified in \"variables\" will be passed through.\n            drop: all remaining columns that were not specified in \"variables\" will be dropped.\n        dt_kwargs: Other parameters passed to DecisionTreeClassifier\n    \"\"\"  # noqa\n    self.variables = variables\n    self.specials = specials\n    self.dt_kwargs = dt_kwargs\n    self.max_n_bins = max_n_bins\n    self.missing_treatment = missing_treatment\n    self.min_bin_size = min_bin_size\n    self.random_state = random_state\n    self.remainder = remainder\n    self.get_statistics = get_statistics\n    self.dt_kwargs.update({\"random_state\": self.random_state})\n\n    check_args(dt_kwargs, DecisionTreeClassifier)\n</code></pre>"},{"location":"api/bucketers/EqualFrequencyBucketer/","title":"EqualFrequencyBucketer","text":"<p>               Bases: <code>BaseBucketer</code></p> <p>The <code>EqualFrequencyBucketer</code> transformer creates buckets with equal number of elements.</p> <p>Support:  </p> <p>Example:</p> <pre><code>from skorecard import datasets\nfrom skorecard.bucketers import EqualFrequencyBucketer\n\nX, y = datasets.load_uci_credit_card(return_X_y=True)\nbucketer = EqualFrequencyBucketer(n_bins = 10, variables=['LIMIT_BAL'])\nbucketer.fit_transform(X)\nbucketer.fit_transform(X)['LIMIT_BAL'].value_counts()\n</code></pre> Source code in <code>skorecard/bucketers/bucketers.py</code> <pre><code>class EqualFrequencyBucketer(BaseBucketer):\n    \"\"\"\n    The `EqualFrequencyBucketer` transformer creates buckets with equal number of elements.\n\n    Support: ![badge](https://img.shields.io/badge/numerical-true-green) ![badge](https://img.shields.io/badge/categorical-false-red) ![badge](https://img.shields.io/badge/supervised-false-red)\n\n    Example:\n\n    ```python\n    from skorecard import datasets\n    from skorecard.bucketers import EqualFrequencyBucketer\n\n    X, y = datasets.load_uci_credit_card(return_X_y=True)\n    bucketer = EqualFrequencyBucketer(n_bins = 10, variables=['LIMIT_BAL'])\n    bucketer.fit_transform(X)\n    bucketer.fit_transform(X)['LIMIT_BAL'].value_counts()\n    ```\n    \"\"\"  # noqa\n\n    def __init__(\n        self,\n        n_bins=5,\n        variables=[],\n        specials={},\n        missing_treatment=\"separate\",\n        remainder=\"passthrough\",\n        get_statistics=True,\n    ):\n        \"\"\"Init the class.\n\n        Args:\n            n_bins (int): Number of bins to create.\n            variables (list): The features to bucket. Uses all features if not defined.\n            specials: (nested) dictionary of special values that require their own binning.\n                The dictionary has the following format:\n                 {\"&lt;column name&gt;\" : {\"name of special bucket\" : &lt;list with 1 or more values&gt;}}\n                For every feature that needs a special value, a dictionary must be passed as value.\n                This dictionary contains a name of a bucket (key) and an array of unique values that should be put\n                in that bucket.\n                When special values are defined, they are not considered in the fitting procedure.\n            missing_treatment: Defines how we treat the missing values present in the data.\n                If a string, it must be one of the following options:\n                    separate: Missing values get put in a separate 'Other' bucket: `-1`\n                    most_risky: Missing values are put into the bucket containing the largest percentage of Class 1.\n                    least_risky: Missing values are put into the bucket containing the largest percentage of Class 0.\n                    most_frequent: Missing values are put into the most common bucket.\n                    neutral: Missing values are put into the bucket with WoE closest to 0.\n                    similar: Missing values are put into the bucket with WoE closest to the bucket with only missing values.\n                    passthrough: Leaves missing values untouched.\n                If a dict, it must be of the following format:\n                    {\"&lt;column name&gt;\": &lt;bucket_number&gt;}\n                    This bucket number is where we will put the missing values..\n            remainder: How we want the non-specified columns to be transformed. It must be in [\"passthrough\", \"drop\"].\n                passthrough (Default): all columns that were not specified in \"variables\" will be passed through.\n                drop: all remaining columns that were not specified in \"variables\" will be dropped.\n        \"\"\"  # noqa\n        self.variables = variables\n        self.n_bins = n_bins\n        self.specials = specials\n        self.missing_treatment = missing_treatment\n        self.remainder = remainder\n        self.get_statistics = get_statistics\n\n    @property\n    def variables_type(self):\n        \"\"\"\n        Signals variables type supported by this bucketer.\n        \"\"\"\n        return \"numerical\"\n\n    def _get_feature_splits(self, feature, X, y, X_unfiltered=None):\n        \"\"\"\n        Finds the splits for a single feature.\n\n        X and y have already been preprocessed, and have specials removed.\n\n        Args:\n            feature (str): Name of the feature.\n            X (pd.Series): df with single column of feature to bucket\n            y (np.ndarray): array with target\n            X_unfiltered (pd.Series): df with single column of feature to bucket before any filtering was applied\n\n        Returns:\n            splits, right (tuple): The splits (dict or array), and whether right=True or False.\n        \"\"\"\n        # Fit the estimator\n        # Uses pd.qcut()\n        # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.qcut.html\n        try:\n            _, boundaries = pd.qcut(X, q=self.n_bins, retbins=True, duplicates=\"raise\")\n        except ValueError:\n            # If there are too many duplicate values (assume a lot of filled missings)\n            # this crashes - the exception drops them.\n            # This means that it will return approximate quantile bins\n            _, boundaries = pd.qcut(X, q=self.n_bins, retbins=True, duplicates=\"drop\")\n            warnings.warn(ApproximationWarning(\"Approximated quantiles - too many unique values\"))\n\n        # pd.qcut returns the min &amp; max values of the fits\n        # On transform, we use np.digitize, which means new data that is outside of this range\n        # will be assigned to their own buckets.\n        # To solve, we simply remove the min and max boundaries\n        boundaries = boundaries[1:-1]\n\n        if isinstance(boundaries, np.ndarray):\n            boundaries = boundaries.tolist()\n\n        # pd.qcut returns bins including right edge: (edge, edge]\n        return (boundaries, True)\n</code></pre>"},{"location":"api/bucketers/EqualFrequencyBucketer/#skorecard.bucketers.EqualFrequencyBucketer.variables_type","title":"<code>variables_type</code>  <code>property</code>","text":"<p>Signals variables type supported by this bucketer.</p>"},{"location":"api/bucketers/EqualFrequencyBucketer/#skorecard.bucketers.EqualFrequencyBucketer.__init__","title":"<code>__init__(n_bins=5, variables=[], specials={}, missing_treatment='separate', remainder='passthrough', get_statistics=True)</code>","text":"<p>Init the class.</p> <p>Parameters:</p> Name Type Description Default <code>n_bins</code> <code>int</code> <p>Number of bins to create.</p> <code>5</code> <code>variables</code> <code>list</code> <p>The features to bucket. Uses all features if not defined.</p> <code>[]</code> <code>specials</code> <p>(nested) dictionary of special values that require their own binning. The dictionary has the following format:  {\"\" : {\"name of special bucket\" : }} For every feature that needs a special value, a dictionary must be passed as value. This dictionary contains a name of a bucket (key) and an array of unique values that should be put in that bucket. When special values are defined, they are not considered in the fitting procedure. <code>{}</code> <code>missing_treatment</code> <p>Defines how we treat the missing values present in the data. If a string, it must be one of the following options:     separate: Missing values get put in a separate 'Other' bucket: <code>-1</code>     most_risky: Missing values are put into the bucket containing the largest percentage of Class 1.     least_risky: Missing values are put into the bucket containing the largest percentage of Class 0.     most_frequent: Missing values are put into the most common bucket.     neutral: Missing values are put into the bucket with WoE closest to 0.     similar: Missing values are put into the bucket with WoE closest to the bucket with only missing values.     passthrough: Leaves missing values untouched. If a dict, it must be of the following format:     {\"\": }     This bucket number is where we will put the missing values.. <code>'separate'</code> <code>remainder</code> <p>How we want the non-specified columns to be transformed. It must be in [\"passthrough\", \"drop\"]. passthrough (Default): all columns that were not specified in \"variables\" will be passed through. drop: all remaining columns that were not specified in \"variables\" will be dropped.</p> <code>'passthrough'</code> Source code in <code>skorecard/bucketers/bucketers.py</code> <pre><code>def __init__(\n    self,\n    n_bins=5,\n    variables=[],\n    specials={},\n    missing_treatment=\"separate\",\n    remainder=\"passthrough\",\n    get_statistics=True,\n):\n    \"\"\"Init the class.\n\n    Args:\n        n_bins (int): Number of bins to create.\n        variables (list): The features to bucket. Uses all features if not defined.\n        specials: (nested) dictionary of special values that require their own binning.\n            The dictionary has the following format:\n             {\"&lt;column name&gt;\" : {\"name of special bucket\" : &lt;list with 1 or more values&gt;}}\n            For every feature that needs a special value, a dictionary must be passed as value.\n            This dictionary contains a name of a bucket (key) and an array of unique values that should be put\n            in that bucket.\n            When special values are defined, they are not considered in the fitting procedure.\n        missing_treatment: Defines how we treat the missing values present in the data.\n            If a string, it must be one of the following options:\n                separate: Missing values get put in a separate 'Other' bucket: `-1`\n                most_risky: Missing values are put into the bucket containing the largest percentage of Class 1.\n                least_risky: Missing values are put into the bucket containing the largest percentage of Class 0.\n                most_frequent: Missing values are put into the most common bucket.\n                neutral: Missing values are put into the bucket with WoE closest to 0.\n                similar: Missing values are put into the bucket with WoE closest to the bucket with only missing values.\n                passthrough: Leaves missing values untouched.\n            If a dict, it must be of the following format:\n                {\"&lt;column name&gt;\": &lt;bucket_number&gt;}\n                This bucket number is where we will put the missing values..\n        remainder: How we want the non-specified columns to be transformed. It must be in [\"passthrough\", \"drop\"].\n            passthrough (Default): all columns that were not specified in \"variables\" will be passed through.\n            drop: all remaining columns that were not specified in \"variables\" will be dropped.\n    \"\"\"  # noqa\n    self.variables = variables\n    self.n_bins = n_bins\n    self.specials = specials\n    self.missing_treatment = missing_treatment\n    self.remainder = remainder\n    self.get_statistics = get_statistics\n</code></pre>"},{"location":"api/bucketers/EqualWidthBucketer/","title":"EqualWidthBucketer","text":"<p>               Bases: <code>BaseBucketer</code></p> <p>The <code>EqualWidthBucketer</code> transformer creates equally spaced bins using numpy.histogram function.</p> <p>Support:  </p> <p>Example:</p> <pre><code>from skorecard import datasets\nfrom skorecard.bucketers import EqualWidthBucketer\n\nspecials = {\"LIMIT_BAL\": {\"=50000\": [50000], \"in [20001,30000]\": [20000, 30000]}}\n\nX, y = datasets.load_uci_credit_card(return_X_y=True)\nbucketer = EqualWidthBucketer(n_bins = 10, variables = ['LIMIT_BAL'], specials=specials)\nbucketer.fit_transform(X)\nbucketer.fit_transform(X)['LIMIT_BAL'].value_counts()\n</code></pre> Source code in <code>skorecard/bucketers/bucketers.py</code> <pre><code>class EqualWidthBucketer(BaseBucketer):\n    \"\"\"\n    The `EqualWidthBucketer` transformer creates equally spaced bins using [numpy.histogram](https://numpy.org/doc/stable/reference/generated/numpy.histogram.html)\n    function.\n\n    Support: ![badge](https://img.shields.io/badge/numerical-true-green) ![badge](https://img.shields.io/badge/categorical-false-red) ![badge](https://img.shields.io/badge/supervised-false-red)\n\n    Example:\n\n    ```python\n    from skorecard import datasets\n    from skorecard.bucketers import EqualWidthBucketer\n\n    specials = {\"LIMIT_BAL\": {\"=50000\": [50000], \"in [20001,30000]\": [20000, 30000]}}\n\n    X, y = datasets.load_uci_credit_card(return_X_y=True)\n    bucketer = EqualWidthBucketer(n_bins = 10, variables = ['LIMIT_BAL'], specials=specials)\n    bucketer.fit_transform(X)\n    bucketer.fit_transform(X)['LIMIT_BAL'].value_counts()\n    ```\n    \"\"\"  # noqa\n\n    def __init__(\n        self,\n        n_bins=5,\n        variables=[],\n        specials={},\n        missing_treatment=\"separate\",\n        remainder=\"passthrough\",\n        get_statistics=True,\n    ):\n        \"\"\"Init the class.\n\n        Args:\n            n_bins (int): Number of bins to create.\n            variables (list): The features to bucket. Uses all features if not defined.\n            specials: (dict) of special values that require their own binning.\n                The dictionary has the following format:\n                 {\"&lt;column name&gt;\" : {\"name of special bucket\" : &lt;list with 1 or more values&gt;}}\n                For every feature that needs a special value, a dictionary must be passed as value.\n                This dictionary contains a name of a bucket (key) and an array of unique values that should be put\n                in that bucket.\n                When special values are defined, they are not considered in the fitting procedure.\n            missing_treatment: Defines how we treat the missing values present in the data.\n                If a string, it must be one of the following options:\n                    separate: Missing values get put in a separate 'Other' bucket: `-1`\n                    most_risky: Missing values are put into the bucket containing the largest percentage of Class 1.\n                    least_risky: Missing values are put into the bucket containing the largest percentage of Class 0.\n                    most_frequent: Missing values are put into the most common bucket.\n                    neutral: Missing values are put into the bucket with WoE closest to 0.\n                    similar: Missing values are put into the bucket with WoE closest to the bucket with only missing values.\n                    passthrough: Leaves missing values untouched.\n                If a dict, it must be of the following format:\n                    {\"&lt;column name&gt;\": &lt;bucket_number&gt;}\n                    This bucket number is where we will put the missing values.\n            remainder: How we want the non-specified columns to be transformed. It must be in [\"passthrough\", \"drop\"].\n                passthrough (Default): all columns that were not specified in \"variables\" will be passed through.\n                drop: all remaining columns that were not specified in \"variables\" will be dropped.\n        \"\"\"  # noqa\n        self.missing_treatment = missing_treatment\n        self.variables = variables\n        self.n_bins = n_bins\n        self.specials = specials\n        self.remainder = remainder\n        self.get_statistics = get_statistics\n\n    @property\n    def variables_type(self):\n        \"\"\"\n        Signals variables type supported by this bucketer.\n        \"\"\"\n        return \"numerical\"\n\n    def _get_feature_splits(self, feature, X, y, X_unfiltered=None):\n        \"\"\"\n        Finds the splits for a single feature.\n\n        X and y have already been preprocessed, and have specials removed.\n\n        Args:\n            feature (str): Name of the feature.\n            X (pd.Series): df with single column of feature to bucket\n            y (np.ndarray): array with target\n            X_unfiltered (pd.Series): df with single column of feature to bucket before any filtering was applied\n\n        Returns:\n            splits, right (tuple): The splits (dict or array), and whether right=True or False.\n        \"\"\"\n        _, boundaries = np.histogram(X.values, bins=self.n_bins)\n\n        # np.histogram returns the min &amp; max values of the fits\n        # On transform, we use np.digitize, which means new data that is outside of this range\n        # will be assigned to their own buckets.\n        # To solve, we simply remove the min and max boundaries\n        boundaries = boundaries[1:-1]\n\n        if isinstance(boundaries, np.ndarray):\n            boundaries = boundaries.tolist()\n\n        return (boundaries, True)\n</code></pre>"},{"location":"api/bucketers/EqualWidthBucketer/#skorecard.bucketers.EqualWidthBucketer.variables_type","title":"<code>variables_type</code>  <code>property</code>","text":"<p>Signals variables type supported by this bucketer.</p>"},{"location":"api/bucketers/EqualWidthBucketer/#skorecard.bucketers.EqualWidthBucketer.__init__","title":"<code>__init__(n_bins=5, variables=[], specials={}, missing_treatment='separate', remainder='passthrough', get_statistics=True)</code>","text":"<p>Init the class.</p> <p>Parameters:</p> Name Type Description Default <code>n_bins</code> <code>int</code> <p>Number of bins to create.</p> <code>5</code> <code>variables</code> <code>list</code> <p>The features to bucket. Uses all features if not defined.</p> <code>[]</code> <code>specials</code> <p>(dict) of special values that require their own binning. The dictionary has the following format:  {\"\" : {\"name of special bucket\" : }} For every feature that needs a special value, a dictionary must be passed as value. This dictionary contains a name of a bucket (key) and an array of unique values that should be put in that bucket. When special values are defined, they are not considered in the fitting procedure. <code>{}</code> <code>missing_treatment</code> <p>Defines how we treat the missing values present in the data. If a string, it must be one of the following options:     separate: Missing values get put in a separate 'Other' bucket: <code>-1</code>     most_risky: Missing values are put into the bucket containing the largest percentage of Class 1.     least_risky: Missing values are put into the bucket containing the largest percentage of Class 0.     most_frequent: Missing values are put into the most common bucket.     neutral: Missing values are put into the bucket with WoE closest to 0.     similar: Missing values are put into the bucket with WoE closest to the bucket with only missing values.     passthrough: Leaves missing values untouched. If a dict, it must be of the following format:     {\"\": }     This bucket number is where we will put the missing values. <code>'separate'</code> <code>remainder</code> <p>How we want the non-specified columns to be transformed. It must be in [\"passthrough\", \"drop\"]. passthrough (Default): all columns that were not specified in \"variables\" will be passed through. drop: all remaining columns that were not specified in \"variables\" will be dropped.</p> <code>'passthrough'</code> Source code in <code>skorecard/bucketers/bucketers.py</code> <pre><code>def __init__(\n    self,\n    n_bins=5,\n    variables=[],\n    specials={},\n    missing_treatment=\"separate\",\n    remainder=\"passthrough\",\n    get_statistics=True,\n):\n    \"\"\"Init the class.\n\n    Args:\n        n_bins (int): Number of bins to create.\n        variables (list): The features to bucket. Uses all features if not defined.\n        specials: (dict) of special values that require their own binning.\n            The dictionary has the following format:\n             {\"&lt;column name&gt;\" : {\"name of special bucket\" : &lt;list with 1 or more values&gt;}}\n            For every feature that needs a special value, a dictionary must be passed as value.\n            This dictionary contains a name of a bucket (key) and an array of unique values that should be put\n            in that bucket.\n            When special values are defined, they are not considered in the fitting procedure.\n        missing_treatment: Defines how we treat the missing values present in the data.\n            If a string, it must be one of the following options:\n                separate: Missing values get put in a separate 'Other' bucket: `-1`\n                most_risky: Missing values are put into the bucket containing the largest percentage of Class 1.\n                least_risky: Missing values are put into the bucket containing the largest percentage of Class 0.\n                most_frequent: Missing values are put into the most common bucket.\n                neutral: Missing values are put into the bucket with WoE closest to 0.\n                similar: Missing values are put into the bucket with WoE closest to the bucket with only missing values.\n                passthrough: Leaves missing values untouched.\n            If a dict, it must be of the following format:\n                {\"&lt;column name&gt;\": &lt;bucket_number&gt;}\n                This bucket number is where we will put the missing values.\n        remainder: How we want the non-specified columns to be transformed. It must be in [\"passthrough\", \"drop\"].\n            passthrough (Default): all columns that were not specified in \"variables\" will be passed through.\n            drop: all remaining columns that were not specified in \"variables\" will be dropped.\n    \"\"\"  # noqa\n    self.missing_treatment = missing_treatment\n    self.variables = variables\n    self.n_bins = n_bins\n    self.specials = specials\n    self.remainder = remainder\n    self.get_statistics = get_statistics\n</code></pre>"},{"location":"api/bucketers/OptimalBucketer/","title":"OptimalBucketer","text":"<p>               Bases: <code>BaseBucketer</code></p> <p>The <code>OptimalBucketer</code> transformer uses the optbinning package to find optimal buckets.</p> <p>Support:  </p> <p>This bucketer basically wraps optbinning.OptimalBinning to be consistent with skorecard. Requires a feature to be pre-bucketed to max 100 buckets. Optbinning uses a constrained programming solver to merge buckets, taking into account the following constraints 1) monotonicity in bad rate, 2) at least 5% of records per bin.</p> <p>Example:</p> <pre><code>from skorecard import datasets\nfrom skorecard.bucketers import OptimalBucketer\n\nX, y = datasets.load_uci_credit_card(return_X_y=True)\nbucketer = OptimalBucketer(variables = ['LIMIT_BAL'])\nbucketer.fit_transform(X, y)\n</code></pre> Source code in <code>skorecard/bucketers/bucketers.py</code> <pre><code>class OptimalBucketer(BaseBucketer):\n    \"\"\"\n    The `OptimalBucketer` transformer uses the [optbinning](http://gnpalencia.org/optbinning) package to find optimal buckets.\n\n    Support: ![badge](https://img.shields.io/badge/numerical-true-green) ![badge](https://img.shields.io/badge/categorical-true-green) ![badge](https://img.shields.io/badge/supervised-true-green)\n\n    This bucketer basically wraps optbinning.OptimalBinning to be consistent with skorecard.\n    Requires a feature to be pre-bucketed to max 100 buckets.\n    Optbinning uses a constrained programming solver to merge buckets,\n    taking into account the following constraints 1) monotonicity in bad rate, 2) at least 5% of records per bin.\n\n    Example:\n\n    ```python\n    from skorecard import datasets\n    from skorecard.bucketers import OptimalBucketer\n\n    X, y = datasets.load_uci_credit_card(return_X_y=True)\n    bucketer = OptimalBucketer(variables = ['LIMIT_BAL'])\n    bucketer.fit_transform(X, y)\n    ```\n    \"\"\"  # noqa\n\n    def __init__(\n        self,\n        variables=[],\n        specials={},\n        variables_type=\"numerical\",\n        max_n_bins=10,\n        missing_treatment=\"separate\",\n        min_bin_size=0.05,\n        cat_cutoff=None,\n        time_limit=25,\n        remainder=\"passthrough\",\n        get_statistics=True,\n        solver=\"cp\",\n        monotonic_trend=\"auto_asc_desc\",\n        gamma=0,\n        ob_kwargs={},\n    ) -&gt; None:\n        \"\"\"Initialize Optimal Bucketer.\n\n        Args:\n            variables: List of variables to bucket.\n            specials: (nested) dictionary of special values that require their own binning.\n                The dictionary has the following format:\n                 {\"&lt;column name&gt;\" : {\"name of special bucket\" : &lt;list with 1 or more values&gt;}}\n                For every feature that needs a special value, a dictionary must be passed as value.\n                This dictionary contains a name of a bucket (key) and an array of unique values that should be put\n                in that bucket.\n                When special values are passed, they are not considered in the fitting procedure.\n            variables_type: Passed to [optbinning.OptimalBinning](http://gnpalencia.org/optbinning/binning_binary.html):\n                Type of the variables. Must be either 'categorical' or 'numerical'.\n            missing_treatment: Defines how we treat the missing values present in the data.\n                If a string, it must be one of the following options:\n                    separate: Missing values get put in a separate 'Other' bucket: `-1`\n                    most_risky: Missing values are put into the bucket containing the largest percentage of Class 1.\n                    least_risky: Missing values are put into the bucket containing the largest percentage of Class 0.\n                    most_frequent: Missing values are put into the most common bucket.\n                    neutral: Missing values are put into the bucket with WoE closest to 0.\n                    similar: Missing values are put into the bucket with WoE closest to the bucket with only missing values.\n                    passthrough: Leaves missing values untouched.\n                If a dict, it must be of the following format:\n                    {\"&lt;column name&gt;\": &lt;bucket_number&gt;}\n                    This bucket number is where we will put the missing values.\n            min_bin_size: Passed to [optbinning.OptimalBinning](http://gnpalencia.org/optbinning/binning_binary.html):\n                Minimum fraction of observations in a bucket.\n            max_n_bins: Passed to [optbinning.OptimalBinning](http://gnpalencia.org/optbinning/binning_binary.html):\n                Maximum numbers of bins to return.\n            cat_cutoff: Passed to [optbinning.OptimalBinning](http://gnpalencia.org/optbinning/binning_binary.html):\n                Threshold ratio (None, or &gt;0 and &lt;=1) below which categories are grouped\n                together in a bucket 'other'.\n            time_limit (float): Passed to [optbinning.OptimalBinning](http://gnpalencia.org/optbinning/binning_binary.html):\n                Time limit in seconds to find an optimal solution.\n            remainder: How we want the non-specified columns to be transformed. It must be in [\"passthrough\", \"drop\"].\n                passthrough (Default): all columns that were not specified in \"variables\" will be passed through.\n                drop: all remaining columns that were not specified in \"variables\" will be dropped.\n            solver (str): Passed to [optbinning.OptimalBinning](http://gnpalencia.org/optbinning/binning_binary.html): The optimizer to solve the optimal binning problem.\n                Supported solvers are \u201cmip\u201d to choose a mixed-integer programming solver, \u201ccp\u201d (default) to choose a constrained programming solver or \u201cls\u201d to choose LocalSolver.\n            monotonic_trend (str): Passed to [optbinning.OptimalBinning](http://gnpalencia.org/optbinning/binning_binary.html):\n                The event rate monotonic trend. Supported trends are \u201cauto\u201d, \u201cauto_heuristic\u201d and \u201cauto_asc_desc\u201d\n                to automatically determine the trend maximizing IV using a machine learning classifier,\n                \u201cascending\u201d, \u201cdescending\u201d, \u201cconcave\u201d, \u201cconvex\u201d, \u201cpeak\u201d and \u201cpeak_heuristic\u201d to allow a peak change point,\n                and \u201cvalley\u201d and \u201cvalley_heuristic\u201d to allow a valley change point.\n                Trends \u201cauto_heuristic\u201d, \u201cpeak_heuristic\u201d and \u201cvalley_heuristic\u201d use a heuristic to determine the change point,\n                and are significantly faster for large size instances (max_n_prebins &gt; 20).\n                Trend \u201cauto_asc_desc\u201d is used to automatically select the best monotonic trend\n                between \u201cascending\u201d and \u201cdescending\u201d. If None, then the monotonic constraint is disabled.\n            gamma (float): Passed to [optbinning.OptimalBinning](http://gnpalencia.org/optbinning/binning_binary.html):\n                Regularization strength to reduce the number of dominating bins.\n                Larger values specify stronger regularization. Default is 0.\n                Option supported by solvers \u201ccp\u201d and \u201cmip\u201d.\n            ob_kwargs (dict): Other parameters passed to [optbinning.OptimalBinning](http://gnpalencia.org/optbinning/binning_binary.html).\n        \"\"\"  # noqa\n        self.variables = variables\n        self.specials = specials\n        self.variables_type = variables_type\n        self.max_n_bins = max_n_bins\n        self.missing_treatment = missing_treatment\n        self.min_bin_size = min_bin_size\n        self.cat_cutoff = cat_cutoff\n        self.time_limit = time_limit\n        self.remainder = remainder\n        self.get_statistics = get_statistics\n        self.solver = solver\n        self.monotonic_trend = monotonic_trend\n        self.gamma = gamma\n        self.ob_kwargs = ob_kwargs\n\n        check_args(ob_kwargs, OptimalBinning)\n\n    def _get_feature_splits(self, feature, X, y, X_unfiltered=None):\n        \"\"\"\n        Finds the splits for a single feature.\n\n        X and y have already been preprocessed, and have specials removed.\n\n        Args:\n            feature (str): Name of the feature.\n            X (pd.Series): df with single column of feature to bucket\n            y (np.ndarray): array with target\n            X_unfiltered (pd.Series): df with single column of feature to bucket before any filtering was applied\n\n        Returns:\n            splits, right (tuple): The splits (dict or array), and whether right=True or False.\n        \"\"\"\n        # Normally Optbinning uses a DecisionTreeBucketer to do automatic prebinning\n        # We require the user to pre-bucket explicitly before using this.\n        if self.variables_type == \"numerical\":\n            uniq_values = np.sort(np.unique(X.values))\n            if len(uniq_values) &gt; 100:\n                raise NotPreBucketedError(\n                    f\"\"\"\n                    OptimalBucketer requires numerical feature '{feature}' to be pre-bucketed\n                    to max 100 unique values (for performance reasons).\n                    Currently there are {len(uniq_values)} unique values present.\n\n                    Apply pre-binning, f.e. with skorecard.bucketers.DecisionTreeBucketer.\n                    \"\"\"\n                )\n            user_splits = uniq_values\n        else:\n            user_splits = None\n\n        # Fit estimator\n        binner = OptimalBinning(\n            name=str(feature),\n            dtype=self.variables_type,\n            solver=self.solver,\n            monotonic_trend=self.monotonic_trend,\n            gamma=self.gamma,\n            # On user_splits:\n            # We want skorecard users to explicitly define pre-binning for numerical features\n            # Setting the user_splits prevents OptimalBinning from doing pre-binning again.\n            user_splits=user_splits,\n            min_bin_size=self.min_bin_size,\n            max_n_bins=self.max_n_bins,\n            cat_cutoff=self.cat_cutoff,\n            time_limit=self.time_limit,\n            **self.ob_kwargs,\n        )\n        binner.fit(X.values, y)\n\n        # Extract fitted boundaries\n        if self.variables_type == \"categorical\":\n            splits = {}\n            for bucket_nr, values in enumerate(binner.splits):\n                for value in values:\n                    splits[value] = bucket_nr\n        else:\n            splits = binner.splits\n\n        # Note that optbinning transform uses right=False\n        # https://github.com/guillermo-navas-palencia/optbinning/blob/396b9bed97581094167c9eb4744c2fd1fb5c7408/optbinning/binning/transformations.py#L126-L132\n        return (splits, False)\n</code></pre>"},{"location":"api/bucketers/OptimalBucketer/#skorecard.bucketers.OptimalBucketer.__init__","title":"<code>__init__(variables=[], specials={}, variables_type='numerical', max_n_bins=10, missing_treatment='separate', min_bin_size=0.05, cat_cutoff=None, time_limit=25, remainder='passthrough', get_statistics=True, solver='cp', monotonic_trend='auto_asc_desc', gamma=0, ob_kwargs={})</code>","text":"<p>Initialize Optimal Bucketer.</p> <p>Parameters:</p> Name Type Description Default <code>variables</code> <p>List of variables to bucket.</p> <code>[]</code> <code>specials</code> <p>(nested) dictionary of special values that require their own binning. The dictionary has the following format:  {\"\" : {\"name of special bucket\" : }} For every feature that needs a special value, a dictionary must be passed as value. This dictionary contains a name of a bucket (key) and an array of unique values that should be put in that bucket. When special values are passed, they are not considered in the fitting procedure. <code>{}</code> <code>variables_type</code> <p>Passed to optbinning.OptimalBinning: Type of the variables. Must be either 'categorical' or 'numerical'.</p> <code>'numerical'</code> <code>missing_treatment</code> <p>Defines how we treat the missing values present in the data. If a string, it must be one of the following options:     separate: Missing values get put in a separate 'Other' bucket: <code>-1</code>     most_risky: Missing values are put into the bucket containing the largest percentage of Class 1.     least_risky: Missing values are put into the bucket containing the largest percentage of Class 0.     most_frequent: Missing values are put into the most common bucket.     neutral: Missing values are put into the bucket with WoE closest to 0.     similar: Missing values are put into the bucket with WoE closest to the bucket with only missing values.     passthrough: Leaves missing values untouched. If a dict, it must be of the following format:     {\"\": }     This bucket number is where we will put the missing values. <code>'separate'</code> <code>min_bin_size</code> <p>Passed to optbinning.OptimalBinning: Minimum fraction of observations in a bucket.</p> <code>0.05</code> <code>max_n_bins</code> <p>Passed to optbinning.OptimalBinning: Maximum numbers of bins to return.</p> <code>10</code> <code>cat_cutoff</code> <p>Passed to optbinning.OptimalBinning: Threshold ratio (None, or &gt;0 and &lt;=1) below which categories are grouped together in a bucket 'other'.</p> <code>None</code> <code>time_limit</code> <code>float</code> <p>Passed to optbinning.OptimalBinning: Time limit in seconds to find an optimal solution.</p> <code>25</code> <code>remainder</code> <p>How we want the non-specified columns to be transformed. It must be in [\"passthrough\", \"drop\"]. passthrough (Default): all columns that were not specified in \"variables\" will be passed through. drop: all remaining columns that were not specified in \"variables\" will be dropped.</p> <code>'passthrough'</code> <code>solver</code> <code>str</code> <p>Passed to optbinning.OptimalBinning: The optimizer to solve the optimal binning problem. Supported solvers are \u201cmip\u201d to choose a mixed-integer programming solver, \u201ccp\u201d (default) to choose a constrained programming solver or \u201cls\u201d to choose LocalSolver.</p> <code>'cp'</code> <code>monotonic_trend</code> <code>str</code> <p>Passed to optbinning.OptimalBinning: The event rate monotonic trend. Supported trends are \u201cauto\u201d, \u201cauto_heuristic\u201d and \u201cauto_asc_desc\u201d to automatically determine the trend maximizing IV using a machine learning classifier, \u201cascending\u201d, \u201cdescending\u201d, \u201cconcave\u201d, \u201cconvex\u201d, \u201cpeak\u201d and \u201cpeak_heuristic\u201d to allow a peak change point, and \u201cvalley\u201d and \u201cvalley_heuristic\u201d to allow a valley change point. Trends \u201cauto_heuristic\u201d, \u201cpeak_heuristic\u201d and \u201cvalley_heuristic\u201d use a heuristic to determine the change point, and are significantly faster for large size instances (max_n_prebins &gt; 20). Trend \u201cauto_asc_desc\u201d is used to automatically select the best monotonic trend between \u201cascending\u201d and \u201cdescending\u201d. If None, then the monotonic constraint is disabled.</p> <code>'auto_asc_desc'</code> <code>gamma</code> <code>float</code> <p>Passed to optbinning.OptimalBinning: Regularization strength to reduce the number of dominating bins. Larger values specify stronger regularization. Default is 0. Option supported by solvers \u201ccp\u201d and \u201cmip\u201d.</p> <code>0</code> <code>ob_kwargs</code> <code>dict</code> <p>Other parameters passed to optbinning.OptimalBinning.</p> <code>{}</code> Source code in <code>skorecard/bucketers/bucketers.py</code> <pre><code>def __init__(\n    self,\n    variables=[],\n    specials={},\n    variables_type=\"numerical\",\n    max_n_bins=10,\n    missing_treatment=\"separate\",\n    min_bin_size=0.05,\n    cat_cutoff=None,\n    time_limit=25,\n    remainder=\"passthrough\",\n    get_statistics=True,\n    solver=\"cp\",\n    monotonic_trend=\"auto_asc_desc\",\n    gamma=0,\n    ob_kwargs={},\n) -&gt; None:\n    \"\"\"Initialize Optimal Bucketer.\n\n    Args:\n        variables: List of variables to bucket.\n        specials: (nested) dictionary of special values that require their own binning.\n            The dictionary has the following format:\n             {\"&lt;column name&gt;\" : {\"name of special bucket\" : &lt;list with 1 or more values&gt;}}\n            For every feature that needs a special value, a dictionary must be passed as value.\n            This dictionary contains a name of a bucket (key) and an array of unique values that should be put\n            in that bucket.\n            When special values are passed, they are not considered in the fitting procedure.\n        variables_type: Passed to [optbinning.OptimalBinning](http://gnpalencia.org/optbinning/binning_binary.html):\n            Type of the variables. Must be either 'categorical' or 'numerical'.\n        missing_treatment: Defines how we treat the missing values present in the data.\n            If a string, it must be one of the following options:\n                separate: Missing values get put in a separate 'Other' bucket: `-1`\n                most_risky: Missing values are put into the bucket containing the largest percentage of Class 1.\n                least_risky: Missing values are put into the bucket containing the largest percentage of Class 0.\n                most_frequent: Missing values are put into the most common bucket.\n                neutral: Missing values are put into the bucket with WoE closest to 0.\n                similar: Missing values are put into the bucket with WoE closest to the bucket with only missing values.\n                passthrough: Leaves missing values untouched.\n            If a dict, it must be of the following format:\n                {\"&lt;column name&gt;\": &lt;bucket_number&gt;}\n                This bucket number is where we will put the missing values.\n        min_bin_size: Passed to [optbinning.OptimalBinning](http://gnpalencia.org/optbinning/binning_binary.html):\n            Minimum fraction of observations in a bucket.\n        max_n_bins: Passed to [optbinning.OptimalBinning](http://gnpalencia.org/optbinning/binning_binary.html):\n            Maximum numbers of bins to return.\n        cat_cutoff: Passed to [optbinning.OptimalBinning](http://gnpalencia.org/optbinning/binning_binary.html):\n            Threshold ratio (None, or &gt;0 and &lt;=1) below which categories are grouped\n            together in a bucket 'other'.\n        time_limit (float): Passed to [optbinning.OptimalBinning](http://gnpalencia.org/optbinning/binning_binary.html):\n            Time limit in seconds to find an optimal solution.\n        remainder: How we want the non-specified columns to be transformed. It must be in [\"passthrough\", \"drop\"].\n            passthrough (Default): all columns that were not specified in \"variables\" will be passed through.\n            drop: all remaining columns that were not specified in \"variables\" will be dropped.\n        solver (str): Passed to [optbinning.OptimalBinning](http://gnpalencia.org/optbinning/binning_binary.html): The optimizer to solve the optimal binning problem.\n            Supported solvers are \u201cmip\u201d to choose a mixed-integer programming solver, \u201ccp\u201d (default) to choose a constrained programming solver or \u201cls\u201d to choose LocalSolver.\n        monotonic_trend (str): Passed to [optbinning.OptimalBinning](http://gnpalencia.org/optbinning/binning_binary.html):\n            The event rate monotonic trend. Supported trends are \u201cauto\u201d, \u201cauto_heuristic\u201d and \u201cauto_asc_desc\u201d\n            to automatically determine the trend maximizing IV using a machine learning classifier,\n            \u201cascending\u201d, \u201cdescending\u201d, \u201cconcave\u201d, \u201cconvex\u201d, \u201cpeak\u201d and \u201cpeak_heuristic\u201d to allow a peak change point,\n            and \u201cvalley\u201d and \u201cvalley_heuristic\u201d to allow a valley change point.\n            Trends \u201cauto_heuristic\u201d, \u201cpeak_heuristic\u201d and \u201cvalley_heuristic\u201d use a heuristic to determine the change point,\n            and are significantly faster for large size instances (max_n_prebins &gt; 20).\n            Trend \u201cauto_asc_desc\u201d is used to automatically select the best monotonic trend\n            between \u201cascending\u201d and \u201cdescending\u201d. If None, then the monotonic constraint is disabled.\n        gamma (float): Passed to [optbinning.OptimalBinning](http://gnpalencia.org/optbinning/binning_binary.html):\n            Regularization strength to reduce the number of dominating bins.\n            Larger values specify stronger regularization. Default is 0.\n            Option supported by solvers \u201ccp\u201d and \u201cmip\u201d.\n        ob_kwargs (dict): Other parameters passed to [optbinning.OptimalBinning](http://gnpalencia.org/optbinning/binning_binary.html).\n    \"\"\"  # noqa\n    self.variables = variables\n    self.specials = specials\n    self.variables_type = variables_type\n    self.max_n_bins = max_n_bins\n    self.missing_treatment = missing_treatment\n    self.min_bin_size = min_bin_size\n    self.cat_cutoff = cat_cutoff\n    self.time_limit = time_limit\n    self.remainder = remainder\n    self.get_statistics = get_statistics\n    self.solver = solver\n    self.monotonic_trend = monotonic_trend\n    self.gamma = gamma\n    self.ob_kwargs = ob_kwargs\n\n    check_args(ob_kwargs, OptimalBinning)\n</code></pre>"},{"location":"api/bucketers/OrdinalCategoricalBucketer/","title":"OrdinalCategoricalBucketer","text":"<p>               Bases: <code>BaseBucketer</code></p> <p>The <code>OrdinalCategoricalBucketer</code> replaces categories by ordinal numbers.</p> <p>Support  </p> <p>When <code>sort_by_target</code> is <code>false</code> the buckets are assigned in order of frequency. When <code>sort_by_target</code> is <code>true</code> the buckets are ordered based on the mean of the target per category.</p> <p>For example, if for a variable <code>colour</code> the means of the target for <code>blue</code>, <code>red</code> and <code>grey</code> is <code>0.5</code>, <code>0.8</code> and <code>0.1</code> respectively, <code>grey</code> will be the first bucket (<code>0</code>), blue the second (<code>1</code>) and <code>red</code> the third (<code>3</code>). If new data contains unknown labels (f.e. yellow), they will be replaced by the 'Other' bucket (<code>-2</code>), and if new data contains missing values, they will be replaced by the 'Missing' bucket (<code>-1</code>).</p> <p>Example:</p> <pre><code>from skorecard import datasets\nfrom skorecard.bucketers import OrdinalCategoricalBucketer\n\nX, y = datasets.load_uci_credit_card(return_X_y=True)\nbucketer = OrdinalCategoricalBucketer(variables=['EDUCATION'])\nbucketer.fit_transform(X, y)\nbucketer = OrdinalCategoricalBucketer(max_n_categories=2, variables=['EDUCATION'])\nbucketer.fit_transform(X, y)\n</code></pre> <p>Credits: Code &amp; ideas adapted from:</p> <ul> <li>feature_engine.categorical_encoders.OrdinalCategoricalEncoder</li> <li>feature_engine.categorical_encoders.RareLabelCategoricalEncoder</li> </ul> Source code in <code>skorecard/bucketers/bucketers.py</code> <pre><code>class OrdinalCategoricalBucketer(BaseBucketer):\n    \"\"\"\n    The `OrdinalCategoricalBucketer` replaces categories by ordinal numbers.\n\n    Support ![badge](https://img.shields.io/badge/numerical-false-red) ![badge](https://img.shields.io/badge/categorical-true-green) ![badge](https://img.shields.io/badge/supervised-true-green)\n\n    When `sort_by_target` is `false` the buckets are assigned in order of frequency.\n    When `sort_by_target` is `true` the buckets are ordered based on the mean of the target per category.\n\n    For example, if for a variable `colour` the means of the target\n    for `blue`, `red` and `grey` is `0.5`, `0.8` and `0.1` respectively,\n    `grey` will be the first bucket (`0`), blue the second (`1`) and\n    `red` the third (`3`). If new data contains unknown labels (f.e. yellow),\n    they will be replaced by the 'Other' bucket (`-2`),\n    and if new data contains missing values, they will be replaced by the 'Missing' bucket (`-1`).\n\n    Example:\n\n    ```python\n    from skorecard import datasets\n    from skorecard.bucketers import OrdinalCategoricalBucketer\n\n    X, y = datasets.load_uci_credit_card(return_X_y=True)\n    bucketer = OrdinalCategoricalBucketer(variables=['EDUCATION'])\n    bucketer.fit_transform(X, y)\n    bucketer = OrdinalCategoricalBucketer(max_n_categories=2, variables=['EDUCATION'])\n    bucketer.fit_transform(X, y)\n    ```\n\n    Credits: Code &amp; ideas adapted from:\n\n    - feature_engine.categorical_encoders.OrdinalCategoricalEncoder\n    - feature_engine.categorical_encoders.RareLabelCategoricalEncoder\n\n    \"\"\"  # noqa\n\n    def __init__(\n        self,\n        tol=0.05,\n        max_n_categories=None,\n        variables=[],\n        specials={},\n        encoding_method=\"frequency\",\n        missing_treatment=\"separate\",\n        remainder=\"passthrough\",\n        get_statistics=True,\n    ):\n        \"\"\"\n        Init the class.\n\n        Args:\n            tol (float): the minimum frequency a label should have to be considered frequent.\n                Categories with frequencies lower than tol will be grouped together (in the 'other' bucket).\n            max_n_categories (int): the maximum number of categories that should be considered frequent.\n                If None, all categories with frequency above the tolerance (tol) will be\n                considered.\n            variables (list): The features to bucket. Uses all features if not defined.\n            specials (dict): (nested) dictionary of special values that require their own binning.\n                The dictionary has the following format:\n                 {\"&lt;column name&gt;\" : {\"name of special bucket\" : &lt;list with 1 or more values&gt;}}\n                For every feature that needs a special value, a dictionary must be passed as value.\n                This dictionary contains a name of a bucket (key) and an array of unique values that should be put\n                in that bucket.\n                When special values are defined, they are not considered in the fitting procedure.\n            encoding_method (string): encoding method.\n                - \"frequency\" (default): orders the buckets based on the frequency of observations in the bucket.\n                    The lower the number of the bucket the most frequent are the observations in that bucket.\n                - \"ordered\": orders the buckets based on the average class 1 rate in the bucket.\n                    The lower the number of the bucket the lower the fraction of class 1 in that bucket.\n            missing_treatment (str or dict): Defines how we treat the missing values present in the data.\n                If a string, it must be one of the following options:\n                    separate: Missing values get put in a separate 'Other' bucket: `-1`\n                    most_risky: Missing values are put into the bucket containing the largest percentage of Class 1.\n                    least_risky: Missing values are put into the bucket containing the largest percentage of Class 0.\n                    most_frequent: Missing values are put into the most common bucket.\n                    neutral: Missing values are put into the bucket with WoE closest to 0.\n                    similar: Missing values are put into the bucket with WoE closest to the bucket with only missing values.\n                    passthrough: Leaves missing values untouched.\n                If a dict, it must be of the following format:\n                    {\"&lt;column name&gt;\": &lt;bucket_number&gt;}\n                    This bucket number is where we will put the missing values.\n            remainder (str): How we want the non-specified columns to be transformed. It must be in [\"passthrough\", \"drop\"].\n                passthrough (Default): all columns that were not specified in \"variables\" will be passed through.\n                drop: all remaining columns that were not specified in \"variables\" will be dropped.\n        \"\"\"  # noqa\n        self.tol = tol\n        self.max_n_categories = max_n_categories\n        self.variables = variables\n        self.specials = specials\n        self.encoding_method = encoding_method\n        self.missing_treatment = missing_treatment\n        self.remainder = remainder\n        self.get_statistics = get_statistics\n\n    @property\n    def variables_type(self):\n        \"\"\"\n        Signals variables type supported by this bucketer.\n        \"\"\"\n        return \"categorical\"\n\n    def _get_feature_splits(self, feature, X, y, X_unfiltered=None):\n        \"\"\"\n        Finds the splits for a single feature.\n\n        X and y have already been preprocessed, and have specials removed.\n\n        Args:\n            feature (str): Name of the feature.\n            X (pd.Series): df with single column of feature to bucket\n            y (np.ndarray): array with target\n            X_unfiltered (pd.Series): df with single column of feature to bucket before any filtering was applied\n\n        Returns:\n            splits, right (tuple): The splits (dict or array), and whether right=True or False.\n        \"\"\"\n        normalized_counts = None\n\n        if y is None:\n            y = pd.Series(None)\n        elif not (isinstance(y, pd.Series) or isinstance(y, pd.DataFrame)):\n            y = pd.Series(y)\n        else:\n            raise AssertionError(\"something wrong with format of y\")\n\n        X_y = pd.concat([X, y], axis=1)\n        X_y.columns = [feature, \"target\"]\n\n        if self.encoding_method == \"ordered\":\n            if y is None:\n                raise ValueError(\"To use encoding_method=='ordered', y cannot be None.\")\n\n            normalized_counts = X_y[feature].value_counts(normalize=True)\n            cats = X_y.groupby([feature])[\"target\"].mean().sort_values(ascending=True).index\n            normalized_counts = normalized_counts[cats]\n\n        elif self.encoding_method == \"frequency\":\n            normalized_counts = X_y[feature].value_counts(normalize=True)\n\n        # Limit number of categories if set.\n        normalized_counts = normalized_counts[: self.max_n_categories]\n        # Remove less frequent categories\n        normalized_counts = normalized_counts[normalized_counts &gt;= self.tol]\n\n        # Determine Ordinal Encoder based on ordered labels\n        # Note we start at 1, to be able to encode missings as 0.\n        mapping = dict(zip(normalized_counts.index, range(0, len(normalized_counts))))\n\n        # Note that right is set to True, but this is not used at all for categoricals\n        return (mapping, True)\n</code></pre>"},{"location":"api/bucketers/OrdinalCategoricalBucketer/#skorecard.bucketers.OrdinalCategoricalBucketer.variables_type","title":"<code>variables_type</code>  <code>property</code>","text":"<p>Signals variables type supported by this bucketer.</p>"},{"location":"api/bucketers/OrdinalCategoricalBucketer/#skorecard.bucketers.OrdinalCategoricalBucketer.__init__","title":"<code>__init__(tol=0.05, max_n_categories=None, variables=[], specials={}, encoding_method='frequency', missing_treatment='separate', remainder='passthrough', get_statistics=True)</code>","text":"<p>Init the class.</p> <p>Parameters:</p> Name Type Description Default <code>tol</code> <code>float</code> <p>the minimum frequency a label should have to be considered frequent. Categories with frequencies lower than tol will be grouped together (in the 'other' bucket).</p> <code>0.05</code> <code>max_n_categories</code> <code>int</code> <p>the maximum number of categories that should be considered frequent. If None, all categories with frequency above the tolerance (tol) will be considered.</p> <code>None</code> <code>variables</code> <code>list</code> <p>The features to bucket. Uses all features if not defined.</p> <code>[]</code> <code>specials</code> <code>dict</code> <p>(nested) dictionary of special values that require their own binning. The dictionary has the following format:  {\"\" : {\"name of special bucket\" : }} For every feature that needs a special value, a dictionary must be passed as value. This dictionary contains a name of a bucket (key) and an array of unique values that should be put in that bucket. When special values are defined, they are not considered in the fitting procedure. <code>{}</code> <code>encoding_method</code> <code>string</code> <p>encoding method. - \"frequency\" (default): orders the buckets based on the frequency of observations in the bucket.     The lower the number of the bucket the most frequent are the observations in that bucket. - \"ordered\": orders the buckets based on the average class 1 rate in the bucket.     The lower the number of the bucket the lower the fraction of class 1 in that bucket.</p> <code>'frequency'</code> <code>missing_treatment</code> <code>str or dict</code> <p>Defines how we treat the missing values present in the data. If a string, it must be one of the following options:     separate: Missing values get put in a separate 'Other' bucket: <code>-1</code>     most_risky: Missing values are put into the bucket containing the largest percentage of Class 1.     least_risky: Missing values are put into the bucket containing the largest percentage of Class 0.     most_frequent: Missing values are put into the most common bucket.     neutral: Missing values are put into the bucket with WoE closest to 0.     similar: Missing values are put into the bucket with WoE closest to the bucket with only missing values.     passthrough: Leaves missing values untouched. If a dict, it must be of the following format:     {\"\": }     This bucket number is where we will put the missing values. <code>'separate'</code> <code>remainder</code> <code>str</code> <p>How we want the non-specified columns to be transformed. It must be in [\"passthrough\", \"drop\"]. passthrough (Default): all columns that were not specified in \"variables\" will be passed through. drop: all remaining columns that were not specified in \"variables\" will be dropped.</p> <code>'passthrough'</code> Source code in <code>skorecard/bucketers/bucketers.py</code> <pre><code>def __init__(\n    self,\n    tol=0.05,\n    max_n_categories=None,\n    variables=[],\n    specials={},\n    encoding_method=\"frequency\",\n    missing_treatment=\"separate\",\n    remainder=\"passthrough\",\n    get_statistics=True,\n):\n    \"\"\"\n    Init the class.\n\n    Args:\n        tol (float): the minimum frequency a label should have to be considered frequent.\n            Categories with frequencies lower than tol will be grouped together (in the 'other' bucket).\n        max_n_categories (int): the maximum number of categories that should be considered frequent.\n            If None, all categories with frequency above the tolerance (tol) will be\n            considered.\n        variables (list): The features to bucket. Uses all features if not defined.\n        specials (dict): (nested) dictionary of special values that require their own binning.\n            The dictionary has the following format:\n             {\"&lt;column name&gt;\" : {\"name of special bucket\" : &lt;list with 1 or more values&gt;}}\n            For every feature that needs a special value, a dictionary must be passed as value.\n            This dictionary contains a name of a bucket (key) and an array of unique values that should be put\n            in that bucket.\n            When special values are defined, they are not considered in the fitting procedure.\n        encoding_method (string): encoding method.\n            - \"frequency\" (default): orders the buckets based on the frequency of observations in the bucket.\n                The lower the number of the bucket the most frequent are the observations in that bucket.\n            - \"ordered\": orders the buckets based on the average class 1 rate in the bucket.\n                The lower the number of the bucket the lower the fraction of class 1 in that bucket.\n        missing_treatment (str or dict): Defines how we treat the missing values present in the data.\n            If a string, it must be one of the following options:\n                separate: Missing values get put in a separate 'Other' bucket: `-1`\n                most_risky: Missing values are put into the bucket containing the largest percentage of Class 1.\n                least_risky: Missing values are put into the bucket containing the largest percentage of Class 0.\n                most_frequent: Missing values are put into the most common bucket.\n                neutral: Missing values are put into the bucket with WoE closest to 0.\n                similar: Missing values are put into the bucket with WoE closest to the bucket with only missing values.\n                passthrough: Leaves missing values untouched.\n            If a dict, it must be of the following format:\n                {\"&lt;column name&gt;\": &lt;bucket_number&gt;}\n                This bucket number is where we will put the missing values.\n        remainder (str): How we want the non-specified columns to be transformed. It must be in [\"passthrough\", \"drop\"].\n            passthrough (Default): all columns that were not specified in \"variables\" will be passed through.\n            drop: all remaining columns that were not specified in \"variables\" will be dropped.\n    \"\"\"  # noqa\n    self.tol = tol\n    self.max_n_categories = max_n_categories\n    self.variables = variables\n    self.specials = specials\n    self.encoding_method = encoding_method\n    self.missing_treatment = missing_treatment\n    self.remainder = remainder\n    self.get_statistics = get_statistics\n</code></pre>"},{"location":"api/bucketers/UserInputBucketer/","title":"UserInputBucketer","text":"<p>               Bases: <code>BaseBucketer</code></p> <p>The <code>UserInputBucketer</code> transformer creates buckets by implementing user-defined boundaries.</p> <p>Support:  </p> <p>This is a special bucketer that is not fitted but rather relies on pre-defined user input. The most common use-case is loading bucket mapping information previously fitted by other bucketers.</p> <p>Example:</p> <pre><code>from skorecard import datasets\nfrom skorecard.bucketers import AgglomerativeClusteringBucketer, UserInputBucketer\n\nX, y = datasets.load_uci_credit_card(return_X_y=True)\n\nac_bucketer = AgglomerativeClusteringBucketer(n_bins=3, variables=['LIMIT_BAL'])\nac_bucketer.fit(X)\nmapping = ac_bucketer.features_bucket_mapping_\n\nui_bucketer = UserInputBucketer(mapping)\nnew_X = ui_bucketer.fit_transform(X)\nassert len(new_X['LIMIT_BAL'].unique()) == 3\n\n#Map some values to the special buckets\nspecials = {\n    \"LIMIT_BAL\":{\n        \"=50000\":[50000],\n        \"in [20001,30000]\":[20000,30000],\n        }\n}\n\nac_bucketer = AgglomerativeClusteringBucketer(n_bins=3, variables=['LIMIT_BAL'], specials = specials)\nac_bucketer.fit(X)\nmapping = ac_bucketer.features_bucket_mapping_\n\nui_bucketer = UserInputBucketer(mapping)\nnew_X = ui_bucketer.fit_transform(X)\nassert len(new_X['LIMIT_BAL'].unique()) == 5\n</code></pre> Source code in <code>skorecard/bucketers/bucketers.py</code> <pre><code>class UserInputBucketer(BaseBucketer):\n    \"\"\"\n    The `UserInputBucketer` transformer creates buckets by implementing user-defined boundaries.\n\n    Support: ![badge](https://img.shields.io/badge/numerical-true-green) ![badge](https://img.shields.io/badge/categorical-true-green) ![badge](https://img.shields.io/badge/supervised-false-blue)\n\n    This is a special bucketer that is not fitted but rather relies\n    on pre-defined user input. The most common use-case is loading\n    bucket mapping information previously fitted by other bucketers.\n\n    Example:\n\n    ```python\n    from skorecard import datasets\n    from skorecard.bucketers import AgglomerativeClusteringBucketer, UserInputBucketer\n\n    X, y = datasets.load_uci_credit_card(return_X_y=True)\n\n    ac_bucketer = AgglomerativeClusteringBucketer(n_bins=3, variables=['LIMIT_BAL'])\n    ac_bucketer.fit(X)\n    mapping = ac_bucketer.features_bucket_mapping_\n\n    ui_bucketer = UserInputBucketer(mapping)\n    new_X = ui_bucketer.fit_transform(X)\n    assert len(new_X['LIMIT_BAL'].unique()) == 3\n\n    #Map some values to the special buckets\n    specials = {\n        \"LIMIT_BAL\":{\n            \"=50000\":[50000],\n            \"in [20001,30000]\":[20000,30000],\n            }\n    }\n\n    ac_bucketer = AgglomerativeClusteringBucketer(n_bins=3, variables=['LIMIT_BAL'], specials = specials)\n    ac_bucketer.fit(X)\n    mapping = ac_bucketer.features_bucket_mapping_\n\n    ui_bucketer = UserInputBucketer(mapping)\n    new_X = ui_bucketer.fit_transform(X)\n    assert len(new_X['LIMIT_BAL'].unique()) == 5\n    ```\n\n    \"\"\"  # noqa\n\n    def __init__(\n        self, features_bucket_mapping=None, variables: List = [], remainder=\"passthrough\", get_statistics=True\n    ) -&gt; None:\n        \"\"\"\n        Initialise the user-defined boundaries with a dictionary.\n\n        Notes:\n        - features_bucket_mapping is stored without the trailing underscore (_) because it is not fitted.\n\n        Args:\n            features_bucket_mapping (None, Dict, FeaturesBucketMapping, str or Path): Contains the feature name and boundaries\n                defined for this feature.\n                If a dict, it will be converted to an internal FeaturesBucketMapping object.\n                If a string or path, which will attempt to load the file as a yaml and convert to FeaturesBucketMapping object.\n            variables (list): The features to bucket. Uses all features in features_bucket_mapping if not defined.\n            remainder (str): How we want the non-specified columns to be transformed. It must be in [\"passthrough\", \"drop\"].\n                passthrough (Default): all columns that were not specified in \"variables\" will be passed through.\n                drop: all remaining columns that were not specified in \"variables\" will be dropped.\n        \"\"\"  # noqa\n        # Assigning the variable in the init to the attribute with the same name is a requirement of\n        # sklearn.base.BaseEstimator. See the notes in\n        # https://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html#sklearn.base.BaseEstimator\n        self.features_bucket_mapping = features_bucket_mapping\n        self.remainder = remainder\n        self.get_statistics = get_statistics\n\n        self.variables = variables\n\n        if features_bucket_mapping is None:\n            self.features_bucket_mapping_ = FeaturesBucketMapping()\n        elif isinstance(features_bucket_mapping, str):\n            buckets_yaml = yaml.safe_load(open(features_bucket_mapping))\n            self.features_bucket_mapping_ = FeaturesBucketMapping(buckets_yaml)\n        elif isinstance(features_bucket_mapping, dict):\n            self.features_bucket_mapping_ = FeaturesBucketMapping(features_bucket_mapping)\n        elif isinstance(features_bucket_mapping, FeaturesBucketMapping):\n            self.features_bucket_mapping_ = features_bucket_mapping\n        else:\n            try:\n                buckets_yaml = yaml.safe_load(features_bucket_mapping)\n                self.features_bucket_mapping_ = FeaturesBucketMapping(buckets_yaml)\n            except Exception:\n                raise TypeError(\n                    \"'features_bucket_mapping' must be a None, dict, str, path, or FeaturesBucketMapping instance\"\n                )\n\n    def fit(self, X, y=None):\n        \"\"\"Init the class.\"\"\"\n        X = ensure_dataframe(X)\n        if y is not None:\n            assert len(y) == X.shape[0], \"y and X not same length\"\n            # Store the classes seen during fit\n            self.classes_ = unique_labels(y)\n\n        # scikit-learn requires checking that X has same shape on transform\n        # this is because scikit-learn is still positional based (no column names used)\n        self.n_train_features_ = X.shape[1]\n\n        # bucket tables can only be computed on fit().\n        # so a user will have to .fit() if she/he wants .plot_buckets() and .bucket_table()\n        self.bucket_tables_ = {}\n\n        # and if user did not specify any variables\n        # use all the variables defined in the features_bucket_mapping\n        if self.variables == []:\n            self.variables_ = list(self.features_bucket_mapping_.maps.keys())\n\n        for feature in self.variables_:\n            # Calculate the bucket table\n            self.bucket_tables_[feature] = build_bucket_table(\n                X,\n                y,\n                column=feature,\n                bucket_mapping=self.features_bucket_mapping_.get(feature),\n            )\n\n        self._generate_summary(X, y)\n\n        return self\n\n    def _more_tags(self):\n        \"\"\"\n        Estimator tags are annotations of estimators that allow programmatic inspection of their capabilities.\n\n        See https://scikit-learn.org/stable/developers/develop.html#estimator-tags\n        \"\"\"  # noqa\n        return {\"binary_only\": True, \"allow_nan\": True, \"requires_fit\": False}\n</code></pre>"},{"location":"api/bucketers/UserInputBucketer/#skorecard.bucketers.UserInputBucketer.__init__","title":"<code>__init__(features_bucket_mapping=None, variables=[], remainder='passthrough', get_statistics=True)</code>","text":"<p>Initialise the user-defined boundaries with a dictionary.</p> <p>Notes: - features_bucket_mapping is stored without the trailing underscore (_) because it is not fitted.</p> <p>Parameters:</p> Name Type Description Default <code>features_bucket_mapping</code> <code>(None, Dict, FeaturesBucketMapping, str or Path)</code> <p>Contains the feature name and boundaries defined for this feature. If a dict, it will be converted to an internal FeaturesBucketMapping object. If a string or path, which will attempt to load the file as a yaml and convert to FeaturesBucketMapping object.</p> <code>None</code> <code>variables</code> <code>list</code> <p>The features to bucket. Uses all features in features_bucket_mapping if not defined.</p> <code>[]</code> <code>remainder</code> <code>str</code> <p>How we want the non-specified columns to be transformed. It must be in [\"passthrough\", \"drop\"]. passthrough (Default): all columns that were not specified in \"variables\" will be passed through. drop: all remaining columns that were not specified in \"variables\" will be dropped.</p> <code>'passthrough'</code> Source code in <code>skorecard/bucketers/bucketers.py</code> <pre><code>def __init__(\n    self, features_bucket_mapping=None, variables: List = [], remainder=\"passthrough\", get_statistics=True\n) -&gt; None:\n    \"\"\"\n    Initialise the user-defined boundaries with a dictionary.\n\n    Notes:\n    - features_bucket_mapping is stored without the trailing underscore (_) because it is not fitted.\n\n    Args:\n        features_bucket_mapping (None, Dict, FeaturesBucketMapping, str or Path): Contains the feature name and boundaries\n            defined for this feature.\n            If a dict, it will be converted to an internal FeaturesBucketMapping object.\n            If a string or path, which will attempt to load the file as a yaml and convert to FeaturesBucketMapping object.\n        variables (list): The features to bucket. Uses all features in features_bucket_mapping if not defined.\n        remainder (str): How we want the non-specified columns to be transformed. It must be in [\"passthrough\", \"drop\"].\n            passthrough (Default): all columns that were not specified in \"variables\" will be passed through.\n            drop: all remaining columns that were not specified in \"variables\" will be dropped.\n    \"\"\"  # noqa\n    # Assigning the variable in the init to the attribute with the same name is a requirement of\n    # sklearn.base.BaseEstimator. See the notes in\n    # https://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html#sklearn.base.BaseEstimator\n    self.features_bucket_mapping = features_bucket_mapping\n    self.remainder = remainder\n    self.get_statistics = get_statistics\n\n    self.variables = variables\n\n    if features_bucket_mapping is None:\n        self.features_bucket_mapping_ = FeaturesBucketMapping()\n    elif isinstance(features_bucket_mapping, str):\n        buckets_yaml = yaml.safe_load(open(features_bucket_mapping))\n        self.features_bucket_mapping_ = FeaturesBucketMapping(buckets_yaml)\n    elif isinstance(features_bucket_mapping, dict):\n        self.features_bucket_mapping_ = FeaturesBucketMapping(features_bucket_mapping)\n    elif isinstance(features_bucket_mapping, FeaturesBucketMapping):\n        self.features_bucket_mapping_ = features_bucket_mapping\n    else:\n        try:\n            buckets_yaml = yaml.safe_load(features_bucket_mapping)\n            self.features_bucket_mapping_ = FeaturesBucketMapping(buckets_yaml)\n        except Exception:\n            raise TypeError(\n                \"'features_bucket_mapping' must be a None, dict, str, path, or FeaturesBucketMapping instance\"\n            )\n</code></pre>"},{"location":"api/bucketers/UserInputBucketer/#skorecard.bucketers.UserInputBucketer.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Init the class.</p> Source code in <code>skorecard/bucketers/bucketers.py</code> <pre><code>def fit(self, X, y=None):\n    \"\"\"Init the class.\"\"\"\n    X = ensure_dataframe(X)\n    if y is not None:\n        assert len(y) == X.shape[0], \"y and X not same length\"\n        # Store the classes seen during fit\n        self.classes_ = unique_labels(y)\n\n    # scikit-learn requires checking that X has same shape on transform\n    # this is because scikit-learn is still positional based (no column names used)\n    self.n_train_features_ = X.shape[1]\n\n    # bucket tables can only be computed on fit().\n    # so a user will have to .fit() if she/he wants .plot_buckets() and .bucket_table()\n    self.bucket_tables_ = {}\n\n    # and if user did not specify any variables\n    # use all the variables defined in the features_bucket_mapping\n    if self.variables == []:\n        self.variables_ = list(self.features_bucket_mapping_.maps.keys())\n\n    for feature in self.variables_:\n        # Calculate the bucket table\n        self.bucket_tables_[feature] = build_bucket_table(\n            X,\n            y,\n            column=feature,\n            bucket_mapping=self.features_bucket_mapping_.get(feature),\n        )\n\n    self._generate_summary(X, y)\n\n    return self\n</code></pre>"},{"location":"api/datasets/load_uci_credit_card/","title":"Datasets","text":"<p>Loads the UCI Credit Card Dataset.</p> <p>This dataset contains a sample of Default of Credit Card Clients Dataset.</p> <p>Example:</p> <pre><code>from skorecard import datasets\ndf = datasets.load_uci_credit_card(as_frame=True)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>return_X_y</code> <code>bool</code> <p>If True, returns <code>(data, target)</code> instead of a dict object.</p> <code>False</code> <code>as_frame</code> <code>bool</code> <p>give the pandas dataframe instead of X, y matrices (default=False).</p> <code>False</code> <p>(pd.DataFrame, dict or tuple) features and target, with as follows:</p> Type Description <ul> <li>if as_frame is True: returns pd.DataFrame with y as a target</li> </ul> <ul> <li>return_X_y is True: returns a tuple: (X,y)</li> </ul> <ul> <li>is both are false (default setting): returns a dictionary where the key <code>data</code> contains the features,</li> </ul> <p>and the key <code>target</code> is the target</p> Source code in <code>skorecard/datasets.py</code> <pre><code>def load_uci_credit_card(return_X_y=False, as_frame=False):\n    \"\"\"Loads the UCI Credit Card Dataset.\n\n    This dataset contains a sample of [Default of Credit Card Clients Dataset](https://www.kaggle.com/uciml/default-of-credit-card-clients-dataset).\n\n    Example:\n\n    ```python\n    from skorecard import datasets\n    df = datasets.load_uci_credit_card(as_frame=True)\n    ```\n\n    Args:\n        return_X_y (bool): If True, returns `(data, target)` instead of a dict object.\n        as_frame (bool): give the pandas dataframe instead of X, y matrices (default=False).\n\n    Returns: (pd.DataFrame, dict or tuple) features and target, with as follows:\n        - if as_frame is True: returns pd.DataFrame with y as a target\n        - return_X_y is True: returns a tuple: (X,y)\n        - is both are false (default setting): returns a dictionary where the key `data` contains the features,\n        and the key `target` is the target\n\n    \"\"\"  # noqa\n    file = pkgutil.get_data(\"skorecard\", \"data/UCI_Credit_Card.zip\")\n    df = pd.read_csv(io.BytesIO(file), compression=\"zip\")\n    df = df.rename(columns={\"default.payment.next.month\": \"default\"})\n    if as_frame:\n        return df[[\"EDUCATION\", \"MARRIAGE\", \"LIMIT_BAL\", \"BILL_AMT1\", \"default\"]]\n    X, y = (\n        df[[\"EDUCATION\", \"MARRIAGE\", \"LIMIT_BAL\", \"BILL_AMT1\"]],\n        df[\"default\"].values,\n    )\n    if return_X_y:\n        return X, y\n\n    return {\"data\": X, \"target\": y}\n</code></pre>"},{"location":"api/linear_model/LogisticRegression/","title":"LogisticRegression","text":"<p>               Bases: <code>LogisticRegression</code></p> <p>Extended Logistic Regression.</p> <p>Extends sklearn.linear_model.LogisticRegression.</p> <p>This class provides the following extra statistics, calculated on <code>.fit()</code> and accessible via <code>.get_stats()</code>:</p> <ul> <li><code>cov_matrix_</code>: covariance matrix for the estimated parameters.</li> <li><code>std_err_intercept_</code>: estimated uncertainty for the intercept</li> <li><code>std_err_coef_</code>: estimated uncertainty for the coefficients</li> <li><code>z_intercept_</code>: estimated z-statistic for the intercept</li> <li><code>z_coef_</code>: estimated z-statistic for the coefficients</li> <li><code>p_value_intercept_</code>: estimated p-value for the intercept</li> <li><code>p_value_coef_</code>: estimated p-value for the coefficients</li> </ul> <p>Example:</p> <pre><code>from skorecard.datasets import load_uci_credit_card\nfrom skorecard.bucketers import EqualFrequencyBucketer\nfrom skorecard.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder\n\nX, y = load_uci_credit_card(return_X_y=True)\n\npipeline = Pipeline([\n    ('bucketer', EqualFrequencyBucketer(n_bins=10)),\n    ('clf', LogisticRegression(calculate_stats=True))\n])\npipeline.fit(X, y)\nassert pipeline.named_steps['clf'].p_val_coef_[0][0] &gt; 0\n\npipeline.named_steps['clf'].get_stats()\n</code></pre> <p>An example output of <code>.get_stats()</code>:</p> Index Coef. Std.Err z Pz const -0.537571 0.096108 -5.593394 2.226735e-08 EDUCATION 0.010091 0.044874 0.224876 8.220757e-01 Source code in <code>skorecard/linear_model/linear_model.py</code> <pre><code>class LogisticRegression(lm.LogisticRegression):\n    \"\"\"Extended Logistic Regression.\n\n    Extends [sklearn.linear_model.LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).\n\n    This class provides the following extra statistics, calculated on `.fit()` and accessible via `.get_stats()`:\n\n    - `cov_matrix_`: covariance matrix for the estimated parameters.\n    - `std_err_intercept_`: estimated uncertainty for the intercept\n    - `std_err_coef_`: estimated uncertainty for the coefficients\n    - `z_intercept_`: estimated z-statistic for the intercept\n    - `z_coef_`: estimated z-statistic for the coefficients\n    - `p_value_intercept_`: estimated p-value for the intercept\n    - `p_value_coef_`: estimated p-value for the coefficients\n\n    Example:\n\n    ```python\n    from skorecard.datasets import load_uci_credit_card\n    from skorecard.bucketers import EqualFrequencyBucketer\n    from skorecard.linear_model import LogisticRegression\n    from sklearn.pipeline import Pipeline\n    from sklearn.preprocessing import OneHotEncoder\n\n    X, y = load_uci_credit_card(return_X_y=True)\n\n    pipeline = Pipeline([\n        ('bucketer', EqualFrequencyBucketer(n_bins=10)),\n        ('clf', LogisticRegression(calculate_stats=True))\n    ])\n    pipeline.fit(X, y)\n    assert pipeline.named_steps['clf'].p_val_coef_[0][0] &gt; 0\n\n    pipeline.named_steps['clf'].get_stats()\n    ```\n\n    An example output of `.get_stats()`:\n\n    Index     | Coef.     | Std.Err  |   z       | Pz\n    --------- | ----------| ---------| ----------| ------------\n    const     | -0.537571 | 0.096108 | -5.593394 | 2.226735e-08\n    EDUCATION | 0.010091  | 0.044874 | 0.224876  | 8.220757e-01\n\n    \"\"\"  # noqa\n\n    def __init__(\n        self,\n        penalty=\"l2\",\n        calculate_stats=False,\n        dual=False,\n        tol=0.0001,\n        C=1.0,\n        fit_intercept=True,\n        intercept_scaling=1,\n        class_weight=None,\n        random_state=None,\n        solver=\"lbfgs\",\n        max_iter=100,\n        multi_class=\"auto\",\n        verbose=0,\n        warm_start=False,\n        n_jobs=None,\n        l1_ratio=None,\n    ):\n        \"\"\"\n        Extends [sklearn.linear_model.LogisticRegression.fit()](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).\n\n        Args:\n            calculate_stats (bool): If true, calculate statistics like standard error during fit, accessible with .get_stats()\n        \"\"\"  # noqa\n        super().__init__(\n            penalty=penalty,\n            dual=dual,\n            tol=tol,\n            C=C,\n            fit_intercept=fit_intercept,\n            intercept_scaling=intercept_scaling,\n            class_weight=class_weight,\n            random_state=random_state,\n            solver=solver,\n            max_iter=max_iter,\n            multi_class=multi_class,\n            verbose=verbose,\n            warm_start=warm_start,\n            n_jobs=n_jobs,\n            l1_ratio=l1_ratio,\n        )\n        self.calculate_stats = calculate_stats\n\n    def fit(self, X, y, sample_weight=None, calculate_stats=False, **kwargs):\n        \"\"\"\n        Fit the model.\n\n        Overwrites [sklearn.linear_model.LogisticRegression.fit()](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).\n\n        In addition to the standard fit by sklearn, this function will compute the covariance of the coefficients.\n\n        Args:\n            X (array-like, sparse matrix): Matrix of shape (n_samples, n_features)\n                Training vector, where n_samples is the number of samples and\n                n_features is the number of features.\n            y (array-like): of shape (n_samples,)\n                Target vector relative to X.\n            sample_weight (array-like): of shape (n_samples,) default=None\n                Array of weights that are assigned to individual samples.\n                If not provided, then each sample is given unit weight.\n            calculate_stats (bool): If true, calculate statistics like standard error during fit, accessible with .get_stats()\n\n        Returns:\n            self (LogisticRegression): Fitted estimator.\n        \"\"\"  # noqa\n\n        if not self.calculate_stats and not calculate_stats:\n            return super().fit(X, y, sample_weight=sample_weight, **kwargs)\n\n        X = convert_sparse_matrix(X)\n        if isinstance(X, pd.DataFrame):\n            self.names_ = [\"const\"] + [f for f in X.columns]\n        else:\n            self.names_ = [\"const\"] + [f\"x{i}\" for i in range(X.shape[1])]\n\n        lr = super().fit(X, y, sample_weight=sample_weight, **kwargs)\n\n        predProbs = self.predict_proba(X)\n\n        # Design matrix -- add column of 1's at the beginning of your X matrix\n        if lr.fit_intercept:\n            X_design = np.hstack([np.ones((X.shape[0], 1)), X])\n        else:\n            X_design = X\n\n        p = np.prod(predProbs, axis=1)\n        self.cov_matrix_ = np.linalg.inv((X_design * p[..., np.newaxis]).T @ X_design)\n        std_err = np.sqrt(np.diag(self.cov_matrix_)).reshape(1, -1)\n\n        # In case fit_intercept is set to True, then in the std_error array\n        # Index 0 corresponds to the intercept, from index 1 onwards it relates to the coefficients\n        # If fit intercept is False, then all the values are related to the coefficients\n        if lr.fit_intercept:\n            self.std_err_intercept_ = std_err[:, 0]\n            self.std_err_coef_ = std_err[:, 1:][0]\n\n            self.z_intercept_ = self.intercept_ / self.std_err_intercept_\n\n            # Get p-values under the gaussian assumption\n            self.p_val_intercept_ = scipy.stats.norm.sf(abs(self.z_intercept_)) * 2\n\n        else:\n            self.std_err_intercept_ = np.array([np.nan])\n            self.std_err_coef_ = std_err[0]\n\n            self.z_intercept_ = np.array([np.nan])\n\n            # Get p-values under the gaussian assumption\n            self.p_val_intercept_ = np.array([np.nan])\n\n        self.z_coef_ = self.coef_ / self.std_err_coef_\n        self.p_val_coef_ = scipy.stats.norm.sf(abs(self.z_coef_)) * 2\n\n        return self\n\n    def get_stats(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Puts the summary statistics of the fit() function into a pandas DataFrame.\n\n        Returns:\n            data (pandas DataFrame): The statistics dataframe, indexed by\n                the column name\n        \"\"\"\n        check_is_fitted(self)\n\n        if not hasattr(self, \"std_err_coef_\"):\n            msg = \"Summary statistics were not calculated on .fit(). Options to fix:\\n\"\n            msg += \"\\t- Re-fit using .fit(X, y, calculate_stats=True)\\n\"\n            msg += \"\\t- Re-inititialize using LogisticRegression(calculate_stats=True)\"\n            raise AssertionError(msg)\n\n        data = {\n            \"Coef.\": (self.intercept_.tolist() + self.coef_.tolist()[0]),\n            \"Std.Err\": (self.std_err_intercept_.tolist() + self.std_err_coef_.tolist()),\n            \"z\": (self.z_intercept_.tolist() + self.z_coef_.tolist()[0]),\n            \"P&gt;|z|\": (self.p_val_intercept_.tolist() + self.p_val_coef_.tolist()[0]),\n        }\n\n        return pd.DataFrame(data, index=self.names_)\n\n    def plot_weights(self):\n        \"\"\"\n        Plots the relative importance of coefficients of the model.\n\n        Example:\n\n        ```from skorecard.datasets import load_uci_credit_card\n        from skorecard.bucketers import EqualFrequencyBucketer\n        from skorecard.linear_model import LogisticRegression\n        from skorecard.reporting.plotting import weight_plot\n        from sklearn.pipeline import Pipeline\n        from sklearn.preprocessing import OneHotEncoder\n        X, y = load_uci_credit_card(return_X_y=True)\n        pipeline = Pipeline([\n            ('bucketer', EqualFrequencyBucketer(n_bins=10)),\n            ('clf', LogisticRegression(calculate_stats=True))\n        ])\n        pipeline.fit(X, y)\n        assert pipeline.named_steps['clf'].p_val_coef_[0][0] &gt; 0\n        stats = pipeline.named_steps['clf'].get_stats()\n        pipeline.named_steps['clf'].plot_weights()```\n        \"\"\"\n        stats = self.get_stats()\n        return weight_plot(stats)\n</code></pre>"},{"location":"api/linear_model/LogisticRegression/#skorecard.linear_model.LogisticRegression.__init__","title":"<code>__init__(penalty='l2', calculate_stats=False, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs', max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)</code>","text":"<p>Extends sklearn.linear_model.LogisticRegression.fit().</p> <p>Parameters:</p> Name Type Description Default <code>calculate_stats</code> <code>bool</code> <p>If true, calculate statistics like standard error during fit, accessible with .get_stats()</p> <code>False</code> Source code in <code>skorecard/linear_model/linear_model.py</code> <pre><code>def __init__(\n    self,\n    penalty=\"l2\",\n    calculate_stats=False,\n    dual=False,\n    tol=0.0001,\n    C=1.0,\n    fit_intercept=True,\n    intercept_scaling=1,\n    class_weight=None,\n    random_state=None,\n    solver=\"lbfgs\",\n    max_iter=100,\n    multi_class=\"auto\",\n    verbose=0,\n    warm_start=False,\n    n_jobs=None,\n    l1_ratio=None,\n):\n    \"\"\"\n    Extends [sklearn.linear_model.LogisticRegression.fit()](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).\n\n    Args:\n        calculate_stats (bool): If true, calculate statistics like standard error during fit, accessible with .get_stats()\n    \"\"\"  # noqa\n    super().__init__(\n        penalty=penalty,\n        dual=dual,\n        tol=tol,\n        C=C,\n        fit_intercept=fit_intercept,\n        intercept_scaling=intercept_scaling,\n        class_weight=class_weight,\n        random_state=random_state,\n        solver=solver,\n        max_iter=max_iter,\n        multi_class=multi_class,\n        verbose=verbose,\n        warm_start=warm_start,\n        n_jobs=n_jobs,\n        l1_ratio=l1_ratio,\n    )\n    self.calculate_stats = calculate_stats\n</code></pre>"},{"location":"api/linear_model/LogisticRegression/#skorecard.linear_model.LogisticRegression.fit","title":"<code>fit(X, y, sample_weight=None, calculate_stats=False, **kwargs)</code>","text":"<p>Fit the model.</p> <p>Overwrites sklearn.linear_model.LogisticRegression.fit().</p> <p>In addition to the standard fit by sklearn, this function will compute the covariance of the coefficients.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like, sparse matrix</code> <p>Matrix of shape (n_samples, n_features) Training vector, where n_samples is the number of samples and n_features is the number of features.</p> required <code>y</code> <code>array - like</code> <p>of shape (n_samples,) Target vector relative to X.</p> required <code>sample_weight</code> <code>array - like</code> <p>of shape (n_samples,) default=None Array of weights that are assigned to individual samples. If not provided, then each sample is given unit weight.</p> <code>None</code> <code>calculate_stats</code> <code>bool</code> <p>If true, calculate statistics like standard error during fit, accessible with .get_stats()</p> <code>False</code> <p>Returns:</p> Name Type Description <code>self</code> <code>LogisticRegression</code> <p>Fitted estimator.</p> Source code in <code>skorecard/linear_model/linear_model.py</code> <pre><code>def fit(self, X, y, sample_weight=None, calculate_stats=False, **kwargs):\n    \"\"\"\n    Fit the model.\n\n    Overwrites [sklearn.linear_model.LogisticRegression.fit()](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).\n\n    In addition to the standard fit by sklearn, this function will compute the covariance of the coefficients.\n\n    Args:\n        X (array-like, sparse matrix): Matrix of shape (n_samples, n_features)\n            Training vector, where n_samples is the number of samples and\n            n_features is the number of features.\n        y (array-like): of shape (n_samples,)\n            Target vector relative to X.\n        sample_weight (array-like): of shape (n_samples,) default=None\n            Array of weights that are assigned to individual samples.\n            If not provided, then each sample is given unit weight.\n        calculate_stats (bool): If true, calculate statistics like standard error during fit, accessible with .get_stats()\n\n    Returns:\n        self (LogisticRegression): Fitted estimator.\n    \"\"\"  # noqa\n\n    if not self.calculate_stats and not calculate_stats:\n        return super().fit(X, y, sample_weight=sample_weight, **kwargs)\n\n    X = convert_sparse_matrix(X)\n    if isinstance(X, pd.DataFrame):\n        self.names_ = [\"const\"] + [f for f in X.columns]\n    else:\n        self.names_ = [\"const\"] + [f\"x{i}\" for i in range(X.shape[1])]\n\n    lr = super().fit(X, y, sample_weight=sample_weight, **kwargs)\n\n    predProbs = self.predict_proba(X)\n\n    # Design matrix -- add column of 1's at the beginning of your X matrix\n    if lr.fit_intercept:\n        X_design = np.hstack([np.ones((X.shape[0], 1)), X])\n    else:\n        X_design = X\n\n    p = np.prod(predProbs, axis=1)\n    self.cov_matrix_ = np.linalg.inv((X_design * p[..., np.newaxis]).T @ X_design)\n    std_err = np.sqrt(np.diag(self.cov_matrix_)).reshape(1, -1)\n\n    # In case fit_intercept is set to True, then in the std_error array\n    # Index 0 corresponds to the intercept, from index 1 onwards it relates to the coefficients\n    # If fit intercept is False, then all the values are related to the coefficients\n    if lr.fit_intercept:\n        self.std_err_intercept_ = std_err[:, 0]\n        self.std_err_coef_ = std_err[:, 1:][0]\n\n        self.z_intercept_ = self.intercept_ / self.std_err_intercept_\n\n        # Get p-values under the gaussian assumption\n        self.p_val_intercept_ = scipy.stats.norm.sf(abs(self.z_intercept_)) * 2\n\n    else:\n        self.std_err_intercept_ = np.array([np.nan])\n        self.std_err_coef_ = std_err[0]\n\n        self.z_intercept_ = np.array([np.nan])\n\n        # Get p-values under the gaussian assumption\n        self.p_val_intercept_ = np.array([np.nan])\n\n    self.z_coef_ = self.coef_ / self.std_err_coef_\n    self.p_val_coef_ = scipy.stats.norm.sf(abs(self.z_coef_)) * 2\n\n    return self\n</code></pre>"},{"location":"api/linear_model/LogisticRegression/#skorecard.linear_model.LogisticRegression.get_stats","title":"<code>get_stats()</code>","text":"<p>Puts the summary statistics of the fit() function into a pandas DataFrame.</p> <p>Returns:</p> Name Type Description <code>data</code> <code>pandas DataFrame</code> <p>The statistics dataframe, indexed by the column name</p> Source code in <code>skorecard/linear_model/linear_model.py</code> <pre><code>def get_stats(self) -&gt; pd.DataFrame:\n    \"\"\"\n    Puts the summary statistics of the fit() function into a pandas DataFrame.\n\n    Returns:\n        data (pandas DataFrame): The statistics dataframe, indexed by\n            the column name\n    \"\"\"\n    check_is_fitted(self)\n\n    if not hasattr(self, \"std_err_coef_\"):\n        msg = \"Summary statistics were not calculated on .fit(). Options to fix:\\n\"\n        msg += \"\\t- Re-fit using .fit(X, y, calculate_stats=True)\\n\"\n        msg += \"\\t- Re-inititialize using LogisticRegression(calculate_stats=True)\"\n        raise AssertionError(msg)\n\n    data = {\n        \"Coef.\": (self.intercept_.tolist() + self.coef_.tolist()[0]),\n        \"Std.Err\": (self.std_err_intercept_.tolist() + self.std_err_coef_.tolist()),\n        \"z\": (self.z_intercept_.tolist() + self.z_coef_.tolist()[0]),\n        \"P&gt;|z|\": (self.p_val_intercept_.tolist() + self.p_val_coef_.tolist()[0]),\n    }\n\n    return pd.DataFrame(data, index=self.names_)\n</code></pre>"},{"location":"api/linear_model/LogisticRegression/#skorecard.linear_model.LogisticRegression.plot_weights","title":"<code>plot_weights()</code>","text":"<p>Plots the relative importance of coefficients of the model.</p> <p>Example:</p> <p><code>from skorecard.datasets import load_uci_credit_card from skorecard.bucketers import EqualFrequencyBucketer from skorecard.linear_model import LogisticRegression from skorecard.reporting.plotting import weight_plot from sklearn.pipeline import Pipeline from sklearn.preprocessing import OneHotEncoder X, y = load_uci_credit_card(return_X_y=True) pipeline = Pipeline([     ('bucketer', EqualFrequencyBucketer(n_bins=10)),     ('clf', LogisticRegression(calculate_stats=True)) ]) pipeline.fit(X, y) assert pipeline.named_steps['clf'].p_val_coef_[0][0] &gt; 0 stats = pipeline.named_steps['clf'].get_stats() pipeline.named_steps['clf'].plot_weights()</code></p> Source code in <code>skorecard/linear_model/linear_model.py</code> <pre><code>def plot_weights(self):\n    \"\"\"\n    Plots the relative importance of coefficients of the model.\n\n    Example:\n\n    ```from skorecard.datasets import load_uci_credit_card\n    from skorecard.bucketers import EqualFrequencyBucketer\n    from skorecard.linear_model import LogisticRegression\n    from skorecard.reporting.plotting import weight_plot\n    from sklearn.pipeline import Pipeline\n    from sklearn.preprocessing import OneHotEncoder\n    X, y = load_uci_credit_card(return_X_y=True)\n    pipeline = Pipeline([\n        ('bucketer', EqualFrequencyBucketer(n_bins=10)),\n        ('clf', LogisticRegression(calculate_stats=True))\n    ])\n    pipeline.fit(X, y)\n    assert pipeline.named_steps['clf'].p_val_coef_[0][0] &gt; 0\n    stats = pipeline.named_steps['clf'].get_stats()\n    pipeline.named_steps['clf'].plot_weights()```\n    \"\"\"\n    stats = self.get_stats()\n    return weight_plot(stats)\n</code></pre>"},{"location":"api/pipeline/BucketingProcess/","title":"BucketingProcess","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code>, <code>BucketTableMethod</code>, <code>PlotBucketMethod</code>, <code>PlotPreBucketMethod</code>, <code>SummaryMethod</code></p> <p>A two-step bucketing pipeline allowing for pre-bucketing before bucketing.</p> <p>Often you want to pre-bucket features (f.e. to 100 buckets) before bucketing to a smaller set. This brings some additional challenges around propagating specials and defining a bucketer that is able to go from raw data to final bucket. This class facilicates the process and also provides all regular methods and attributes:</p> <ul> <li><code>.summary()</code>: See which columns are bucketed</li> <li><code>.plot_bucket()</code>: Plot buckets of a column</li> <li><code>.bucket_table()</code>: Table with buckets of a column</li> <li><code>.save_to_yaml()</code>: Save information necessary for bucketing to a YAML file</li> <li><code>.features_bucket_mapping_</code>: Access bucketing information</li> </ul> <p>Example:</p> <pre><code>from skorecard import datasets\nfrom skorecard.bucketers import DecisionTreeBucketer, OptimalBucketer, AsIsCategoricalBucketer\nfrom skorecard.pipeline import BucketingProcess\nfrom sklearn.pipeline import make_pipeline\n\ndf = datasets.load_uci_credit_card(as_frame=True)\ny = df[\"default\"]\nX = df.drop(columns=[\"default\"])\n\nnum_cols = [\"LIMIT_BAL\", \"BILL_AMT1\"]\ncat_cols = [\"EDUCATION\", \"MARRIAGE\"]\n\nbucketing_process = BucketingProcess(\n    specials={'LIMIT_BAL': {'=400000.0' : [400000.0]}},\n    prebucketing_pipeline=make_pipeline(\n        DecisionTreeBucketer(variables=num_cols, max_n_bins=100, min_bin_size=0.05),\n        AsIsCategoricalBucketer(variables=cat_cols),\n    ),\n    bucketing_pipeline=make_pipeline(\n        OptimalBucketer(variables=num_cols, max_n_bins=10, min_bin_size=0.05),\n        OptimalBucketer(variables=cat_cols, variables_type='categorical', max_n_bins=10, min_bin_size=0.05),\n    )\n)\n\nbucketing_process.fit(X, y)\n\n# Details\nbucketing_process.summary() # all vars, and # buckets\nbucketing_process.bucket_table(\"LIMIT_BAL\")\nbucketing_process.plot_bucket(\"LIMIT_BAL\")\nbucketing_process.prebucket_table(\"LIMIT_BAL\")\nbucketing_process.plot_prebucket(\"LIMIT_BAL\")\n</code></pre> Source code in <code>skorecard/pipeline/bucketing_process.py</code> <pre><code>class BucketingProcess(\n    BaseEstimator,\n    TransformerMixin,\n    BucketTableMethod,\n    PlotBucketMethod,\n    PlotPreBucketMethod,\n    SummaryMethod,\n):\n    \"\"\"\n    A two-step bucketing pipeline allowing for pre-bucketing before bucketing.\n\n    Often you want to pre-bucket features (f.e. to 100 buckets) before bucketing to a smaller set.\n    This brings some additional challenges around propagating specials and defining a bucketer that is able to go from raw data to final bucket.\n    This class facilicates the process and also provides all regular methods and attributes:\n\n    - `.summary()`: See which columns are bucketed\n    - `.plot_bucket()`: Plot buckets of a column\n    - `.bucket_table()`: Table with buckets of a column\n    - `.save_to_yaml()`: Save information necessary for bucketing to a YAML file\n    - `.features_bucket_mapping_`: Access bucketing information\n\n    Example:\n\n    ```python\n    from skorecard import datasets\n    from skorecard.bucketers import DecisionTreeBucketer, OptimalBucketer, AsIsCategoricalBucketer\n    from skorecard.pipeline import BucketingProcess\n    from sklearn.pipeline import make_pipeline\n\n    df = datasets.load_uci_credit_card(as_frame=True)\n    y = df[\"default\"]\n    X = df.drop(columns=[\"default\"])\n\n    num_cols = [\"LIMIT_BAL\", \"BILL_AMT1\"]\n    cat_cols = [\"EDUCATION\", \"MARRIAGE\"]\n\n    bucketing_process = BucketingProcess(\n        specials={'LIMIT_BAL': {'=400000.0' : [400000.0]}},\n        prebucketing_pipeline=make_pipeline(\n            DecisionTreeBucketer(variables=num_cols, max_n_bins=100, min_bin_size=0.05),\n            AsIsCategoricalBucketer(variables=cat_cols),\n        ),\n        bucketing_pipeline=make_pipeline(\n            OptimalBucketer(variables=num_cols, max_n_bins=10, min_bin_size=0.05),\n            OptimalBucketer(variables=cat_cols, variables_type='categorical', max_n_bins=10, min_bin_size=0.05),\n        )\n    )\n\n    bucketing_process.fit(X, y)\n\n    # Details\n    bucketing_process.summary() # all vars, and # buckets\n    bucketing_process.bucket_table(\"LIMIT_BAL\")\n    bucketing_process.plot_bucket(\"LIMIT_BAL\")\n    bucketing_process.prebucket_table(\"LIMIT_BAL\")\n    bucketing_process.plot_prebucket(\"LIMIT_BAL\")\n    ```\n    \"\"\"  # noqa\n\n    def __init__(\n        self,\n        prebucketing_pipeline=make_pipeline(DecisionTreeBucketer(max_n_bins=50, min_bin_size=0.02)),\n        bucketing_pipeline=make_pipeline(OptimalBucketer(max_n_bins=6, min_bin_size=0.05)),\n        variables: List = [],\n        specials: Dict = {},\n        random_state: Optional[int] = None,\n        remainder=\"passthrough\",\n    ):\n        \"\"\"\n        Define a BucketingProcess to first prebucket and then bucket multiple columns in one go.\n\n        Args:\n            prebucketing_pipeline (Pipeline): The scikit-learn pipeline that does pre-bucketing.\n                Defaults to an all-numeric DecisionTreeBucketer pipeline.\n            bucketing_pipeline (Pipeline): The scikit-learn pipeline that does bucketing.\n                Defaults to an all-numeric OptimalBucketer pipeline.\n                Must transform same features as the prebucketing pipeline.\n            variables (list): The features to bucket. Uses all features if not defined.\n            specials: (nested) dictionary of special values that require their own binning.\n                Will merge when specials are also defined in any bucketers in a (pre)bucketing pipeline, and overwrite in case there are shared keys.\n                The dictionary has the following format:\n                 {\"&lt;column name&gt;\" : {\"name of special bucket\" : &lt;list with 1 or more values&gt;}}\n                For every feature that needs a special value, a dictionary must be passed as value.\n                This dictionary contains a name of a bucket (key) and an array of unique values that should be put\n                in that bucket.\n                When special values are defined, they are not considered in the fitting procedure.\n            remainder (str): How we want the non-specified columns to be transformed. It must be in [\"passthrough\", \"drop\"].\n                passthrough (Default): all columns that were not specified in \"variables\" will be passed through.\n                drop: all remaining columns that were not specified in \"variables\" will be dropped.\n        \"\"\"  # noqa\n        # Save original input params\n        # We overwrite the input later, so we need to save\n        # original so we can clone instances\n        # https://scikit-learn.org/dev/developers/develop.html#cloning\n        # https://scikit-learn.org/dev/developers/develop.html#get-params-and-set-params\n        # Assigning the variable in the init to the attribute with the same name is a requirement of\n        # sklearn.base.BaseEstimator. See the notes in\n        # https://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html#sklearn.base.BaseEstimator\n        self.prebucketing_pipeline = prebucketing_pipeline\n        self.bucketing_pipeline = bucketing_pipeline\n        self.remainder = remainder\n        self.variables = variables\n        self.specials = specials\n        self.random_state = random_state\n\n    @property\n    def name(self):\n        \"\"\"\n        To be able to identity the bucketingprocess in a pipeline.\n        \"\"\"\n        return \"bucketingprocess\"\n\n    def fit(self, X, y=None):\n        \"\"\"\n        Fit the prebucketing and bucketing pipeline with `X`, `y`.\n\n        Args:\n            X (pd.DataFrame): Data to fit on.\n            y (np.array, optional): target. Defaults to None.\n        \"\"\"\n        X = ensure_dataframe(X)\n\n        # input validation\n        assert self.remainder in [\"passthrough\", \"drop\"]\n\n        # Convert to skorecard pipelines\n        # This does some checks on the pipelines\n        # and adds some convenience methods to the pipeline.\n        self.pre_pipeline_ = to_skorecard_pipeline(deepcopy(self.prebucketing_pipeline))\n        self.pipeline_ = to_skorecard_pipeline(deepcopy(self.bucketing_pipeline))\n\n        # Add/Overwrite specials to all pre-bucketers\n        for step in _get_all_steps(self.pre_pipeline_):\n            if hasattr(step, \"specials\") and len(step.specials) != 0 and len(self.specials) != 0:\n                # note, specials defined BucketingProcess level\n                # will overwrite any specials on bucketer level.\n                warnings.warn(f\"Overwriting specials of {step} with specials of bucketingprocess\", UserWarning)\n                step.specials = {**step.specials, **self.specials}\n            else:\n                step.specials = self.specials\n\n            if len(self.variables) != 0:\n                if len(step.variables) != 0:\n                    warnings.warn(f\"Overwriting variables of {step} with variables of bucketingprocess\", UserWarning)\n                step.variables = self.variables\n\n            # Overwrite random_state to bucketers\n            if hasattr(step, \"random_state\") and self.random_state is not None:\n                if step.random_state is not None:\n                    warnings.warn(\n                        f\"Overwriting random_state of {step} with random_state of bucketingprocess\", UserWarning\n                    )\n                step.random_state = self.random_state\n\n        # Overwrite variables to all bucketers\n        if len(self.variables) != 0:\n            for step in _get_all_steps(self.pipeline_):\n                if len(step.variables) != 0:\n                    warnings.warn(f\"Overwriting variables of {step} with variables of bucketingprocess\", UserWarning)\n                step.variables = self.variables\n\n        # Overwrite random_state to bucketers\n        for step in _get_all_steps(self.pipeline_):\n            if hasattr(step, \"random_state\") and self.random_state is not None:\n                if step.random_state is not None:\n                    warnings.warn(\n                        f\"Overwriting random_state of {step} with random_state of bucketingprocess\", UserWarning\n                    )\n                step.random_state = self.random_state\n\n        self._prebucketing_specials = self.specials\n        self._bucketing_specials = dict()  # will be determined later.\n\n        # Fit the prebucketing pipeline\n        X_prebucketed_ = self.pre_pipeline_.fit_transform(X, y)\n        assert isinstance(X_prebucketed_, pd.DataFrame)\n\n        # Calculate the prebucket tables.\n        self.prebucket_tables_ = dict()\n        for column in X.columns:\n            if column in self.pre_pipeline_.features_bucket_mapping_.maps.keys():\n                self.prebucket_tables_[column] = build_bucket_table(\n                    X, y, column=column, bucket_mapping=self.pre_pipeline_.features_bucket_mapping_.get(column)\n                )\n\n        # Find the new bucket numbers of the specials after prebucketing,\n        for var, var_specials in self._prebucketing_specials.items():\n            bucket_labels = self.pre_pipeline_.features_bucket_mapping_.get(var).labels\n            new_specials = _find_remapped_specials(bucket_labels, var_specials)\n            if len(new_specials):\n                self._bucketing_specials[var] = new_specials\n\n        # Then assign the new specials to all bucketers in the bucketing pipeline\n        for step in self.pipeline_.steps:\n            if not isinstance(step, tuple):\n                step.specials = self._bucketing_specials\n            else:\n                step[1].specials = self._bucketing_specials\n\n        # Fit the bucketing pipeline\n        # And save the bucket mapping\n        self.pipeline_.fit(X_prebucketed_, y)\n\n        # Make sure all columns that are bucketed have also been pre-bucketed.\n        not_prebucketed = []\n        for col in self.pipeline_.features_bucket_mapping_.columns:\n            if self.pipeline_.features_bucket_mapping_.get(col).type == \"numerical\":\n                if col not in self.pre_pipeline_.features_bucket_mapping_.columns:\n                    not_prebucketed.append(col)\n        if len(not_prebucketed):\n            msg = \"These numerical columns are bucketed but have not been pre-bucketed: \"\n            msg += f\"{', '.join(not_prebucketed)}.\\n\"\n            msg += \"Consider adding a numerical bucketer to the prebucketing pipeline,\"\n            msg += \"for example AsIsNumericalBucketer or DecisionTreeBucketer.\"\n            raise NotPreBucketedError(msg)\n\n        # Make sure all columns that have been pre-bucketed also have been bucketed\n        not_bucketed = []\n        for col in self.pre_pipeline_.features_bucket_mapping_.columns:\n            if self.pre_pipeline_.features_bucket_mapping_.get(col).type == \"numerical\":\n                if col not in self.pipeline_.features_bucket_mapping_.columns:\n                    not_bucketed.append(col)\n        if len(not_bucketed):\n            msg = \"These numerical columns are prebucketed but have not been bucketed: \"\n            msg += f\"{', '.join(not_bucketed)}.\\n\"\n            msg += \"Consider updating the bucketing pipeline.\"\n            raise NotBucketedError(msg)\n\n        # calculate the bucket tables.\n        self.bucket_tables_ = dict()\n        for column in X.columns:\n            if column in self.pipeline_.features_bucket_mapping_.maps.keys():\n                self.bucket_tables_[column] = build_bucket_table(\n                    X_prebucketed_,\n                    y,\n                    column=column,\n                    bucket_mapping=self.pipeline_.features_bucket_mapping_.get(column),\n                )\n\n        # Calculate the summary\n        self._generate_summary(X, y)\n\n        return self\n\n    def fit_interactive(self, X, y=None, mode=\"external\", **server_kwargs):\n        \"\"\"\n        Fit a bucketer and then interactive edit the fit using a dash app.\n\n        Note we are using a [jupyterdash](https://medium.com/plotly/introducing-jupyterdash-811f1f57c02e) app,\n        which supports 3 different modes:\n\n        - 'external' (default): Start dash server and print URL\n        - 'inline': Start dash app inside an Iframe in the jupyter notebook\n        - 'jupyterlab': Start dash app as a new tab inside jupyterlab\n\n        \"\"\"\n        # We need to make sure we only fit if not already fitted\n        # This prevents a user losing manually defined boundaries\n        # when re-running .fit_interactive()\n        if not is_fitted(self):\n            self.fit(X, y)\n\n        self.app = JupyterDash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP])\n        add_bucketing_process_layout(self)\n        add_bucketing_process_callbacks(self, X, y)\n        self.app.run_server(mode=mode, **server_kwargs)\n\n    def transform(self, X):\n        \"\"\"\n        Transform `X` through the prebucketing and bucketing pipelines.\n        \"\"\"\n        check_is_fitted(self)\n        X_prebucketed = self.pre_pipeline_.transform(X)\n\n        new_X = self.pipeline_.transform(X_prebucketed)\n\n        if self.remainder == \"drop\":\n            return new_X[self.variables]\n        else:\n            return new_X\n\n    def save_yml(self, fout: PathLike) -&gt; None:\n        \"\"\"\n        Save the features bucket to a yaml file.\n\n        Args:\n            fout: path for output file\n        \"\"\"\n        check_is_fitted(self)\n        fbm = self.features_bucket_mapping_\n        if isinstance(fbm, dict):\n            FeaturesBucketMapping(fbm).save_yml(fout)\n        else:\n            fbm.save_yml(fout)\n\n    @property\n    def features_bucket_mapping_(self):\n        \"\"\"\n        Returns a `FeaturesBucketMapping` instance.\n\n        In normal bucketers, you can access `.features_bucket_mapping_`\n        to retrieve a `FeaturesBucketMapping` instance. This contains\n        all the info you need to transform values into their buckets.\n\n        In this class, we basically have a two step bucketing process:\n        first prebucketing, and then we bucket the prebuckets.\n\n        In order to still be able to use BucketingProcess as if it were a normal bucketer,\n        we'll need to merge both into one.\n        \"\"\"\n        check_is_fitted(self)\n\n        return merge_features_bucket_mapping(\n            self.pre_pipeline_.features_bucket_mapping_, self.pipeline_.features_bucket_mapping_\n        )\n\n    def prebucket_table(self, column: str) -&gt; pd.DataFrame:\n        \"\"\"\n        Generates the statistics for the buckets of a particular column.\n\n        An example is seen below:\n\n        pre-bucket | label      | Count | Count (%) | Non-event | Event | Event Rate | WoE   | IV   | bucket\n        -----------|------------|-------|-----------|-----------|-------|------------|-------|------|------\n        0          | (-inf, 1.0)| 479   | 7.98      | 300       | 179   |  37.37     |  0.73 | 0.05 | 0\n        1          | [1.0, 2.0) | 370   | 6.17      | 233       | 137   |  37.03     |  0.71 | 0.04 | 0\n\n        Args:\n            column (str): The column we wish to analyse\n\n        Returns:\n            df (pd.DataFrame): A pandas dataframe of the format above\n        \"\"\"  # noqa\n        check_is_fitted(self)\n        if column not in self.prebucket_tables_.keys():\n            raise ValueError(f\"column '{column}' was not part of the pre-bucketing process\")\n\n        table = self.prebucket_tables_.get(column)\n        table = table.rename(columns={\"bucket_id\": \"pre-bucket\"})\n\n        # Find bucket for each pre-bucket\n        bucket_mapping = self.pipeline_.features_bucket_mapping_.get(column)\n        table[\"bucket\"] = bucket_mapping.transform(table[\"pre-bucket\"])\n\n        # Find out missing bucket\n        if -1 in table[\"pre-bucket\"].values:\n            table.loc[table[\"pre-bucket\"] == -1, \"bucket\"] = bucket_mapping.transform([np.nan])[0]\n\n        # Find out the 'other' bucket\n        if bucket_mapping.type == \"categorical\" and -2 in table[\"pre-bucket\"].values:\n            something_random = \"84a088e251d2fa058f37145222e536dc\"\n            table.loc[table[\"pre-bucket\"] == -2, \"bucket\"] = bucket_mapping.transform([something_random])[0]\n\n        return table\n\n    def _more_tags(self):\n        \"\"\"\n        Estimator tags are annotations of estimators that allow programmatic inspection of their capabilities.\n\n        See https://scikit-learn.org/stable/developers/develop.html#estimator-tags\n        \"\"\"  # noqa\n        return {\"binary_only\": True}\n</code></pre>"},{"location":"api/pipeline/BucketingProcess/#skorecard.pipeline.BucketingProcess.features_bucket_mapping_","title":"<code>features_bucket_mapping_</code>  <code>property</code>","text":"<p>Returns a <code>FeaturesBucketMapping</code> instance.</p> <p>In normal bucketers, you can access <code>.features_bucket_mapping_</code> to retrieve a <code>FeaturesBucketMapping</code> instance. This contains all the info you need to transform values into their buckets.</p> <p>In this class, we basically have a two step bucketing process: first prebucketing, and then we bucket the prebuckets.</p> <p>In order to still be able to use BucketingProcess as if it were a normal bucketer, we'll need to merge both into one.</p>"},{"location":"api/pipeline/BucketingProcess/#skorecard.pipeline.BucketingProcess.name","title":"<code>name</code>  <code>property</code>","text":"<p>To be able to identity the bucketingprocess in a pipeline.</p>"},{"location":"api/pipeline/BucketingProcess/#skorecard.pipeline.BucketingProcess.__init__","title":"<code>__init__(prebucketing_pipeline=make_pipeline(DecisionTreeBucketer(max_n_bins=50, min_bin_size=0.02)), bucketing_pipeline=make_pipeline(OptimalBucketer(max_n_bins=6, min_bin_size=0.05)), variables=[], specials={}, random_state=None, remainder='passthrough')</code>","text":"<p>Define a BucketingProcess to first prebucket and then bucket multiple columns in one go.</p> <p>Parameters:</p> Name Type Description Default <code>prebucketing_pipeline</code> <code>Pipeline</code> <p>The scikit-learn pipeline that does pre-bucketing. Defaults to an all-numeric DecisionTreeBucketer pipeline.</p> <code>make_pipeline(DecisionTreeBucketer(max_n_bins=50, min_bin_size=0.02))</code> <code>bucketing_pipeline</code> <code>Pipeline</code> <p>The scikit-learn pipeline that does bucketing. Defaults to an all-numeric OptimalBucketer pipeline. Must transform same features as the prebucketing pipeline.</p> <code>make_pipeline(OptimalBucketer(max_n_bins=6, min_bin_size=0.05))</code> <code>variables</code> <code>list</code> <p>The features to bucket. Uses all features if not defined.</p> <code>[]</code> <code>specials</code> <code>Dict</code> <p>(nested) dictionary of special values that require their own binning. Will merge when specials are also defined in any bucketers in a (pre)bucketing pipeline, and overwrite in case there are shared keys. The dictionary has the following format:  {\"\" : {\"name of special bucket\" : }} For every feature that needs a special value, a dictionary must be passed as value. This dictionary contains a name of a bucket (key) and an array of unique values that should be put in that bucket. When special values are defined, they are not considered in the fitting procedure. <code>{}</code> <code>remainder</code> <code>str</code> <p>How we want the non-specified columns to be transformed. It must be in [\"passthrough\", \"drop\"]. passthrough (Default): all columns that were not specified in \"variables\" will be passed through. drop: all remaining columns that were not specified in \"variables\" will be dropped.</p> <code>'passthrough'</code> Source code in <code>skorecard/pipeline/bucketing_process.py</code> <pre><code>def __init__(\n    self,\n    prebucketing_pipeline=make_pipeline(DecisionTreeBucketer(max_n_bins=50, min_bin_size=0.02)),\n    bucketing_pipeline=make_pipeline(OptimalBucketer(max_n_bins=6, min_bin_size=0.05)),\n    variables: List = [],\n    specials: Dict = {},\n    random_state: Optional[int] = None,\n    remainder=\"passthrough\",\n):\n    \"\"\"\n    Define a BucketingProcess to first prebucket and then bucket multiple columns in one go.\n\n    Args:\n        prebucketing_pipeline (Pipeline): The scikit-learn pipeline that does pre-bucketing.\n            Defaults to an all-numeric DecisionTreeBucketer pipeline.\n        bucketing_pipeline (Pipeline): The scikit-learn pipeline that does bucketing.\n            Defaults to an all-numeric OptimalBucketer pipeline.\n            Must transform same features as the prebucketing pipeline.\n        variables (list): The features to bucket. Uses all features if not defined.\n        specials: (nested) dictionary of special values that require their own binning.\n            Will merge when specials are also defined in any bucketers in a (pre)bucketing pipeline, and overwrite in case there are shared keys.\n            The dictionary has the following format:\n             {\"&lt;column name&gt;\" : {\"name of special bucket\" : &lt;list with 1 or more values&gt;}}\n            For every feature that needs a special value, a dictionary must be passed as value.\n            This dictionary contains a name of a bucket (key) and an array of unique values that should be put\n            in that bucket.\n            When special values are defined, they are not considered in the fitting procedure.\n        remainder (str): How we want the non-specified columns to be transformed. It must be in [\"passthrough\", \"drop\"].\n            passthrough (Default): all columns that were not specified in \"variables\" will be passed through.\n            drop: all remaining columns that were not specified in \"variables\" will be dropped.\n    \"\"\"  # noqa\n    # Save original input params\n    # We overwrite the input later, so we need to save\n    # original so we can clone instances\n    # https://scikit-learn.org/dev/developers/develop.html#cloning\n    # https://scikit-learn.org/dev/developers/develop.html#get-params-and-set-params\n    # Assigning the variable in the init to the attribute with the same name is a requirement of\n    # sklearn.base.BaseEstimator. See the notes in\n    # https://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html#sklearn.base.BaseEstimator\n    self.prebucketing_pipeline = prebucketing_pipeline\n    self.bucketing_pipeline = bucketing_pipeline\n    self.remainder = remainder\n    self.variables = variables\n    self.specials = specials\n    self.random_state = random_state\n</code></pre>"},{"location":"api/pipeline/BucketingProcess/#skorecard.pipeline.BucketingProcess.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Fit the prebucketing and bucketing pipeline with <code>X</code>, <code>y</code>.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>Data to fit on.</p> required <code>y</code> <code>array</code> <p>target. Defaults to None.</p> <code>None</code> Source code in <code>skorecard/pipeline/bucketing_process.py</code> <pre><code>def fit(self, X, y=None):\n    \"\"\"\n    Fit the prebucketing and bucketing pipeline with `X`, `y`.\n\n    Args:\n        X (pd.DataFrame): Data to fit on.\n        y (np.array, optional): target. Defaults to None.\n    \"\"\"\n    X = ensure_dataframe(X)\n\n    # input validation\n    assert self.remainder in [\"passthrough\", \"drop\"]\n\n    # Convert to skorecard pipelines\n    # This does some checks on the pipelines\n    # and adds some convenience methods to the pipeline.\n    self.pre_pipeline_ = to_skorecard_pipeline(deepcopy(self.prebucketing_pipeline))\n    self.pipeline_ = to_skorecard_pipeline(deepcopy(self.bucketing_pipeline))\n\n    # Add/Overwrite specials to all pre-bucketers\n    for step in _get_all_steps(self.pre_pipeline_):\n        if hasattr(step, \"specials\") and len(step.specials) != 0 and len(self.specials) != 0:\n            # note, specials defined BucketingProcess level\n            # will overwrite any specials on bucketer level.\n            warnings.warn(f\"Overwriting specials of {step} with specials of bucketingprocess\", UserWarning)\n            step.specials = {**step.specials, **self.specials}\n        else:\n            step.specials = self.specials\n\n        if len(self.variables) != 0:\n            if len(step.variables) != 0:\n                warnings.warn(f\"Overwriting variables of {step} with variables of bucketingprocess\", UserWarning)\n            step.variables = self.variables\n\n        # Overwrite random_state to bucketers\n        if hasattr(step, \"random_state\") and self.random_state is not None:\n            if step.random_state is not None:\n                warnings.warn(\n                    f\"Overwriting random_state of {step} with random_state of bucketingprocess\", UserWarning\n                )\n            step.random_state = self.random_state\n\n    # Overwrite variables to all bucketers\n    if len(self.variables) != 0:\n        for step in _get_all_steps(self.pipeline_):\n            if len(step.variables) != 0:\n                warnings.warn(f\"Overwriting variables of {step} with variables of bucketingprocess\", UserWarning)\n            step.variables = self.variables\n\n    # Overwrite random_state to bucketers\n    for step in _get_all_steps(self.pipeline_):\n        if hasattr(step, \"random_state\") and self.random_state is not None:\n            if step.random_state is not None:\n                warnings.warn(\n                    f\"Overwriting random_state of {step} with random_state of bucketingprocess\", UserWarning\n                )\n            step.random_state = self.random_state\n\n    self._prebucketing_specials = self.specials\n    self._bucketing_specials = dict()  # will be determined later.\n\n    # Fit the prebucketing pipeline\n    X_prebucketed_ = self.pre_pipeline_.fit_transform(X, y)\n    assert isinstance(X_prebucketed_, pd.DataFrame)\n\n    # Calculate the prebucket tables.\n    self.prebucket_tables_ = dict()\n    for column in X.columns:\n        if column in self.pre_pipeline_.features_bucket_mapping_.maps.keys():\n            self.prebucket_tables_[column] = build_bucket_table(\n                X, y, column=column, bucket_mapping=self.pre_pipeline_.features_bucket_mapping_.get(column)\n            )\n\n    # Find the new bucket numbers of the specials after prebucketing,\n    for var, var_specials in self._prebucketing_specials.items():\n        bucket_labels = self.pre_pipeline_.features_bucket_mapping_.get(var).labels\n        new_specials = _find_remapped_specials(bucket_labels, var_specials)\n        if len(new_specials):\n            self._bucketing_specials[var] = new_specials\n\n    # Then assign the new specials to all bucketers in the bucketing pipeline\n    for step in self.pipeline_.steps:\n        if not isinstance(step, tuple):\n            step.specials = self._bucketing_specials\n        else:\n            step[1].specials = self._bucketing_specials\n\n    # Fit the bucketing pipeline\n    # And save the bucket mapping\n    self.pipeline_.fit(X_prebucketed_, y)\n\n    # Make sure all columns that are bucketed have also been pre-bucketed.\n    not_prebucketed = []\n    for col in self.pipeline_.features_bucket_mapping_.columns:\n        if self.pipeline_.features_bucket_mapping_.get(col).type == \"numerical\":\n            if col not in self.pre_pipeline_.features_bucket_mapping_.columns:\n                not_prebucketed.append(col)\n    if len(not_prebucketed):\n        msg = \"These numerical columns are bucketed but have not been pre-bucketed: \"\n        msg += f\"{', '.join(not_prebucketed)}.\\n\"\n        msg += \"Consider adding a numerical bucketer to the prebucketing pipeline,\"\n        msg += \"for example AsIsNumericalBucketer or DecisionTreeBucketer.\"\n        raise NotPreBucketedError(msg)\n\n    # Make sure all columns that have been pre-bucketed also have been bucketed\n    not_bucketed = []\n    for col in self.pre_pipeline_.features_bucket_mapping_.columns:\n        if self.pre_pipeline_.features_bucket_mapping_.get(col).type == \"numerical\":\n            if col not in self.pipeline_.features_bucket_mapping_.columns:\n                not_bucketed.append(col)\n    if len(not_bucketed):\n        msg = \"These numerical columns are prebucketed but have not been bucketed: \"\n        msg += f\"{', '.join(not_bucketed)}.\\n\"\n        msg += \"Consider updating the bucketing pipeline.\"\n        raise NotBucketedError(msg)\n\n    # calculate the bucket tables.\n    self.bucket_tables_ = dict()\n    for column in X.columns:\n        if column in self.pipeline_.features_bucket_mapping_.maps.keys():\n            self.bucket_tables_[column] = build_bucket_table(\n                X_prebucketed_,\n                y,\n                column=column,\n                bucket_mapping=self.pipeline_.features_bucket_mapping_.get(column),\n            )\n\n    # Calculate the summary\n    self._generate_summary(X, y)\n\n    return self\n</code></pre>"},{"location":"api/pipeline/BucketingProcess/#skorecard.pipeline.BucketingProcess.fit_interactive","title":"<code>fit_interactive(X, y=None, mode='external', **server_kwargs)</code>","text":"<p>Fit a bucketer and then interactive edit the fit using a dash app.</p> <p>Note we are using a jupyterdash app, which supports 3 different modes:</p> <ul> <li>'external' (default): Start dash server and print URL</li> <li>'inline': Start dash app inside an Iframe in the jupyter notebook</li> <li>'jupyterlab': Start dash app as a new tab inside jupyterlab</li> </ul> Source code in <code>skorecard/pipeline/bucketing_process.py</code> <pre><code>def fit_interactive(self, X, y=None, mode=\"external\", **server_kwargs):\n    \"\"\"\n    Fit a bucketer and then interactive edit the fit using a dash app.\n\n    Note we are using a [jupyterdash](https://medium.com/plotly/introducing-jupyterdash-811f1f57c02e) app,\n    which supports 3 different modes:\n\n    - 'external' (default): Start dash server and print URL\n    - 'inline': Start dash app inside an Iframe in the jupyter notebook\n    - 'jupyterlab': Start dash app as a new tab inside jupyterlab\n\n    \"\"\"\n    # We need to make sure we only fit if not already fitted\n    # This prevents a user losing manually defined boundaries\n    # when re-running .fit_interactive()\n    if not is_fitted(self):\n        self.fit(X, y)\n\n    self.app = JupyterDash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP])\n    add_bucketing_process_layout(self)\n    add_bucketing_process_callbacks(self, X, y)\n    self.app.run_server(mode=mode, **server_kwargs)\n</code></pre>"},{"location":"api/pipeline/BucketingProcess/#skorecard.pipeline.BucketingProcess.prebucket_table","title":"<code>prebucket_table(column)</code>","text":"<p>Generates the statistics for the buckets of a particular column.</p> <p>An example is seen below:</p> pre-bucket label Count Count (%) Non-event Event Event Rate WoE IV bucket 0 (-inf, 1.0) 479 7.98 300 179 37.37 0.73 0.05 0 1 [1.0, 2.0) 370 6.17 233 137 37.03 0.71 0.04 0 <p>Parameters:</p> Name Type Description Default <code>column</code> <code>str</code> <p>The column we wish to analyse</p> required <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>A pandas dataframe of the format above</p> Source code in <code>skorecard/pipeline/bucketing_process.py</code> <pre><code>def prebucket_table(self, column: str) -&gt; pd.DataFrame:\n    \"\"\"\n    Generates the statistics for the buckets of a particular column.\n\n    An example is seen below:\n\n    pre-bucket | label      | Count | Count (%) | Non-event | Event | Event Rate | WoE   | IV   | bucket\n    -----------|------------|-------|-----------|-----------|-------|------------|-------|------|------\n    0          | (-inf, 1.0)| 479   | 7.98      | 300       | 179   |  37.37     |  0.73 | 0.05 | 0\n    1          | [1.0, 2.0) | 370   | 6.17      | 233       | 137   |  37.03     |  0.71 | 0.04 | 0\n\n    Args:\n        column (str): The column we wish to analyse\n\n    Returns:\n        df (pd.DataFrame): A pandas dataframe of the format above\n    \"\"\"  # noqa\n    check_is_fitted(self)\n    if column not in self.prebucket_tables_.keys():\n        raise ValueError(f\"column '{column}' was not part of the pre-bucketing process\")\n\n    table = self.prebucket_tables_.get(column)\n    table = table.rename(columns={\"bucket_id\": \"pre-bucket\"})\n\n    # Find bucket for each pre-bucket\n    bucket_mapping = self.pipeline_.features_bucket_mapping_.get(column)\n    table[\"bucket\"] = bucket_mapping.transform(table[\"pre-bucket\"])\n\n    # Find out missing bucket\n    if -1 in table[\"pre-bucket\"].values:\n        table.loc[table[\"pre-bucket\"] == -1, \"bucket\"] = bucket_mapping.transform([np.nan])[0]\n\n    # Find out the 'other' bucket\n    if bucket_mapping.type == \"categorical\" and -2 in table[\"pre-bucket\"].values:\n        something_random = \"84a088e251d2fa058f37145222e536dc\"\n        table.loc[table[\"pre-bucket\"] == -2, \"bucket\"] = bucket_mapping.transform([something_random])[0]\n\n    return table\n</code></pre>"},{"location":"api/pipeline/BucketingProcess/#skorecard.pipeline.BucketingProcess.save_yml","title":"<code>save_yml(fout)</code>","text":"<p>Save the features bucket to a yaml file.</p> <p>Parameters:</p> Name Type Description Default <code>fout</code> <code>PathLike</code> <p>path for output file</p> required Source code in <code>skorecard/pipeline/bucketing_process.py</code> <pre><code>def save_yml(self, fout: PathLike) -&gt; None:\n    \"\"\"\n    Save the features bucket to a yaml file.\n\n    Args:\n        fout: path for output file\n    \"\"\"\n    check_is_fitted(self)\n    fbm = self.features_bucket_mapping_\n    if isinstance(fbm, dict):\n        FeaturesBucketMapping(fbm).save_yml(fout)\n    else:\n        fbm.save_yml(fout)\n</code></pre>"},{"location":"api/pipeline/BucketingProcess/#skorecard.pipeline.BucketingProcess.transform","title":"<code>transform(X)</code>","text":"<p>Transform <code>X</code> through the prebucketing and bucketing pipelines.</p> Source code in <code>skorecard/pipeline/bucketing_process.py</code> <pre><code>def transform(self, X):\n    \"\"\"\n    Transform `X` through the prebucketing and bucketing pipelines.\n    \"\"\"\n    check_is_fitted(self)\n    X_prebucketed = self.pre_pipeline_.transform(X)\n\n    new_X = self.pipeline_.transform(X_prebucketed)\n\n    if self.remainder == \"drop\":\n        return new_X[self.variables]\n    else:\n        return new_X\n</code></pre>"},{"location":"api/pipeline/KeepPandas/","title":"KeepPandas","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Wrapper to keep column names of pandas dataframes in a <code>scikit-learn</code> transformer.</p> <p>Any scikit-learn transformer wrapped in KeepPandas will return a <code>pd.DataFrame</code> on <code>.transform()</code>.</p> <p>Warning</p> <p>You should only use <code>KeepPandas()</code> when you know for sure <code>scikit-learn</code> did not change the order of your columns.</p> <p>Example:</p> <pre><code>from skorecard.pipeline import KeepPandas\nfrom skorecard import datasets\nfrom skorecard.bucketers import EqualWidthBucketer\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nX, y = datasets.load_uci_credit_card(return_X_y=True)\n\nbucket_pipeline = make_pipeline(\n    KeepPandas(StandardScaler()),\n    EqualWidthBucketer(n_bins=5, variables=['LIMIT_BAL', 'BILL_AMT1']),\n)\nbucket_pipeline.fit_transform(X, y)\n</code></pre> Source code in <code>skorecard/pipeline/pipeline.py</code> <pre><code>class KeepPandas(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Wrapper to keep column names of pandas dataframes in a `scikit-learn` transformer.\n\n    Any scikit-learn transformer wrapped in KeepPandas will return a `pd.DataFrame` on `.transform()`.\n\n    !!! warning\n        You should only use `KeepPandas()` when you know for sure `scikit-learn`\n        did not change the order of your columns.\n\n    Example:\n\n    ```python\n    from skorecard.pipeline import KeepPandas\n    from skorecard import datasets\n    from skorecard.bucketers import EqualWidthBucketer\n\n    from sklearn.pipeline import make_pipeline\n    from sklearn.preprocessing import StandardScaler\n\n    X, y = datasets.load_uci_credit_card(return_X_y=True)\n\n    bucket_pipeline = make_pipeline(\n        KeepPandas(StandardScaler()),\n        EqualWidthBucketer(n_bins=5, variables=['LIMIT_BAL', 'BILL_AMT1']),\n    )\n    bucket_pipeline.fit_transform(X, y)\n    ```\n    \"\"\"\n\n    def __init__(self, transformer):\n        \"\"\"Initialize.\"\"\"\n        self.transformer = transformer\n\n        # Warn if there is a chance order of columns are changed\n        if isinstance(transformer, Pipeline):\n            for step in _get_all_steps(transformer):\n                self._check_for_column_transformer(step)\n        else:\n            self._check_for_column_transformer(transformer)\n\n    def __repr__(self):\n        \"\"\"String representation.\"\"\"\n        return self.transformer.__repr__()\n\n    @staticmethod\n    def _check_for_column_transformer(obj):\n        msg = \"sklearn.compose.ColumnTransformer can change the order of columns\"\n        msg += \", be very careful when using with KeepPandas()\"\n        if type(obj).__name__ == \"ColumnTransformer\":\n            logging.warning(msg)\n\n    def fit(self, X, y=None, *args, **kwargs):\n        \"\"\"Fit estimator.\"\"\"\n        assert isinstance(X, pd.DataFrame)\n        self.columns_ = list(X.columns)\n        self.transformer.fit(X, y, *args, **kwargs)\n        return self\n\n    def transform(self, X, *args, **kwargs):\n        \"\"\"Transform X.\"\"\"\n        check_is_fitted(self)\n        new_X = self.transformer.transform(X, *args, **kwargs)\n        return pd.DataFrame(new_X, columns=self.columns_)\n\n    def get_feature_names(self):\n        \"\"\"Return estimator feature names.\"\"\"\n        check_is_fitted(self)\n        return self.columns_\n</code></pre>"},{"location":"api/pipeline/KeepPandas/#skorecard.pipeline.KeepPandas.__init__","title":"<code>__init__(transformer)</code>","text":"<p>Initialize.</p> Source code in <code>skorecard/pipeline/pipeline.py</code> <pre><code>def __init__(self, transformer):\n    \"\"\"Initialize.\"\"\"\n    self.transformer = transformer\n\n    # Warn if there is a chance order of columns are changed\n    if isinstance(transformer, Pipeline):\n        for step in _get_all_steps(transformer):\n            self._check_for_column_transformer(step)\n    else:\n        self._check_for_column_transformer(transformer)\n</code></pre>"},{"location":"api/pipeline/KeepPandas/#skorecard.pipeline.KeepPandas.__repr__","title":"<code>__repr__()</code>","text":"<p>String representation.</p> Source code in <code>skorecard/pipeline/pipeline.py</code> <pre><code>def __repr__(self):\n    \"\"\"String representation.\"\"\"\n    return self.transformer.__repr__()\n</code></pre>"},{"location":"api/pipeline/KeepPandas/#skorecard.pipeline.KeepPandas.fit","title":"<code>fit(X, y=None, *args, **kwargs)</code>","text":"<p>Fit estimator.</p> Source code in <code>skorecard/pipeline/pipeline.py</code> <pre><code>def fit(self, X, y=None, *args, **kwargs):\n    \"\"\"Fit estimator.\"\"\"\n    assert isinstance(X, pd.DataFrame)\n    self.columns_ = list(X.columns)\n    self.transformer.fit(X, y, *args, **kwargs)\n    return self\n</code></pre>"},{"location":"api/pipeline/KeepPandas/#skorecard.pipeline.KeepPandas.get_feature_names","title":"<code>get_feature_names()</code>","text":"<p>Return estimator feature names.</p> Source code in <code>skorecard/pipeline/pipeline.py</code> <pre><code>def get_feature_names(self):\n    \"\"\"Return estimator feature names.\"\"\"\n    check_is_fitted(self)\n    return self.columns_\n</code></pre>"},{"location":"api/pipeline/KeepPandas/#skorecard.pipeline.KeepPandas.transform","title":"<code>transform(X, *args, **kwargs)</code>","text":"<p>Transform X.</p> Source code in <code>skorecard/pipeline/pipeline.py</code> <pre><code>def transform(self, X, *args, **kwargs):\n    \"\"\"Transform X.\"\"\"\n    check_is_fitted(self)\n    new_X = self.transformer.transform(X, *args, **kwargs)\n    return pd.DataFrame(new_X, columns=self.columns_)\n</code></pre>"},{"location":"api/pipeline/SkorecardPipeline/","title":"SkorecardPipeline","text":"<p>               Bases: <code>Pipeline</code>, <code>PlotBucketMethod</code>, <code>BucketTableMethod</code>, <code>SummaryMethod</code></p> <p>A sklearn Pipeline with several attribute and methods added.</p> <p>This Pipeline of bucketers behaves more like a bucketer and adds:</p> <ul> <li><code>.summary()</code>: See which columns are bucketed</li> <li><code>.plot_bucket()</code>: Plot buckets of a column</li> <li><code>.bucket_table()</code>: Table with buckets of a column</li> <li><code>.save_to_yaml()</code>: Save information necessary for bucketing to a YAML file</li> <li><code>.features_bucket_mapping_</code>: Access bucketing information</li> <li><code>.fit_interactive()</code>: Edit fitted buckets interactively in a dash app</li> </ul> <pre><code>from skorecard.pipeline.pipeline import SkorecardPipeline\nfrom skorecard.bucketers import DecisionTreeBucketer, OrdinalCategoricalBucketer\nfrom skorecard import datasets\n\npipe = SkorecardPipeline([\n    ('decisiontreebucketer', DecisionTreeBucketer(variables = [\"LIMIT_BAL\", \"BILL_AMT1\"],max_n_bins=5)),\n    ('ordinalcategoricalbucketer', OrdinalCategoricalBucketer(variables = [\"EDUCATION\", \"MARRIAGE\"], tol =0.05)),\n])\n\ndf = datasets.load_uci_credit_card(as_frame=True)\nfeatures = [\"LIMIT_BAL\", \"BILL_AMT1\", \"EDUCATION\", \"MARRIAGE\"]\nX = df[features]\ny = df[\"default\"].values\n\npipe.fit(X, y)\npipe.bucket_table('LIMIT_BAL')\n</code></pre> Source code in <code>skorecard/pipeline/pipeline.py</code> <pre><code>class SkorecardPipeline(Pipeline, PlotBucketMethod, BucketTableMethod, SummaryMethod):\n    \"\"\"\n    A sklearn Pipeline with several attribute and methods added.\n\n    This Pipeline of bucketers behaves more like a bucketer and adds:\n\n    - `.summary()`: See which columns are bucketed\n    - `.plot_bucket()`: Plot buckets of a column\n    - `.bucket_table()`: Table with buckets of a column\n    - `.save_to_yaml()`: Save information necessary for bucketing to a YAML file\n    - `.features_bucket_mapping_`: Access bucketing information\n    - `.fit_interactive()`: Edit fitted buckets interactively in a dash app\n\n    ```python\n    from skorecard.pipeline.pipeline import SkorecardPipeline\n    from skorecard.bucketers import DecisionTreeBucketer, OrdinalCategoricalBucketer\n    from skorecard import datasets\n\n    pipe = SkorecardPipeline([\n        ('decisiontreebucketer', DecisionTreeBucketer(variables = [\"LIMIT_BAL\", \"BILL_AMT1\"],max_n_bins=5)),\n        ('ordinalcategoricalbucketer', OrdinalCategoricalBucketer(variables = [\"EDUCATION\", \"MARRIAGE\"], tol =0.05)),\n    ])\n\n    df = datasets.load_uci_credit_card(as_frame=True)\n    features = [\"LIMIT_BAL\", \"BILL_AMT1\", \"EDUCATION\", \"MARRIAGE\"]\n    X = df[features]\n    y = df[\"default\"].values\n\n    pipe.fit(X, y)\n    pipe.bucket_table('LIMIT_BAL')\n    ```\n    \"\"\"\n\n    def __init__(self, steps, *, memory=None, verbose=False):\n        \"\"\"\n        Wraps sklearn Pipeline.\n        \"\"\"\n        super().__init__(steps=steps, memory=memory, verbose=verbose)\n        self._check_pipeline_all_bucketers(self)\n        self._check_pipeline_duplicated_columns(self)\n\n    @property\n    def features_bucket_mapping_(self):\n        \"\"\"\n        Retrieve features bucket mapping.\n        \"\"\"\n        check_is_fitted(self.steps[-1][1])\n        return get_features_bucket_mapping(Pipeline(self.steps))\n\n    @property\n    def bucket_tables_(self):\n        \"\"\"\n        Retrieve bucket tables.\n\n        Used by .bucket_table()\n        \"\"\"\n        check_is_fitted(self.steps[-1][1])\n        bucket_tables = dict()\n        for step in self.steps:\n            bucket_tables.update(step[1].bucket_tables_)\n        return bucket_tables\n\n    @property\n    def summary_dict_(self) -&gt; Dict:\n        \"\"\"\n        Retrieve summary_dicts and combine.\n\n        Used by .summary()\n        \"\"\"\n        summary_dict = {}\n        for step in self.steps:\n            summary_dict.update(step[1].summary_dict_)\n        return summary_dict\n\n    def save_yml(self, fout):\n        \"\"\"\n        Save the features bucket to a yaml file.\n\n        Args:\n            fout: file output\n        \"\"\"\n        check_is_fitted(self.steps[-1][1])\n        self.features_bucket_mapping_.save_yml(fout)\n\n    @staticmethod\n    def _check_pipeline_duplicated_columns(pipeline: Pipeline) -&gt; None:\n        \"\"\"\n        Check that the pipeline has no duplicated columns.\n\n        This check only works on fitted pipelines!\n        \"\"\"\n        assert isinstance(pipeline, Pipeline)\n\n        bucketers_vars = []\n        bucketers_on_all = []\n        bucketers_with_vars = []\n\n        for step in _get_all_steps(pipeline):\n            if is_fitted(step):\n                if hasattr(step, \"variables_\"):\n                    if len(step.variables_) == 0:\n                        bucketers_vars += [\"**all**\"]\n                        bucketers_on_all += [step]\n                    else:\n                        bucketers_vars += step.variables_\n                        bucketers_with_vars += [step]\n            else:\n                if hasattr(step, \"variables\"):\n                    if len(step.variables) == 0:\n                        bucketers_vars += [\"**all**\"]\n                        bucketers_on_all += [step]\n                    else:\n                        bucketers_vars += step.variables\n                        bucketers_with_vars += [step]\n\n        if len(list(set(bucketers_vars))) &gt; 1 and \"**all**\" in list(set(bucketers_vars)):\n            msg = \"A SkorecardPipeline should bucket each feature only once.\\n\"\n            msg += f\"These bucketers bucket all features: {bucketers_on_all}\\n\"\n            msg += f\"While these bucket specific ones: {bucketers_with_vars}\\n\"\n            msg += \"This means some features would have been bucketed sequentially.\"\n            msg += \"To solve this, either use a BucketingProcess, or remove the duplicates from one of the bucketers.\"\n            msg += \"Remember that if you don't specify 'variables', a bucketer will bucket all columns.\"\n            raise BucketingPipelineError(msg)\n\n        if len(set(bucketers_vars)) != len(bucketers_vars):\n            values, counts = np.unique(bucketers_vars, return_counts=True)\n            duplicates = list(set(values[counts &gt; 1]))\n\n            msg = \"A SkorecardPipeline should bucket each feature only once. \"\n            msg += f\"The features {duplicates} appear in multiple bucketers, \"\n            msg += \"meaning they would have been bucketed sequentially.\"\n            msg += \"To solve this, either use a BucketingProcess, or remove the duplicates from one of the bucketers.\"\n            msg += \"Remember that if you don't specify 'variables', a bucketer will bucket all columns.\"\n            raise BucketingPipelineError(msg)\n\n    @staticmethod\n    def _check_pipeline_all_bucketers(pipeline: Pipeline) -&gt; None:\n        \"\"\"\n        Ensure all specified bucketing steps are skorecard bucketers.\n\n        Args:\n            pipeline: scikit-learn pipeline.\n        \"\"\"\n        assert isinstance(pipeline, Pipeline)\n\n        for step in _get_all_steps(pipeline):\n            if all(x not in str(type(step)) for x in [\"bucketing_process\", \"skorecard.bucketers\"]):\n                msg = \"All bucketing steps must be skorecard bucketers.\"\n                msg += f\"Remove {step} from the pipeline.\"\n                raise NotBucketObjectError(msg)\n\n    def fit_interactive(self, X, y=None, mode=\"external\"):\n        \"\"\"\n        Fit a bucketer and then interactively edit the fit using a dash app.\n\n        Note we are using a [jupyterdash](https://medium.com/plotly/introducing-jupyterdash-811f1f57c02e) app,\n        which supports 3 different modes:\n\n        - 'external' (default): Start dash server and print URL\n        - 'inline': Start dash app inside an Iframe in the jupyter notebook\n        - 'jupyterlab': Start dash app as a new tab inside jupyterlab\n\n        \"\"\"\n        # We need to make sure we only fit if not already fitted\n        # This prevents a user losing manually defined boundaries\n        # when re-running .fit_interactive()\n        if not is_fitted(self):\n            self.fit(X, y)\n\n        self.app = JupyterDash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP])\n        add_basic_layout(self)\n        add_bucketing_callbacks(self, X, y)\n        self.app.run_server(mode=mode)\n\n    def _update_column_fit(self, X, y, feature, special, splits, right, generate_summary=False):\n        \"\"\"\n        Extract out part of the fit for a column.\n\n        Useful when we want to interactively update the fit.\n        \"\"\"\n        for step in self.steps:\n            if feature in step[1].variables:\n                step[1]._update_column_fit(\n                    X=X,\n                    y=y,\n                    feature=feature,\n                    special=special,\n                    splits=splits,\n                    right=right,\n                    generate_summary=generate_summary,\n                )\n</code></pre>"},{"location":"api/pipeline/SkorecardPipeline/#skorecard.pipeline.SkorecardPipeline.bucket_tables_","title":"<code>bucket_tables_</code>  <code>property</code>","text":"<p>Retrieve bucket tables.</p> <p>Used by .bucket_table()</p>"},{"location":"api/pipeline/SkorecardPipeline/#skorecard.pipeline.SkorecardPipeline.features_bucket_mapping_","title":"<code>features_bucket_mapping_</code>  <code>property</code>","text":"<p>Retrieve features bucket mapping.</p>"},{"location":"api/pipeline/SkorecardPipeline/#skorecard.pipeline.SkorecardPipeline.summary_dict_","title":"<code>summary_dict_: Dict</code>  <code>property</code>","text":"<p>Retrieve summary_dicts and combine.</p> <p>Used by .summary()</p>"},{"location":"api/pipeline/SkorecardPipeline/#skorecard.pipeline.SkorecardPipeline.__init__","title":"<code>__init__(steps, *, memory=None, verbose=False)</code>","text":"<p>Wraps sklearn Pipeline.</p> Source code in <code>skorecard/pipeline/pipeline.py</code> <pre><code>def __init__(self, steps, *, memory=None, verbose=False):\n    \"\"\"\n    Wraps sklearn Pipeline.\n    \"\"\"\n    super().__init__(steps=steps, memory=memory, verbose=verbose)\n    self._check_pipeline_all_bucketers(self)\n    self._check_pipeline_duplicated_columns(self)\n</code></pre>"},{"location":"api/pipeline/SkorecardPipeline/#skorecard.pipeline.SkorecardPipeline.fit_interactive","title":"<code>fit_interactive(X, y=None, mode='external')</code>","text":"<p>Fit a bucketer and then interactively edit the fit using a dash app.</p> <p>Note we are using a jupyterdash app, which supports 3 different modes:</p> <ul> <li>'external' (default): Start dash server and print URL</li> <li>'inline': Start dash app inside an Iframe in the jupyter notebook</li> <li>'jupyterlab': Start dash app as a new tab inside jupyterlab</li> </ul> Source code in <code>skorecard/pipeline/pipeline.py</code> <pre><code>def fit_interactive(self, X, y=None, mode=\"external\"):\n    \"\"\"\n    Fit a bucketer and then interactively edit the fit using a dash app.\n\n    Note we are using a [jupyterdash](https://medium.com/plotly/introducing-jupyterdash-811f1f57c02e) app,\n    which supports 3 different modes:\n\n    - 'external' (default): Start dash server and print URL\n    - 'inline': Start dash app inside an Iframe in the jupyter notebook\n    - 'jupyterlab': Start dash app as a new tab inside jupyterlab\n\n    \"\"\"\n    # We need to make sure we only fit if not already fitted\n    # This prevents a user losing manually defined boundaries\n    # when re-running .fit_interactive()\n    if not is_fitted(self):\n        self.fit(X, y)\n\n    self.app = JupyterDash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP])\n    add_basic_layout(self)\n    add_bucketing_callbacks(self, X, y)\n    self.app.run_server(mode=mode)\n</code></pre>"},{"location":"api/pipeline/SkorecardPipeline/#skorecard.pipeline.SkorecardPipeline.save_yml","title":"<code>save_yml(fout)</code>","text":"<p>Save the features bucket to a yaml file.</p> <p>Parameters:</p> Name Type Description Default <code>fout</code> <p>file output</p> required Source code in <code>skorecard/pipeline/pipeline.py</code> <pre><code>def save_yml(self, fout):\n    \"\"\"\n    Save the features bucket to a yaml file.\n\n    Args:\n        fout: file output\n    \"\"\"\n    check_is_fitted(self.steps[-1][1])\n    self.features_bucket_mapping_.save_yml(fout)\n</code></pre>"},{"location":"api/pipeline/to_skorecard_pipeline/","title":"To skorecard pipeline","text":"<p>Transform a scikit-learn Pipeline to a SkorecardPipeline.</p> <p>A SkorecardPipeline is a normal scikit-learn pipeline with some extra methods and attributes.</p> <p>Example:</p> <pre><code>from skorecard.pipeline.pipeline import SkorecardPipeline, to_skorecard_pipeline\nfrom skorecard.bucketers import DecisionTreeBucketer, OrdinalCategoricalBucketer\nfrom skorecard import datasets\n\nfrom sklearn.pipeline import make_pipeline\n\npipe = make_pipeline(\n    DecisionTreeBucketer(variables = [\"LIMIT_BAL\", \"BILL_AMT1\"],max_n_bins=5),\n    OrdinalCategoricalBucketer(variables = [\"EDUCATION\", \"MARRIAGE\"], tol =0.05)\n)\nsk_pipe = to_skorecard_pipeline(pipe)\n\ndf = datasets.load_uci_credit_card(as_frame=True)\n\nfeatures = [\"LIMIT_BAL\", \"BILL_AMT1\", \"EDUCATION\", \"MARRIAGE\"]\nX = df[features]\ny = df[\"default\"].values\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>pipeline</code> <code>Pipeline</code> <p><code>scikit-learn</code> pipeline instance.</p> required <p>Returns:</p> Name Type Description <code>pipeline</code> <code>SkorecardPipeline</code> <p>modified pipeline instance.</p> Source code in <code>skorecard/pipeline/pipeline.py</code> <pre><code>def to_skorecard_pipeline(pipeline: Pipeline) -&gt; SkorecardPipeline:\n    \"\"\"\n    Transform a scikit-learn Pipeline to a SkorecardPipeline.\n\n    A SkorecardPipeline is a normal scikit-learn pipeline with some extra methods and attributes.\n\n    Example:\n\n    ```python\n    from skorecard.pipeline.pipeline import SkorecardPipeline, to_skorecard_pipeline\n    from skorecard.bucketers import DecisionTreeBucketer, OrdinalCategoricalBucketer\n    from skorecard import datasets\n\n    from sklearn.pipeline import make_pipeline\n\n    pipe = make_pipeline(\n        DecisionTreeBucketer(variables = [\"LIMIT_BAL\", \"BILL_AMT1\"],max_n_bins=5),\n        OrdinalCategoricalBucketer(variables = [\"EDUCATION\", \"MARRIAGE\"], tol =0.05)\n    )\n    sk_pipe = to_skorecard_pipeline(pipe)\n\n    df = datasets.load_uci_credit_card(as_frame=True)\n\n    features = [\"LIMIT_BAL\", \"BILL_AMT1\", \"EDUCATION\", \"MARRIAGE\"]\n    X = df[features]\n    y = df[\"default\"].values\n    ```\n\n    Args:\n        pipeline (Pipeline): `scikit-learn` pipeline instance.\n\n    Returns:\n        pipeline (skorecard.pipeline.SkorecardPipeline): modified pipeline instance.\n    \"\"\"\n    assert isinstance(pipeline, Pipeline)\n    if isinstance(pipeline, SkorecardPipeline):\n        return pipeline\n    else:\n        return SkorecardPipeline(steps=pipeline.steps, memory=pipeline.memory, verbose=pipeline.verbose)\n</code></pre>"},{"location":"api/preprocessing/ColumnSelector/","title":"ColumnSelector","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Transformer that performs selection of variables from a pandas dataframe.</p> <p>Useful in pipelines, where we require a step that selects features.</p> <p>Example:</p> <pre><code>from skorecard import datasets\nfrom skorecard.preprocessing import ColumnSelector\n\nX, y = datasets.load_uci_credit_card(return_X_y=True)\ncs = ColumnSelector(variables=['EDUCATION'])\nassert cs.fit_transform(X, y).columns == ['EDUCATION']\n</code></pre> Source code in <code>skorecard/preprocessing/preprocessing.py</code> <pre><code>class ColumnSelector(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Transformer that performs selection of variables from a pandas dataframe.\n\n    Useful in pipelines, where we require a step that selects features.\n\n    Example:\n\n    ```python\n    from skorecard import datasets\n    from skorecard.preprocessing import ColumnSelector\n\n    X, y = datasets.load_uci_credit_card(return_X_y=True)\n    cs = ColumnSelector(variables=['EDUCATION'])\n    assert cs.fit_transform(X, y).columns == ['EDUCATION']\n    ```\n    \"\"\"\n\n    def __init__(self, variables: List = []):\n        \"\"\"Transformer constructor.\n\n        Args:\n            variables: list of columns to select. Default value is set to None - in this case, there is no selection of\n                columns.\n        \"\"\"\n        self.variables = variables\n\n    def fit(self, X, y=None):\n        \"\"\"\n        Fit the transformer.\n\n        Here to be compliant with the sklearn API, does not fit anything.\n        \"\"\"\n        # scikit-learn requires checking that X has same shape on transform\n        # this is because scikit-learn is still positional based (no column names used)\n        self.n_train_features_ = X.shape[1]\n\n        return self\n\n    def transform(self, X):\n        \"\"\"\n        Selects the columns.\n\n        Args:\n            X (pd.DataFrame): Dataset\n        \"\"\"\n        X = ensure_dataframe(X)\n        if hasattr(self, \"n_train_features_\"):\n            if X.shape[1] != self.n_train_features_:\n                msg = f\"Number of features in X ({X.shape[1]}) is different \"\n                msg += f\"from the number of features in X during fit ({self.n_train_features_})\"\n                raise ValueError(msg)\n\n        if len(self.variables) &gt; 0:\n            return X[self.variables]\n        else:\n            return X\n\n    def _more_tags(self):\n        \"\"\"\n        Estimator tags are annotations of estimators that allow programmatic inspection of their capabilities.\n\n        See https://scikit-learn.org/stable/developers/develop.html#estimator-tags\n        \"\"\"  # noqa\n        return {\"requires_fit\": False}\n</code></pre>"},{"location":"api/preprocessing/ColumnSelector/#skorecard.preprocessing.ColumnSelector.__init__","title":"<code>__init__(variables=[])</code>","text":"<p>Transformer constructor.</p> <p>Parameters:</p> Name Type Description Default <code>variables</code> <code>List</code> <p>list of columns to select. Default value is set to None - in this case, there is no selection of columns.</p> <code>[]</code> Source code in <code>skorecard/preprocessing/preprocessing.py</code> <pre><code>def __init__(self, variables: List = []):\n    \"\"\"Transformer constructor.\n\n    Args:\n        variables: list of columns to select. Default value is set to None - in this case, there is no selection of\n            columns.\n    \"\"\"\n    self.variables = variables\n</code></pre>"},{"location":"api/preprocessing/ColumnSelector/#skorecard.preprocessing.ColumnSelector.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Fit the transformer.</p> <p>Here to be compliant with the sklearn API, does not fit anything.</p> Source code in <code>skorecard/preprocessing/preprocessing.py</code> <pre><code>def fit(self, X, y=None):\n    \"\"\"\n    Fit the transformer.\n\n    Here to be compliant with the sklearn API, does not fit anything.\n    \"\"\"\n    # scikit-learn requires checking that X has same shape on transform\n    # this is because scikit-learn is still positional based (no column names used)\n    self.n_train_features_ = X.shape[1]\n\n    return self\n</code></pre>"},{"location":"api/preprocessing/ColumnSelector/#skorecard.preprocessing.ColumnSelector.transform","title":"<code>transform(X)</code>","text":"<p>Selects the columns.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>Dataset</p> required Source code in <code>skorecard/preprocessing/preprocessing.py</code> <pre><code>def transform(self, X):\n    \"\"\"\n    Selects the columns.\n\n    Args:\n        X (pd.DataFrame): Dataset\n    \"\"\"\n    X = ensure_dataframe(X)\n    if hasattr(self, \"n_train_features_\"):\n        if X.shape[1] != self.n_train_features_:\n            msg = f\"Number of features in X ({X.shape[1]}) is different \"\n            msg += f\"from the number of features in X during fit ({self.n_train_features_})\"\n            raise ValueError(msg)\n\n    if len(self.variables) &gt; 0:\n        return X[self.variables]\n    else:\n        return X\n</code></pre>"},{"location":"api/preprocessing/WoeEncoder/","title":"WoeEncoder","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Transformer that encodes unique values in features to their Weight of Evidence estimation.</p> <p>This class has been deprecated in favor of category_encoders.woe.WOEEncoder</p> <p>Only works for binary classification (target y has 0 and 1 values).</p> <p>The weight of evidence is given by: <code>np.log( p(1) / p(0) )</code> The target probability ratio is given by: <code>p(1) / p(0)</code></p> <p>For example in the variable colour, if the mean of the target = 1 for blue is 0.8 and the mean of the target = 0 is 0.2, blue will be replaced by: np.log(0.8/0.2) = 1.386 if log_ratio is selected. Alternatively, blue will be replaced by 0.8 / 0.2 = 4 if ratio is selected.</p> <p>More formally:</p> <ul> <li>for each unique value \ud835\udc65,  consider the corresponding rows in the training set</li> <li>compute what percentage of positives is in these rows, compared to the whole set</li> <li>compute what percentage of negatives is in these rows, compared to the whole set</li> <li>take the ratio of these percentages</li> <li>take the natural logarithm of that ratio to get the weight of evidence corresponding to  \ud835\udc65,  so that  \ud835\udc4a\ud835\udc42\ud835\udc38(\ud835\udc65)  is either positive or negative according to whether  \ud835\udc65  is more representative of positives or negatives</li> </ul> <p>More details:</p> <ul> <li>blogpost on weight of evidence</li> </ul> <p>Example:</p> <pre><code>from skorecard import datasets\nfrom skorecard.preprocessing import WoeEncoder\n\nX, y = datasets.load_uci_credit_card(return_X_y=True)\nwe = WoeEncoder(variables=['EDUCATION'])\nwe.fit_transform(X, y)\nwe.fit_transform(X, y)['EDUCATION'].value_counts()\n</code></pre> <p>Credits: Some inspiration taken from feature_engine.categorical_encoders.</p> Source code in <code>skorecard/preprocessing/_WoEEncoder.py</code> <pre><code>class WoeEncoder(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Transformer that encodes unique values in features to their Weight of Evidence estimation.\n\n    **This class has been deprecated in favor of category_encoders.woe.WOEEncoder**\n\n    Only works for binary classification (target y has 0 and 1 values).\n\n    The weight of evidence is given by: `np.log( p(1) / p(0) )`\n    The target probability ratio is given by: `p(1) / p(0)`\n\n    For example in the variable colour, if the mean of the target = 1 for blue is 0.8 and\n    the mean of the target = 0 is 0.2, blue will be replaced by: np.log(0.8/0.2) = 1.386\n    if log_ratio is selected. Alternatively, blue will be replaced by 0.8 / 0.2 = 4 if ratio is selected.\n\n    More formally:\n\n    - for each unique value \ud835\udc65,  consider the corresponding rows in the training set\n    - compute what percentage of positives is in these rows, compared to the whole set\n    - compute what percentage of negatives is in these rows, compared to the whole set\n    - take the ratio of these percentages\n    - take the natural logarithm of that ratio to get the weight of evidence corresponding to  \ud835\udc65,  so that  \ud835\udc4a\ud835\udc42\ud835\udc38(\ud835\udc65)  is either positive or negative according to whether  \ud835\udc65  is more representative of positives or negatives\n\n    More details:\n\n    - [blogpost on weight of evidence](https://multithreaded.stitchfix.com/blog/2015/08/13/weight-of-evidence/)\n\n    Example:\n\n    ```python\n    from skorecard import datasets\n    from skorecard.preprocessing import WoeEncoder\n\n    X, y = datasets.load_uci_credit_card(return_X_y=True)\n    we = WoeEncoder(variables=['EDUCATION'])\n    we.fit_transform(X, y)\n    we.fit_transform(X, y)['EDUCATION'].value_counts()\n    ```\n\n    Credits: Some inspiration taken from [feature_engine.categorical_encoders](https://feature-engine.readthedocs.io/en/latest/encoding/index.html).\n    \"\"\"  # noqa\n\n    def __init__(self, epsilon=0.0001, variables=[], handle_unknown=\"value\"):\n        \"\"\"\n        Constructor for WoEEncoder.\n\n        Args:\n            epsilon (float): Amount to be added to relative counts in order to avoid division by zero in the WOE\n                calculation.\n            variables (list): The features to bucket. Uses all features if not defined.\n            handle_unknown (str): How to handle any new values encountered in X on transform().\n                options are 'return_nan', 'error' and 'value', defaults to 'value', which will assume WOE=0.\n        \"\"\"\n        self.epsilon = epsilon\n        self.variables = variables\n        self.handle_unknown = handle_unknown\n\n        warnings.warn(\n            \"This encoder will be deprecated. Please use category_encoders.woe.WOEEncoder instead.\", DeprecationWarning\n        )\n\n    def fit(self, X, y):\n        \"\"\"Calculate the WOE for every column.\n\n        Args:\n            X (np.array): (binned) features\n            y (np.array): target\n        \"\"\"\n        assert self.epsilon &gt;= 0\n        # Check data\n        X = ensure_dataframe(X)\n        assert y is not None, \"WoEBucketer needs a target y\"\n        y = BaseBucketer._check_y(y)\n\n        y = y.astype(float)\n        if len(np.unique(y)) &gt; 2:\n            raise AssertionError(\"WoEBucketer is only suited for binary classification\")\n        self.variables_ = BaseBucketer._check_variables(X, self.variables)\n\n        # WoE currently does not support NAs\n        # This is also flagged in self._more_tags()\n        # We could treat missing values as a separate bin (-1) and thus handle seamlessly.\n        BaseBucketer._check_contains_na(X, self.variables_)\n\n        # scikit-learn requires checking that X has same shape on transform\n        # this is because scikit-learn is still positional based (no column names used)\n        self.n_train_features_ = X.shape[1]\n\n        self.woe_mapping_ = {}\n        for var in self.variables_:\n            t = woe_1d(X[var], y, epsilon=self.epsilon)\n\n            woe_dict = t[\"woe\"].to_dict()\n            # If new categories encountered, returns WoE = 0\n            if self.handle_unknown == \"value\":\n                woe_dict = defaultdict(int, woe_dict)\n\n            self.woe_mapping_[var] = woe_dict\n\n        return self\n\n    def transform(self, X):\n        \"\"\"Transform X to weight of evidence encoding.\n\n        Args:\n            X (pd.DataFrame): dataset\n        \"\"\"\n        assert self.handle_unknown in [\"value\", \"error\", \"return_nan\"]\n        check_is_fitted(self)\n        X = ensure_dataframe(X)\n\n        if X.shape[1] != self.n_train_features_:\n            msg = f\"Number of features in X ({X.shape[1]}) is different \"\n            msg += f\"from the number of features in X during fit ({self.n_train_features_})\"\n            raise ValueError(msg)\n\n        for feature in self.variables_:\n            woe_dict = self.woe_mapping_.get(feature)\n            if self.handle_unknown == \"error\":\n                new_cats = [x for x in list(X[feature].unique()) if x not in list(woe_dict.keys())]\n                if len(new_cats) &gt; 0:\n                    msg = \"WoEEncoder encountered unknown new categories \"\n                    msg += f\"in column {feature} on .transform(): {new_cats}\"\n                    raise AssertionError(msg)\n\n            X[feature] = X[feature].map(woe_dict)\n\n        return X\n\n    def _more_tags(self):\n        \"\"\"\n        Estimator tags are annotations of estimators that allow programmatic inspection of their capabilities.\n\n        See https://scikit-learn.org/stable/developers/develop.html#estimator-tags\n        \"\"\"  # noqa\n        return {\"binary_only\": True, \"allow_nan\": False}\n</code></pre>"},{"location":"api/preprocessing/WoeEncoder/#skorecard.preprocessing.WoeEncoder.__init__","title":"<code>__init__(epsilon=0.0001, variables=[], handle_unknown='value')</code>","text":"<p>Constructor for WoEEncoder.</p> <p>Parameters:</p> Name Type Description Default <code>epsilon</code> <code>float</code> <p>Amount to be added to relative counts in order to avoid division by zero in the WOE calculation.</p> <code>0.0001</code> <code>variables</code> <code>list</code> <p>The features to bucket. Uses all features if not defined.</p> <code>[]</code> <code>handle_unknown</code> <code>str</code> <p>How to handle any new values encountered in X on transform(). options are 'return_nan', 'error' and 'value', defaults to 'value', which will assume WOE=0.</p> <code>'value'</code> Source code in <code>skorecard/preprocessing/_WoEEncoder.py</code> <pre><code>def __init__(self, epsilon=0.0001, variables=[], handle_unknown=\"value\"):\n    \"\"\"\n    Constructor for WoEEncoder.\n\n    Args:\n        epsilon (float): Amount to be added to relative counts in order to avoid division by zero in the WOE\n            calculation.\n        variables (list): The features to bucket. Uses all features if not defined.\n        handle_unknown (str): How to handle any new values encountered in X on transform().\n            options are 'return_nan', 'error' and 'value', defaults to 'value', which will assume WOE=0.\n    \"\"\"\n    self.epsilon = epsilon\n    self.variables = variables\n    self.handle_unknown = handle_unknown\n\n    warnings.warn(\n        \"This encoder will be deprecated. Please use category_encoders.woe.WOEEncoder instead.\", DeprecationWarning\n    )\n</code></pre>"},{"location":"api/preprocessing/WoeEncoder/#skorecard.preprocessing.WoeEncoder.fit","title":"<code>fit(X, y)</code>","text":"<p>Calculate the WOE for every column.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array</code> <p>(binned) features</p> required <code>y</code> <code>array</code> <p>target</p> required Source code in <code>skorecard/preprocessing/_WoEEncoder.py</code> <pre><code>def fit(self, X, y):\n    \"\"\"Calculate the WOE for every column.\n\n    Args:\n        X (np.array): (binned) features\n        y (np.array): target\n    \"\"\"\n    assert self.epsilon &gt;= 0\n    # Check data\n    X = ensure_dataframe(X)\n    assert y is not None, \"WoEBucketer needs a target y\"\n    y = BaseBucketer._check_y(y)\n\n    y = y.astype(float)\n    if len(np.unique(y)) &gt; 2:\n        raise AssertionError(\"WoEBucketer is only suited for binary classification\")\n    self.variables_ = BaseBucketer._check_variables(X, self.variables)\n\n    # WoE currently does not support NAs\n    # This is also flagged in self._more_tags()\n    # We could treat missing values as a separate bin (-1) and thus handle seamlessly.\n    BaseBucketer._check_contains_na(X, self.variables_)\n\n    # scikit-learn requires checking that X has same shape on transform\n    # this is because scikit-learn is still positional based (no column names used)\n    self.n_train_features_ = X.shape[1]\n\n    self.woe_mapping_ = {}\n    for var in self.variables_:\n        t = woe_1d(X[var], y, epsilon=self.epsilon)\n\n        woe_dict = t[\"woe\"].to_dict()\n        # If new categories encountered, returns WoE = 0\n        if self.handle_unknown == \"value\":\n            woe_dict = defaultdict(int, woe_dict)\n\n        self.woe_mapping_[var] = woe_dict\n\n    return self\n</code></pre>"},{"location":"api/preprocessing/WoeEncoder/#skorecard.preprocessing.WoeEncoder.transform","title":"<code>transform(X)</code>","text":"<p>Transform X to weight of evidence encoding.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>dataset</p> required Source code in <code>skorecard/preprocessing/_WoEEncoder.py</code> <pre><code>def transform(self, X):\n    \"\"\"Transform X to weight of evidence encoding.\n\n    Args:\n        X (pd.DataFrame): dataset\n    \"\"\"\n    assert self.handle_unknown in [\"value\", \"error\", \"return_nan\"]\n    check_is_fitted(self)\n    X = ensure_dataframe(X)\n\n    if X.shape[1] != self.n_train_features_:\n        msg = f\"Number of features in X ({X.shape[1]}) is different \"\n        msg += f\"from the number of features in X during fit ({self.n_train_features_})\"\n        raise ValueError(msg)\n\n    for feature in self.variables_:\n        woe_dict = self.woe_mapping_.get(feature)\n        if self.handle_unknown == \"error\":\n            new_cats = [x for x in list(X[feature].unique()) if x not in list(woe_dict.keys())]\n            if len(new_cats) &gt; 0:\n                msg = \"WoEEncoder encountered unknown new categories \"\n                msg += f\"in column {feature} on .transform(): {new_cats}\"\n                raise AssertionError(msg)\n\n        X[feature] = X[feature].map(woe_dict)\n\n    return X\n</code></pre>"},{"location":"api/reporting/iv/","title":"Information Value","text":"<p>Calculate the Information Value (IV) of the features in <code>X</code>.</p> <p><code>X</code> must be the output of fitted bucketers.</p> \\[ IV = \\sum { (\\% goods - \\% bads) } * { WOE } \\] \\[ WOE=\\ln (\\% { goods } /  \\% { bads }) \\] <p>Example:</p> <pre><code>from skorecard import datasets\nfrom sklearn.model_selection import train_test_split\nfrom skorecard.bucketers import DecisionTreeBucketer\nfrom skorecard.reporting import iv\n\nX, y = datasets.load_uci_credit_card(return_X_y=True)\n\ndbt = DecisionTreeBucketer()\nX_bins = dbt.fit_transform(X,y)\n\niv_dict = iv(X_bins, y)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>pd.DataFrame (bucketed) features</p> required <code>y</code> <code>Series</code> <p>pd.Series: target values</p> required <code>epsilon</code> <code>float</code> <p>Amount to be added to relative counts in order to avoid division by zero in the WOE calculation.</p> <code>0.0001</code> <code>digits</code> <code>int</code> <p>number of significant decimal digits in the IV calculation</p> <code>None</code> <p>Returns:</p> Name Type Description <code>IVs</code> <code>dict</code> <p>Keys are feature names, values are the IV values</p> Source code in <code>skorecard/reporting/report.py</code> <pre><code>def iv(X: pd.DataFrame, y: pd.Series, epsilon: float = 0.0001, digits: Optional[int] = None) -&gt; Dict:\n    r\"\"\"\n    Calculate the Information Value (IV) of the features in `X`.\n\n    `X` must be the output of fitted bucketers.\n\n    $$\n    IV = \\sum { (\\% goods - \\% bads) } * { WOE }\n    $$\n\n    $$\n    WOE=\\ln (\\% { goods } /  \\% { bads })\n    $$\n\n    Example:\n\n    ```python\n    from skorecard import datasets\n    from sklearn.model_selection import train_test_split\n    from skorecard.bucketers import DecisionTreeBucketer\n    from skorecard.reporting import iv\n\n    X, y = datasets.load_uci_credit_card(return_X_y=True)\n\n    dbt = DecisionTreeBucketer()\n    X_bins = dbt.fit_transform(X,y)\n\n    iv_dict = iv(X_bins, y)\n    ```\n\n    Args:\n        X: pd.DataFrame (bucketed) features\n        y: pd.Series: target values\n        epsilon (float): Amount to be added to relative counts in order to avoid division by zero in the WOE\n            calculation.\n        digits (int): number of significant decimal digits in the IV calculation\n\n    Returns:\n        IVs (dict): Keys are feature names, values are the IV values\n    \"\"\"  # noqa\n    return {col: _IV_score(y, X[col], epsilon=epsilon, digits=digits) for col in X.columns}\n</code></pre>"},{"location":"api/reporting/psi/","title":"Population Stability Index","text":"<p>Calculate the PSI between the features in two dataframes, <code>X1</code> and <code>X2</code>.</p> <p><code>X1</code> and <code>X2</code> should be bucketed (outputs of fitted bucketers).</p> \\[ PSI = \\sum((\\%{ Good } - \\%{ Bad })         imes \\ln \frac{\\%{ Good }}{\\%{ Bad }}) \\] <p>Parameters:</p> Name Type Description Default <code>X1</code> <code>DataFrame</code> <p>bucketed features, expected</p> required <code>X2</code> <code>DataFrame</code> <p>bucketed features, actual data</p> required <code>epsilon</code> <code>float</code> <p>Amount to be added to relative counts in order to avoid division by zero in the WOE calculation.</p> <code>0.0001</code> <code>digits</code> <p>(int): number of significant decimal digits in the IV calculation</p> <code>None</code> <p>Examples:</p> <pre><code>from skorecard import datasets\nfrom sklearn.model_selection import train_test_split\nfrom skorecard.bucketers import DecisionTreeBucketer\nfrom skorecard.reporting import psi\n\nX, y = datasets.load_uci_credit_card(return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(\n    X,y,\n    test_size=0.25,\n    random_state=42\n)\n\ndbt = DecisionTreeBucketer()\nX_train_bins = dbt.fit_transform(X_train,y_train)\nX_test_bins = dbt.transform(X_test)\n\npsi_dict = psi(X_train_bins, X_test_bins)\n</code></pre> Source code in <code>skorecard/reporting/report.py</code> <pre><code>def psi(X1: pd.DataFrame, X2: pd.DataFrame, epsilon=0.0001, digits=None) -&gt; Dict:\n    \"\"\"\n    Calculate the PSI between the features in two dataframes, `X1` and `X2`.\n\n    `X1` and `X2` should be bucketed (outputs of fitted bucketers).\n\n    $$\n    PSI = \\\\sum((\\\\%{ Good } - \\\\%{ Bad }) \\times \\\\ln \\frac{\\\\%{ Good }}{\\\\%{ Bad }})\n    $$\n\n    Args:\n        X1 (pd.DataFrame): bucketed features, expected\n        X2 (pd.DataFrame): bucketed features, actual data\n        epsilon (float): Amount to be added to relative counts in order to avoid division by zero in the WOE\n            calculation.\n        digits: (int): number of significant decimal digits in the IV calculation\n\n    Returns: dictionary of psi values. keys are feature names, values are the psi values\n\n    Examples:\n\n    ```python\n    from skorecard import datasets\n    from sklearn.model_selection import train_test_split\n    from skorecard.bucketers import DecisionTreeBucketer\n    from skorecard.reporting import psi\n\n    X, y = datasets.load_uci_credit_card(return_X_y=True)\n    X_train, X_test, y_train, y_test = train_test_split(\n        X,y,\n        test_size=0.25,\n        random_state=42\n    )\n\n    dbt = DecisionTreeBucketer()\n    X_train_bins = dbt.fit_transform(X_train,y_train)\n    X_test_bins = dbt.transform(X_test)\n\n    psi_dict = psi(X_train_bins, X_test_bins)\n    ```\n    \"\"\"  # noqa\n    assert (X1.columns == X2.columns).all(), \"X1 and X2 must have same columns\"\n\n    y1 = pd.Series(0, index=X1.index)\n    y2 = pd.Series(1, index=X2.index)\n\n    X = pd.concat([X1, X2], axis=0)\n    y = pd.concat([y1, y2], axis=0).reset_index(drop=True)\n\n    psis = {col: _IV_score(y, X[col], epsilon=epsilon, digits=digits) for col in X1.columns}\n\n    return psis\n</code></pre>"},{"location":"api/rescale/rescale/","title":"ScoreCardPoints","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Transformer to map the the buckets from the skorecard model and maps them to the rescaled points.</p> <p>Examples:</p> <pre><code>from skorecard import Skorecard\nfrom skorecard.rescale import ScoreCardPoints\nfrom skorecard.datasets import load_uci_credit_card\n\nX,y = load_uci_credit_card(return_X_y=True)\nmodel = Skorecard(variables = [\"LIMIT_BAL\", \"BILL_AMT1\",\"EDUCATION\", \"MARRIAGE\"])\nmodel.fit(X, y)\n\nscp = ScoreCardPoints(model)\nscp.transform(X)\n</code></pre> Source code in <code>skorecard/rescale/rescale.py</code> <pre><code>class ScoreCardPoints(BaseEstimator, TransformerMixin):\n    \"\"\"Transformer to map the the buckets from the skorecard model and maps them to the rescaled points.\n\n    Examples:\n\n    ```python\n    from skorecard import Skorecard\n    from skorecard.rescale import ScoreCardPoints\n    from skorecard.datasets import load_uci_credit_card\n\n    X,y = load_uci_credit_card(return_X_y=True)\n    model = Skorecard(variables = [\"LIMIT_BAL\", \"BILL_AMT1\",\"EDUCATION\", \"MARRIAGE\"])\n    model.fit(X, y)\n\n    scp = ScoreCardPoints(model)\n    scp.transform(X)\n    ```\n\n    \"\"\"\n\n    def __init__(self, skorecard_model, *, pdo=20, ref_score=100, ref_odds=1):\n        \"\"\"Initialize the transformer.\n\n        Args:\n            skorecard_model: the fitted Skorecard class\n            pdo: number of points necessary to double the odds\n            ref_score: reference score set for the reference odds\n            ref_odds: odds that correspond to the ref_score\n        \"\"\"\n        assert isinstance(skorecard_model, Skorecard), (\n            f\"The skorecard_model must be an instance of \"\n            f\"skorecard.Skorecard, got {skorecard_model.__class__.__name__} instead.\"\n        )\n        check_is_fitted(skorecard_model)\n        self.skorecard_model = skorecard_model\n        # self.pipeline = skorecard_model.pipeline\n        self.pdo = pdo\n        self.ref_score = ref_score\n        self.ref_odds = ref_odds\n        self._get_pipeline_elements()\n        self._calculate_scorecard_points()\n\n    def _get_pipeline_elements(self):\n        bucketers = self.skorecard_model.pipeline_.named_steps[\"bucketer\"]\n        woe_enc = self.skorecard_model.pipeline_.named_steps[\"encoder\"]\n        self.features = self.skorecard_model.variables\n        self.model = self.skorecard_model.pipeline_.named_steps[\"model\"]\n\n        assert hasattr(self.model, \"predict_proba\"), (\n            f\"Expected a model at the end of the pipeline, \" f\"got {self.model.__class__}\"\n        )\n        if not (isinstance(woe_enc, WoeEncoder) or isinstance(woe_enc, WOEEncoder)):\n            raise ValueError(\"Pipeline must have WoE encoder\")\n\n        fbm = bucketers.features_bucket_mapping_\n\n        if len(self.features) == 0:\n            # there is no feature selector\n            self.features = fbm.columns\n        woe_dict = woe_enc.mapping\n\n        self.buckets = {k: fbm.get(k) for k in fbm.columns if k in self.features}\n        self.woes = {k: woe_dict[k] for k in woe_dict.keys() if k in self.features}\n\n    def _calculate_scorecard_points(self):\n        # Put together the features in a list of tables, containing all the buckets.\n        list_dfs = list()\n        for ix, col in enumerate(self.features):\n            df_ = (\n                pd.concat([pd.Series(self.buckets[col].labels), pd.Series(self.woes[col])], axis=1)\n                .reset_index()\n                .rename(columns={\"index\": \"bin_index\", 0: \"map\", 1: \"woe\"})\n            )\n            df_.loc[:, \"feature\"] = col\n\n            df_.loc[:, \"coef\"] = self.model.coef_[0][ix]\n            #\n            list_dfs.append(df_)\n\n        # Reduce the list of tables, to build the final scorecard feature points\n        scorecard = reduce(lambda x, y: pd.concat([x, y]), list_dfs)\n        scorecard = pd.concat(\n            [\n                scorecard,\n                pd.DataFrame.from_records(\n                    [{\"feature\": \"Intercept\", \"coef\": self.model.intercept_[0], \"bin_index\": 0, \"map\": 0, \"woe\": 0}]\n                ),\n            ],\n            ignore_index=True,\n        )\n\n        #     return buckets, woes\n        scorecard[\"contribution\"] = scorecard[\"woe\"] * scorecard[\"coef\"]\n\n        self.scorecard = _scale_scorecard(\n            scorecard, pdo=self.pdo, ref_score=self.ref_score, ref_odds=self.ref_odds, features=self.features\n        )\n\n        self.points_mapper = dict()\n        for feat in self.scorecard[\"feature\"].unique():\n            one_feat_df = self.scorecard.loc[self.scorecard[\"feature\"] == feat, [\"bin_index\", \"Points\"]]\n            self.points_mapper[feat] = {\n                k: v for k, v in zip(one_feat_df[\"bin_index\"].values, one_feat_df[\"Points\"].values)\n            }\n\n    def get_scorecard_points(self):\n        \"\"\"Get the scorecard points.\"\"\"\n        return self.scorecard\n\n    def transform(self, X):\n        \"\"\"Transform the features to the points.\"\"\"\n        X_buckets = self.skorecard_model.pipeline_.named_steps[\"bucketer\"].transform(X)\n\n        bin_points = pd.concat(\n            [\n                X_buckets[feat].map(self.points_mapper[feat])\n                for feat in self.points_mapper.keys()\n                if feat != \"Intercept\"\n            ],\n            axis=1,\n        )\n        bin_points.index = X.index\n\n        return bin_points\n</code></pre>"},{"location":"api/rescale/rescale/#skorecard.rescale.ScoreCardPoints.__init__","title":"<code>__init__(skorecard_model, *, pdo=20, ref_score=100, ref_odds=1)</code>","text":"<p>Initialize the transformer.</p> <p>Parameters:</p> Name Type Description Default <code>skorecard_model</code> <p>the fitted Skorecard class</p> required <code>pdo</code> <p>number of points necessary to double the odds</p> <code>20</code> <code>ref_score</code> <p>reference score set for the reference odds</p> <code>100</code> <code>ref_odds</code> <p>odds that correspond to the ref_score</p> <code>1</code> Source code in <code>skorecard/rescale/rescale.py</code> <pre><code>def __init__(self, skorecard_model, *, pdo=20, ref_score=100, ref_odds=1):\n    \"\"\"Initialize the transformer.\n\n    Args:\n        skorecard_model: the fitted Skorecard class\n        pdo: number of points necessary to double the odds\n        ref_score: reference score set for the reference odds\n        ref_odds: odds that correspond to the ref_score\n    \"\"\"\n    assert isinstance(skorecard_model, Skorecard), (\n        f\"The skorecard_model must be an instance of \"\n        f\"skorecard.Skorecard, got {skorecard_model.__class__.__name__} instead.\"\n    )\n    check_is_fitted(skorecard_model)\n    self.skorecard_model = skorecard_model\n    # self.pipeline = skorecard_model.pipeline\n    self.pdo = pdo\n    self.ref_score = ref_score\n    self.ref_odds = ref_odds\n    self._get_pipeline_elements()\n    self._calculate_scorecard_points()\n</code></pre>"},{"location":"api/rescale/rescale/#skorecard.rescale.ScoreCardPoints.get_scorecard_points","title":"<code>get_scorecard_points()</code>","text":"<p>Get the scorecard points.</p> Source code in <code>skorecard/rescale/rescale.py</code> <pre><code>def get_scorecard_points(self):\n    \"\"\"Get the scorecard points.\"\"\"\n    return self.scorecard\n</code></pre>"},{"location":"api/rescale/rescale/#skorecard.rescale.ScoreCardPoints.transform","title":"<code>transform(X)</code>","text":"<p>Transform the features to the points.</p> Source code in <code>skorecard/rescale/rescale.py</code> <pre><code>def transform(self, X):\n    \"\"\"Transform the features to the points.\"\"\"\n    X_buckets = self.skorecard_model.pipeline_.named_steps[\"bucketer\"].transform(X)\n\n    bin_points = pd.concat(\n        [\n            X_buckets[feat].map(self.points_mapper[feat])\n            for feat in self.points_mapper.keys()\n            if feat != \"Intercept\"\n        ],\n        axis=1,\n    )\n    bin_points.index = X.index\n\n    return bin_points\n</code></pre>"},{"location":"discussion/benchmark_stats_feature/","title":"Benchmarks vs alternatives","text":"<pre><code>import pandas as pd\nfrom skorecard.datasets import load_credit_card\nfrom sklearn.model_selection import train_test_split\n\nfrom skorecard import Skorecard\nfrom skorecard.pipeline.bucketing_process import BucketingProcess\nfrom sklearn.pipeline import make_pipeline\nfrom skorecard.bucketers.bucketers import DecisionTreeBucketer, OptimalBucketer\n\nfrom time import time\n\n\ndata = load_credit_card(as_frame=True)\n# data = pd.read_csv('UCI_Credit_Card.csv')\n# cols = [\"EDUCATION\", \"MARRIAGE\", \"LIMIT_BAL\", \"BILL_AMT1\", \"default\"]\n# data = data[cols]\n# data.rename(columns={'default': 'y'}, inplace=True)\n\n\nprint(f\"data shape: {data.shape}\")\n\nX_train, X_test, y_train, y_test = train_test_split(\n    data.drop([\"y\"], axis=1), data[[\"y\"]], test_size=0.25, random_state=42\n)\n\ndata_train_opt, data_test_opt = train_test_split(data, test_size=0.25, random_state=42)\ndata_train_opt.head()\n\n\ny_train = y_train.to_numpy().flatten()\ny_test = y_test.to_numpy().flatten()\n</code></pre> <pre>\n<code>data shape: (30000, 24)\n</code>\n</pre> <pre><code>from sklearn.metrics import roc_auc_score\n\n\ndef report_auc(clf, X_train, y_train, X_test, y_test):\n    proba_train = clf.predict_proba(X_train)[:, 1]\n    proba_test = clf.predict_proba(X_test)[:, 1]\n\n    auc_train = round(roc_auc_score(y_train, proba_train), 4)\n    auc_test = round(roc_auc_score(y_test, proba_test), 4)\n\n    return auc_train, auc_test\n</code></pre> <pre><code>from memo import memlist, time_taken\n\ndata = []\n\n\n@memlist(data=data)\n@time_taken()\ndef fit_eval_record(clf, name, opt=False):\n    if opt:\n        clf.fit(data_train_opt, data_train_opt[\"y\"])\n        proba_train = clf.predict_proba(data_train_opt)[:, 1]\n        proba_test = clf.predict_proba(data_test_opt)[:, 1]\n\n        auc_train = round(roc_auc_score(y_train, proba_train), 4)\n        auc_test = round(roc_auc_score(y_test, proba_test), 4)\n\n    else:\n        clf.fit(X_train, y_train)\n        auc_train, auc_test = report_auc(clf, X_train, y_train, X_test, y_test)\n\n    return {\"auc_train\": auc_train, \"auc_test\": auc_test}\n</code></pre> <pre><code>start_slow = time()\nfor i in range(10):\n    bucketer_slow = DecisionTreeBucketer(max_n_bins=100, min_bin_size=0.05, get_statistics=True)\n    X_train_b1 = bucketer_slow.fit_transform(X_train, y_train)\nend_slow = time()\n\nprint(\"Time for a single bucket when summary is computed:\", end_slow - start_slow)\n\nstart = time()\nfor i in range(10):\n    bucketer = DecisionTreeBucketer(max_n_bins=100, min_bin_size=0.05, get_statistics=False)\n    X_train_b2 = bucketer.fit_transform(X_train, y_train)\nend = time()\n\nprint(\"Time for a single bucket when summary is not computed:\", end - start)\n</code></pre> <pre>\n<code>Time for a single bucket when summary is computed: 39.013752937316895\nTime for a single bucket when summary is not computed: 12.374780178070068\n</code>\n</pre> <pre><code>start_slow = time()\nfor i in range(5):\n    clf_slow = BucketingProcess(\n        prebucketing_pipeline=make_pipeline(\n            DecisionTreeBucketer(max_n_bins=100, min_bin_size=0.05, get_statistics=True)\n        ),\n        bucketing_pipeline=make_pipeline(OptimalBucketer(max_n_bins=10, min_bin_size=0.05, get_statistics=True)),\n    )\n    clf_slow.fit(X_train, y_train)\nend_slow = time()\nprint(\"Time for a bucketing process when redundant summary is computed:\", end_slow - start_slow)\n\nstart = time()\nfor i in range(5):\n    clf = BucketingProcess(\n        prebucketing_pipeline=make_pipeline(\n            DecisionTreeBucketer(max_n_bins=100, min_bin_size=0.05, get_statistics=False)\n        ),\n        bucketing_pipeline=make_pipeline(OptimalBucketer(max_n_bins=10, min_bin_size=0.05, get_statistics=False)),\n    )\n    clf.fit(X_train, y_train)\nend = time()\nprint(\"Time for a bucketing process when redundant summary is not computed:\", end - start)\n</code></pre> <pre>\n<code>Time for a bucketing process when redundant summary is computed: 66.75277090072632\nTime for a bucketing process when redundant summary is not computed: 39.94823408126831\n</code>\n</pre> <pre><code>bucketing_process_slow = BucketingProcess(\n    prebucketing_pipeline=make_pipeline(DecisionTreeBucketer(max_n_bins=100, min_bin_size=0.05, get_statistics=True)),\n    bucketing_pipeline=make_pipeline(OptimalBucketer(max_n_bins=10, min_bin_size=0.05, get_statistics=True)),\n)\nscorecard_slow = Skorecard(bucketing=bucketing_process_slow)\n\nd_slow = fit_eval_record(scorecard_slow, name=\"skorecard.Scorecard\")\nprint(\"Time for a scorecard model when redundant summary is computed:\", d_slow[\"time_taken\"])\n\nbucketing_process = BucketingProcess(\n    prebucketing_pipeline=make_pipeline(DecisionTreeBucketer(max_n_bins=100, min_bin_size=0.05, get_statistics=False)),\n    bucketing_pipeline=make_pipeline(OptimalBucketer(max_n_bins=10, min_bin_size=0.05, get_statistics=False)),\n)\n\nscorecard = Skorecard(bucketing=bucketing_process)\n\nd = fit_eval_record(scorecard, name=\"skorecard.Scorecard\")\nprint(\"Time for a scorecard model when redundant summary is not computed:\", d[\"time_taken\"])\n</code></pre> <pre>\n<code>/Users/CK58LU/opt/anaconda3/envs/skorecard_env/lib/python3.9/site-packages/category_encoders/utils.py:21: FutureWarning: is_categorical is deprecated and will be removed in a future version. Use is_categorical_dtype instead.\n  elif pd.api.types.is_categorical(cols):\n</code>\n</pre> <pre>\n<code>Time for a scorecard model when redundant summary is computed: 19.25\n</code>\n</pre> <pre>\n<code>/Users/CK58LU/opt/anaconda3/envs/skorecard_env/lib/python3.9/site-packages/category_encoders/utils.py:21: FutureWarning: is_categorical is deprecated and will be removed in a future version. Use is_categorical_dtype instead.\n  elif pd.api.types.is_categorical(cols):\n</code>\n</pre> <pre>\n<code>Time for a scorecard model when redundant summary is not computed: 13.61\n</code>\n</pre>"},{"location":"discussion/benchmark_stats_feature/#benchmarks","title":"Benchmarks","text":"<p>Here we will demonstrate some benchmarks against some alternatives.</p>"},{"location":"discussion/benchmark_stats_feature/#data","title":"Data","text":"<p>UCI Credit card dataset with 30k rows and 23 features.</p>"},{"location":"discussion/benchmark_stats_feature/#experiment-setup","title":"Experiment setup","text":""},{"location":"discussion/benchmark_stats_feature/#skorecard-is-currently-rather-slow-a-minor-speed-up-can-be-obtained-by-noting-that-both-bucketingprocess-and-its-pre-bucketers-and-bucketers-compute-identical-bucket_tables-and-summaries-this-is-redundant-when-using-a-bucketingprocess-a-boolean-variable-get_statistics-has-been-added-to-the-bucketers-to-remove-the-calculation-of-these-statistics-below-a-comparison-is-made-to-show-the-difference-in-speed-this-makes-at-the-level-of","title":"Skorecard is currently rather slow. A minor speed-up can be obtained by noting that both BucketingProcess and its pre-bucketers and bucketers compute identical bucket_tables and summaries: this is redundant when using a BucketingProcess. A boolean variable 'get_statistics' has been added to the bucketers to remove the calculation of these statistics. Below, a comparison is made to show the difference in speed this makes at the level of:","text":""},{"location":"discussion/benchmark_stats_feature/#1-a-single-bucketer","title":"1) A single bucketer","text":""},{"location":"discussion/benchmark_stats_feature/#2-a-bucketingprocess","title":"2) A BucketingProcess","text":""},{"location":"discussion/benchmark_stats_feature/#3-a-full-scorecard-pipeline","title":"3) A full Scorecard pipeline","text":""},{"location":"discussion/benchmark_with_EBM/","title":"Benchmarks with Explainable Boosting Classifier","text":"<pre><code># To run benchmark script, you will need to install XGBoost\n# (pip install XGBoost)\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.datasets import load_breast_cancer\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\n\ndef load_breast_data():\n    breast = load_breast_cancer()\n    feature_names = list(breast.feature_names)\n    X, y = pd.DataFrame(breast.data, columns=feature_names), breast.target\n    dataset = {\n        \"problem\": \"classification\",\n        \"full\": {\n            \"X\": X,\n            \"y\": y,\n        },\n    }\n    return dataset\n\n\ndef load_adult_data():\n    df = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\", header=None)\n    df.columns = [\n        \"Age\",\n        \"WorkClass\",\n        \"fnlwgt\",\n        \"Education\",\n        \"EducationNum\",\n        \"MaritalStatus\",\n        \"Occupation\",\n        \"Relationship\",\n        \"Race\",\n        \"Gender\",\n        \"CapitalGain\",\n        \"CapitalLoss\",\n        \"HoursPerWeek\",\n        \"NativeCountry\",\n        \"Income\",\n    ]\n    train_cols = df.columns[0:-1]\n    label = df.columns[-1]\n    X_df = df[train_cols]\n    y_df = df[label]\n\n    dataset = {\n        \"problem\": \"classification\",\n        \"full\": {\n            \"X\": X_df,\n            \"y\": y_df,\n        },\n    }\n\n    return dataset\n\n\ndef load_heart_data():\n    # https://www.kaggle.com/ronitf/heart-disease-uci\n    df = pd.read_csv(r\"heart.csv\")\n    train_cols = df.columns[0:-1]\n    label = df.columns[-1]\n    X_df = df[train_cols]\n    y_df = df[label]\n    dataset = {\n        \"problem\": \"classification\",\n        \"full\": {\n            \"X\": X_df,\n            \"y\": y_df,\n        },\n    }\n\n    return dataset\n\n\ndef load_credit_data():\n    # https://www.kaggle.com/mlg-ulb/creditcardfraud\n    df = pd.read_csv(r\"creditcard.csv\")\n    train_cols = df.columns[0:-1]\n    label = df.columns[-1]\n    X_df = df[train_cols]\n    y_df = df[label]\n    dataset = {\n        \"problem\": \"classification\",\n        \"full\": {\n            \"X\": X_df,\n            \"y\": y_df,\n        },\n    }\n\n    return dataset\n\n\ndef load_telco_churn_data():\n    # https://www.kaggle.com/blastchar/telco-customer-churn\n    df = pd.read_csv(r\"WA_Fn-UseC_-Telco-Customer-Churn.csv\")\n    train_cols = df.columns[1:-1]  # First column is an ID\n    label = df.columns[-1]\n    X_df = df[train_cols]\n    y_df = df[label]  # 'Yes, No'\n    dataset = {\n        \"problem\": \"classification\",\n        \"full\": {\n            \"X\": X_df,\n            \"y\": y_df,\n        },\n    }\n\n    return dataset\n</code></pre> <pre><code>from sklearn.preprocessing import OneHotEncoder, FunctionTransformer, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import StratifiedShuffleSplit, cross_validate\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import OrdinalEncoder\n\nfrom xgboost import XGBClassifier\n\nfrom sklearn.linear_model import LogisticRegression\n\nfrom interpret.glassbox import ExplainableBoostingClassifier\n\nfrom skorecard import Skorecard\n\nfrom optbinning import BinningProcess\nfrom optbinning import Scorecard\n\n\ndef format_n(x):\n    return f\"{x:.3f}\"\n\n\ndef process_model(clf, name, X, y, n_splits=3):\n    # Evaluate model\n    ss = StratifiedShuffleSplit(n_splits=n_splits, test_size=0.25, random_state=1337)\n    scores = cross_validate(clf, X, y, scoring=\"roc_auc\", cv=ss, n_jobs=-1, return_estimator=True)\n\n    record = dict()\n    record[\"model_name\"] = name\n    record[\"fit_time_mean\"] = format_n(np.mean(scores[\"fit_time\"]))\n    record[\"fit_time_std\"] = format_n(np.std(scores[\"fit_time\"]))\n    record[\"test_score_mean\"] = format_n(np.mean(scores[\"test_score\"]))\n    record[\"test_score_std\"] = format_n(np.std(scores[\"test_score\"]))\n\n    return record\n\n\ndef benchmark_models(dataset_name, X, y, ct=None, n_splits=3, random_state=1337):\n    if ct is None:\n        is_cat = np.array([dt.kind == \"O\" for dt in X.dtypes])\n        cat_cols = X.columns.values[is_cat]\n        num_cols = X.columns.values[~is_cat]\n\n        cat_ohe_step = (\"ohe\", OneHotEncoder(sparse=False, handle_unknown=\"ignore\"))\n\n        cat_pipe = Pipeline([cat_ohe_step])\n        num_pipe = Pipeline([(\"identity\", FunctionTransformer())])\n        transformers = [(\"cat\", cat_pipe, cat_cols), (\"num\", num_pipe, num_cols)]\n        ct = ColumnTransformer(transformers=transformers)\n\n        cat_ord_step = (\"ord_enc\", OrdinalEncoder())\n        cat_pipe = Pipeline([cat_ord_step])\n        transformers = [(\"cat\", cat_pipe, cat_cols), (\"num\", num_pipe, num_cols)]\n        ot = ColumnTransformer(transformers=transformers)\n\n    records = []\n\n    summary_record = {}\n    summary_record[\"dataset_name\"] = dataset_name\n    print()\n    print(\"-\" * 78)\n    print(dataset_name)\n    print(\"-\" * 78)\n    print(summary_record)\n    print()\n\n    pipe = Pipeline(\n        [\n            (\"ct\", ct),\n            (\"std\", StandardScaler()),\n            (\"lr\", LogisticRegression(random_state=random_state)),\n        ]\n    )\n    record = process_model(pipe, \"lr_ohe\", X, y, n_splits=n_splits)\n    print(record)\n    record.update(summary_record)\n    records.append(record)\n\n    pipe = Pipeline(\n        [\n            (\"ot\", ot),\n            (\"std\", StandardScaler()),\n            (\"lr\", LogisticRegression(max_iter=7000, random_state=random_state)),\n        ]\n    )\n    record = process_model(pipe, \"lr_ordinal\", X, y, n_splits=n_splits)\n    print(record)\n    record.update(summary_record)\n    records.append(record)\n\n    # Skorecard\n    skorecard = Skorecard()\n    record = process_model(skorecard, \"skorecard\", X, y, n_splits=n_splits)\n    print(record)\n    record.update(summary_record)\n    records.append(record)\n\n    pipe = Pipeline(\n        [\n            (\"ct\", ct),\n            # n_estimators updated from 10 to 100 due to sci-kit defaults changing in future versions\n            (\"rf-100\", RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=random_state)),\n        ]\n    )\n    record = process_model(pipe, \"rf-100\", X, y, n_splits=n_splits)\n    print(record)\n    record.update(summary_record)\n    records.append(record)\n\n    pipe = Pipeline(\n        [\n            (\"ct\", ct),\n            (\"xgb\", XGBClassifier(random_state=random_state, eval_metric=\"logloss\")),\n        ]\n    )\n    record = process_model(pipe, \"xgb\", X, y, n_splits=n_splits)\n    print(record)\n    record.update(summary_record)\n    records.append(record)\n\n    # No pipeline needed due to EBM handling string datatypes\n    ebm_inter = ExplainableBoostingClassifier(n_jobs=-1, random_state=random_state)\n    record = process_model(ebm_inter, \"ebm\", X, y, n_splits=n_splits)\n    print(record)\n    record.update(summary_record)\n    records.append(record)\n\n    return records\n</code></pre> <pre><code>results = []\nn_splits = 3\n</code></pre> <pre><code>from skorecard.datasets import load_uci_credit_card\n\nX, y = load_uci_credit_card(return_X_y=True)\nresult = benchmark_models(\"UCI-creditcard\", X, y, n_splits=n_splits)\nresults.append(result)\n</code></pre> <pre>\n<code>\n------------------------------------------------------------------------------\nUCI-creditcard\n------------------------------------------------------------------------------\n{'dataset_name': 'UCI-creditcard'}\n\n{'model_name': 'lr_ohe', 'fit_time_mean': '0.009', 'fit_time_std': '0.000', 'test_score_mean': '0.621', 'test_score_std': '0.023'}\n{'model_name': 'lr_ordinal', 'fit_time_mean': '0.008', 'fit_time_std': '0.001', 'test_score_mean': '0.621', 'test_score_std': '0.023'}\n{'model_name': 'skorecard', 'fit_time_mean': '1.489', 'fit_time_std': '0.044', 'test_score_mean': '0.627', 'test_score_std': '0.018'}\n{'model_name': 'rf-100', 'fit_time_mean': '0.326', 'fit_time_std': '0.051', 'test_score_mean': '0.588', 'test_score_std': '0.013'}\n{'model_name': 'xgb', 'fit_time_mean': '0.957', 'fit_time_std': '0.114', 'test_score_mean': '0.596', 'test_score_std': '0.005'}\n{'model_name': 'ebm', 'fit_time_mean': '1.219', 'fit_time_std': '0.151', 'test_score_mean': '0.644', 'test_score_std': '0.012'}\n</code>\n</pre> <pre><code>dataset = load_breast_data()\nresult = benchmark_models(\"breast-cancer\", dataset[\"full\"][\"X\"], dataset[\"full\"][\"y\"], n_splits=n_splits)\nresults.append(result)\n</code></pre> <pre>\n<code>\n------------------------------------------------------------------------------\nbreast-cancer\n------------------------------------------------------------------------------\n{'dataset_name': 'breast-cancer'}\n\n{'model_name': 'lr_ohe', 'fit_time_mean': '0.014', 'fit_time_std': '0.001', 'test_score_mean': '0.994', 'test_score_std': '0.006'}\n{'model_name': 'lr_ordinal', 'fit_time_mean': '0.015', 'fit_time_std': '0.001', 'test_score_mean': '0.994', 'test_score_std': '0.006'}\n{'model_name': 'skorecard', 'fit_time_mean': '13.034', 'fit_time_std': '0.868', 'test_score_mean': '0.996', 'test_score_std': '0.004'}\n{'model_name': 'rf-100', 'fit_time_mean': '0.253', 'fit_time_std': '0.002', 'test_score_mean': '0.992', 'test_score_std': '0.009'}\n{'model_name': 'xgb', 'fit_time_mean': '0.196', 'fit_time_std': '0.014', 'test_score_mean': '0.992', 'test_score_std': '0.010'}\n{'model_name': 'ebm', 'fit_time_mean': '5.866', 'fit_time_std': '1.070', 'test_score_mean': '0.995', 'test_score_std': '0.006'}\n</code>\n</pre> <pre><code>dataset = load_adult_data()\nresult = benchmark_models(\"adult\", dataset[\"full\"][\"X\"], dataset[\"full\"][\"y\"], n_splits=n_splits)\nresults.append(result)\n# 0.888\n</code></pre> <pre>\n<code>\n------------------------------------------------------------------------------\nadult\n------------------------------------------------------------------------------\n{'dataset_name': 'adult'}\n\n{'model_name': 'lr_ohe', 'fit_time_mean': '0.527', 'fit_time_std': '0.032', 'test_score_mean': '0.906', 'test_score_std': '0.003'}\n{'model_name': 'lr_ordinal', 'fit_time_mean': '0.131', 'fit_time_std': '0.003', 'test_score_mean': '0.855', 'test_score_std': '0.002'}\n{'model_name': 'skorecard', 'fit_time_mean': '8.047', 'fit_time_std': '0.025', 'test_score_mean': '0.888', 'test_score_std': '0.004'}\n{'model_name': 'rf-100', 'fit_time_mean': '2.445', 'fit_time_std': '0.008', 'test_score_mean': '0.903', 'test_score_std': '0.002'}\n{'model_name': 'xgb', 'fit_time_mean': '14.403', 'fit_time_std': '0.403', 'test_score_mean': '0.927', 'test_score_std': '0.001'}\n{'model_name': 'ebm', 'fit_time_mean': '51.093', 'fit_time_std': '1.137', 'test_score_mean': '0.928', 'test_score_std': '0.002'}\n</code>\n</pre> <pre><code>dataset = load_telco_churn_data()\nresult = benchmark_models(\"telco_churn\", dataset[\"full\"][\"X\"], dataset[\"full\"][\"y\"], n_splits=n_splits)\nresults.append(result)\n</code></pre> <pre>\n<code>\n------------------------------------------------------------------------------\ntelco_churn\n------------------------------------------------------------------------------\n{'dataset_name': 'telco_churn'}\n\n{'model_name': 'lr_ohe', 'fit_time_mean': '9.091', 'fit_time_std': '0.030', 'test_score_mean': '0.809', 'test_score_std': '0.014'}\n{'model_name': 'lr_ordinal', 'fit_time_mean': '0.064', 'fit_time_std': '0.002', 'test_score_mean': 'nan', 'test_score_std': 'nan'}\n{'model_name': 'skorecard', 'fit_time_mean': '4.672', 'fit_time_std': '0.069', 'test_score_mean': '0.764', 'test_score_std': '0.014'}\n{'model_name': 'rf-100', 'fit_time_mean': '7.118', 'fit_time_std': '0.016', 'test_score_mean': '0.824', 'test_score_std': '0.002'}\n{'model_name': 'xgb', 'fit_time_mean': '104.738', 'fit_time_std': '0.969', 'test_score_mean': '0.825', 'test_score_std': '0.003'}\n{'model_name': 'ebm', 'fit_time_mean': '36.052', 'fit_time_std': '3.779', 'test_score_mean': '0.852', 'test_score_std': '0.004'}\n</code>\n</pre> <pre><code>dataset = load_heart_data()\nresult = benchmark_models(\"heart\", dataset[\"full\"][\"X\"], dataset[\"full\"][\"y\"], n_splits=n_splits)\nresults.append(result)\n</code></pre> <pre>\n<code>\n------------------------------------------------------------------------------\nheart\n------------------------------------------------------------------------------\n{'dataset_name': 'heart'}\n\n{'model_name': 'lr_ohe', 'fit_time_mean': '0.007', 'fit_time_std': '0.001', 'test_score_mean': '0.895', 'test_score_std': '0.030'}\n{'model_name': 'lr_ordinal', 'fit_time_mean': '0.007', 'fit_time_std': '0.000', 'test_score_mean': '0.895', 'test_score_std': '0.030'}\n{'model_name': 'skorecard', 'fit_time_mean': '2.148', 'fit_time_std': '0.013', 'test_score_mean': '0.911', 'test_score_std': '0.015'}\n{'model_name': 'rf-100', 'fit_time_mean': '0.241', 'fit_time_std': '0.003', 'test_score_mean': '0.890', 'test_score_std': '0.008'}\n{'model_name': 'xgb', 'fit_time_mean': '0.318', 'fit_time_std': '0.035', 'test_score_mean': '0.851', 'test_score_std': '0.018'}\n{'model_name': 'ebm', 'fit_time_mean': '1.925', 'fit_time_std': '0.392', 'test_score_mean': '0.906', 'test_score_std': '0.011'}\n</code>\n</pre> <pre><code>records = [item for result in results for item in result]\nrecord_df = pd.DataFrame.from_records(records)[[\"dataset_name\", \"model_name\", \"test_score_mean\", \"test_score_std\"]]\n</code></pre> <pre><code>record_df = record_df.sort_values([\"dataset_name\", \"test_score_mean\"], ascending=False)\n</code></pre> <pre><code>print(\n    record_df[record_df[\"model_name\"].isin([\"lr_ohe\", \"lr_ordinal\", \"rf-100\", \"skorecard\", \"xgb\"])]\n    .drop([\"test_score_std\"], axis=1)\n    .to_markdown(tablefmt=\"github\", showindex=False)\n)\n</code></pre> <pre>\n<code>| dataset_name   | model_name   |   test_score_mean |\n|----------------|--------------|-------------------|\n| telco_churn    | lr_ordinal   |           nan     |\n| telco_churn    | xgb          |             0.825 |\n| telco_churn    | rf-100       |             0.824 |\n| telco_churn    | lr_ohe       |             0.809 |\n| telco_churn    | skorecard    |             0.764 |\n| heart          | skorecard    |             0.911 |\n| heart          | lr_ohe       |             0.895 |\n| heart          | lr_ordinal   |             0.895 |\n| heart          | rf-100       |             0.89  |\n| heart          | xgb          |             0.851 |\n| breast-cancer  | skorecard    |             0.996 |\n| breast-cancer  | lr_ohe       |             0.994 |\n| breast-cancer  | lr_ordinal   |             0.994 |\n| breast-cancer  | rf-100       |             0.992 |\n| breast-cancer  | xgb          |             0.992 |\n| adult          | xgb          |             0.927 |\n| adult          | lr_ohe       |             0.906 |\n| adult          | rf-100       |             0.903 |\n| adult          | skorecard    |             0.888 |\n| adult          | lr_ordinal   |             0.855 |\n| UCI-creditcard | skorecard    |             0.627 |\n| UCI-creditcard | lr_ohe       |             0.621 |\n| UCI-creditcard | lr_ordinal   |             0.621 |\n| UCI-creditcard | xgb          |             0.596 |\n| UCI-creditcard | rf-100       |             0.588 |\n</code>\n</pre>"},{"location":"discussion/benchmark_with_EBM/#ebm-benchmark-with-skorecard","title":"EBM benchmark with skorecard","text":"<p>This benchmark was adjusted from this notebook</p>"},{"location":"discussion/benchmarks/","title":"Benchmarks","text":"<pre><code>import pandas as pd\nfrom skorecard.datasets import load_credit_card\nfrom sklearn.model_selection import train_test_split\n\ndata = load_credit_card(as_frame=True)\nprint(f\"data shape: {data.shape}\")\n\nX_train, X_test, y_train, y_test = train_test_split(\n    data.drop([\"y\"], axis=1), data[[\"y\"]], test_size=0.25, random_state=42\n)\n\ndata_train_opt, data_test_opt = train_test_split(data, test_size=0.25, random_state=42)\n</code></pre> <pre>\n<code>data shape: (30000, 24)\n</code>\n</pre> <pre><code>from sklearn.metrics import roc_auc_score\n\n\ndef report_auc(clf, X_train, y_train, X_test, y_test):\n    proba_train = clf.predict_proba(X_train)[:, 1]\n    proba_test = clf.predict_proba(X_test)[:, 1]\n\n    auc_train = round(roc_auc_score(y_train, proba_train), 4)\n    auc_test = round(roc_auc_score(y_test, proba_test), 4)\n\n    return auc_train, auc_test\n</code></pre> <pre><code>from memo import memlist, time_taken\n\ndata = []\n\n\n@memlist(data=data)\n@time_taken()\ndef fit_eval_record(clf, name, opt=False):\n    if opt:\n        clf.fit(data_train_opt)\n        proba_train = clf.predict_proba(data_train_opt)[:, 1]\n        proba_test = clf.predict_proba(data_test_opt)[:, 1]\n\n        auc_train = round(roc_auc_score(y_train, proba_train), 4)\n        auc_test = round(roc_auc_score(y_test, proba_test), 4)\n\n    else:\n        clf.fit(X_train, y_train)\n        auc_train, auc_test = report_auc(clf, X_train, y_train, X_test, y_test)\n\n    return {\"auc_train\": auc_train, \"auc_test\": auc_test}\n</code></pre> <pre><code>from skorecard import Skorecard\n\nscorecard = Skorecard()\nfit_eval_record(scorecard, name=\"skorecard.Scorecard\")\n</code></pre> <pre>\n<code>{'auc_train': 0.7727, 'auc_test': 0.766, 'time_taken': 16.73}</code>\n</pre> <pre><code># from sklearn.pipeline import make_pipeline\n# from sklearn.linear_model import LogisticRegression\n# from skorecard.preprocessing import WoeEncoder\n# from skorecard.bucketers import DecisionTreeBucketer, OptimalBucketer\n# from category_encoders.woe import WOEEncoder\n\n# pipe = make_pipeline(\n#     DecisionTreeBucketer(),\n#     OptimalBucketer(),\n#     #WoeEncoder(),\n#     WOEEncoder(cols=X_train.columns),\n#     LogisticRegression(solver=\"lbfgs\", max_iter=400)\n# )\n\n# fit_eval_record(pipe, name=\"pipeline\")\n\n# # .7166 with skorecard woe in 3.7s\n# # 0.758 with no WOE in 3.9s\n# # 0.7661 with WOE on all cols.\n</code></pre> <pre><code>from optbinning import BinningProcess\nfrom optbinning import Scorecard\nfrom sklearn.linear_model import LogisticRegression\nimport pandas as pd\n\nselection_criteria = {\"iv\": {\"min\": 0.02, \"max\": 1}, \"quality_score\": {\"min\": 0.01}}\nbinning_process = BinningProcess(variable_names=list(X_train.columns), selection_criteria=selection_criteria)\n\nestimator = LogisticRegression(solver=\"lbfgs\")\n\nopt_scorecard = Scorecard(\n    target=\"y\",\n    binning_process=binning_process,\n    estimator=estimator,\n    scaling_method=\"min_max\",\n    scaling_method_params={\"min\": 300, \"max\": 850},\n)\n\nopt_scorecard.fit(data_train_opt)\nfit_eval_record(opt_scorecard, name=\"optbinning.Scorecard\", opt=True)\n</code></pre> <pre>\n<code>{'auc_train': 0.7719, 'auc_test': 0.7628, 'time_taken': 1.88}</code>\n</pre> <pre><code>from sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\n\npipe = make_pipeline(StandardScaler(), LogisticRegression(random_state=42, solver=\"lbfgs\"))\n\nfit_eval_record(pipe, name=\"sklearn.LogisticRegression\")\n</code></pre> <pre>\n<code>/Users/iv58uq/miniconda3/envs/dancard_py37/lib/python3.7/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  return f(*args, **kwargs)\n</code>\n</pre> <pre>\n<code>{'auc_train': 0.724, 'auc_test': 0.7232, 'time_taken': 0.11}</code>\n</pre> <pre><code>from lightgbm import LGBMClassifier\n\nclf = LGBMClassifier(random_state=42, max_depth=10, learning_rate=0.01)\n\nfit_eval_record(clf, name=\"LightGBM\")\n</code></pre> <pre>\n<code>/Users/iv58uq/miniconda3/envs/dancard_py37/lib/python3.7/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  return f(*args, **kwargs)\n</code>\n</pre> <pre>\n<code>{'auc_train': 0.8038, 'auc_test': 0.7778, 'time_taken': 0.33}</code>\n</pre> <pre><code>pd.DataFrame(data).sort_values(\"auc_test\", ascending=False).drop(\"opt\", axis=1)\n</code></pre> name auc_train auc_test time_taken 3 LightGBM 0.8038 0.7778 0.33 0 skorecard.Scorecard 0.7727 0.7660 16.73 1 optbinning.Scorecard 0.7719 0.7628 1.88 2 sklearn.LogisticRegression 0.7240 0.7232 0.11"},{"location":"discussion/benchmarks/#benchmarks","title":"Benchmarks","text":"<p>Here we will demonstrate some benchmarks against some alternatives.</p>"},{"location":"discussion/benchmarks/#data","title":"Data","text":"<p>UCI Credit card dataset with 30k rows and 23 features.</p>"},{"location":"discussion/benchmarks/#experiment-setup","title":"Experiment setup","text":""},{"location":"discussion/benchmarks/#baseline","title":"Baseline","text":""},{"location":"discussion/benchmarks/#optbinning","title":"Optbinning","text":"<p>See the excellent package Optbinning.</p>"},{"location":"discussion/benchmarks/#basic-lr","title":"Basic LR","text":""},{"location":"discussion/benchmarks/#lightgbm-model","title":"LightGBM model","text":"<p>The LightGBM Classifier documentation can be found here</p>"},{"location":"discussion/benchmarks/#results","title":"Results","text":""},{"location":"howto/Optimizations/","title":"Optimizations in the bucketing process","text":"<pre><code>import pandas as pd\nfrom IPython.display import display\n\nfrom skorecard.datasets import load_credit_card\n\ndf = load_credit_card(as_frame=True)\n\n# Show\ndisplay(df.head(4))\n\nnum_feats = [\"x1\", \"x15\", \"x16\"]\n\nX = df[num_feats]\ny = df[\"y\"]\n</code></pre> x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 ... x15 x16 x17 x18 x19 x20 x21 x22 x23 y 0 20000.0 2.0 2.0 1.0 24.0 2.0 2.0 -1.0 -1.0 -2.0 ... 0.0 0.0 0.0 0.0 689.0 0.0 0.0 0.0 0.0 1 1 120000.0 2.0 2.0 2.0 26.0 -1.0 2.0 0.0 0.0 0.0 ... 3272.0 3455.0 3261.0 0.0 1000.0 1000.0 1000.0 0.0 2000.0 1 2 90000.0 2.0 2.0 2.0 34.0 0.0 0.0 0.0 0.0 0.0 ... 14331.0 14948.0 15549.0 1518.0 1500.0 1000.0 1000.0 1000.0 5000.0 0 3 50000.0 2.0 2.0 1.0 37.0 0.0 0.0 0.0 0.0 0.0 ... 28314.0 28959.0 29547.0 2000.0 2019.0 1200.0 1100.0 1069.0 1000.0 0 <p>4 rows \u00d7 24 columns</p> <pre><code>from skorecard.metrics import IV_scorer\nfrom skorecard.bucketers import DecisionTreeBucketer\nfrom sklearn.model_selection import GridSearchCV\n</code></pre> <p>The DecisionTreeBucketer has two main hyperparameters to grid-search: - <code>max_n_bins</code>, maximum number of bins allowed for the bucketing - <code>min_bin_size</code> minimum fraction of data in the buckets</p> <pre><code>gs_params = {\n    \"max_n_bins\": [3, 4, 5, 6],\n    \"min_bin_size\": [0.05, 0.06, 0.07, 0.08],  # , 0.12]\n}\n</code></pre> <p>The optimization has to be done for every feature indipendently, therefore we need a loop, and all the parameters are best stored in a data collector, like a dictionary</p> <pre><code># Define the specials\nbest_params = dict()\nmax_iv = dict()\ncv_results = dict()\n\n# Add a special for demo purposes\nspecials = {\"x1\": {\"special 0\": [\"50000.0\"]}}\n\nfor feat in num_feats:\n    # This snippet illustrates what to do with special values\n    if feat in specials.keys():\n        # This construct is needed to remap the specials, because skorecard validates that the key\n        # of the dictionary is present in the variables\n        special = {feat: specials[feat]}\n    else:\n        special = {}\n    bucketer = DecisionTreeBucketer(variables=[feat], specials=special)\n    gs = GridSearchCV(bucketer, gs_params, scoring=IV_scorer, cv=3, return_train_score=True)\n    gs.fit(X[[feat]], y)\n\n    best_params[feat] = gs.best_params_\n    max_iv[feat] = gs.best_score_\n    cv_results[feat] = gs.cv_results_\n</code></pre> <p>Checking the best parameters per feature</p> <pre><code>best_params\n</code></pre> <pre>\n<code>{'x1': {'max_n_bins': 3, 'min_bin_size': 0.05},\n 'x15': {'max_n_bins': 3, 'min_bin_size': 0.05},\n 'x16': {'max_n_bins': 3, 'min_bin_size': 0.05}}</code>\n</pre> <p>Because of its additive nature, IV is likely to be maximal for the highest <code>max_n_bins</code>.  Therefore it is worth looking analysing the CV results!</p> <pre><code>cv_results[\"x1\"]\n</code></pre> <pre>\n<code>{'mean_fit_time': array([0.14118997, 0.13273303, 0.13474902, 0.15843304, 0.17114846,\n        0.1259594 , 0.12854441, 0.13791513, 0.14939396, 0.12906257,\n        0.15454125, 0.11709793, 0.1234947 , 0.11326059, 0.11524073,\n        0.11928709]),\n 'std_fit_time': array([0.01615798, 0.00538481, 0.00918157, 0.02513249, 0.02130305,\n        0.0088008 , 0.0078898 , 0.00226504, 0.01736914, 0.00537724,\n        0.04489044, 0.00418452, 0.00750423, 0.00055744, 0.00241629,\n        0.01126566]),\n 'mean_score_time': array([0.03244432, 0.03500628, 0.03295326, 0.04452038, 0.04895496,\n        0.03155041, 0.03200722, 0.03328069, 0.0405368 , 0.03386513,\n        0.02966809, 0.03014151, 0.03117593, 0.02836776, 0.02895562,\n        0.02856787]),\n 'std_score_time': array([0.00520814, 0.00130717, 0.00347241, 0.00365442, 0.01206228,\n        0.00173939, 0.00279055, 0.0009404 , 0.01539335, 0.00089022,\n        0.00115331, 0.00201206, 0.00159311, 0.00142928, 0.0013334 ,\n        0.00086222]),\n 'param_max_n_bins': masked_array(data=[3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 6, 6, 6, 6],\n              mask=[False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False],\n        fill_value='?',\n             dtype=object),\n 'param_min_bin_size': masked_array(data=[0.05, 0.06, 0.07, 0.08, 0.05, 0.06, 0.07, 0.08, 0.05,\n                    0.06, 0.07, 0.08, 0.05, 0.06, 0.07, 0.08],\n              mask=[False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False],\n        fill_value='?',\n             dtype=object),\n 'params': [{'max_n_bins': 3, 'min_bin_size': 0.05},\n  {'max_n_bins': 3, 'min_bin_size': 0.06},\n  {'max_n_bins': 3, 'min_bin_size': 0.07},\n  {'max_n_bins': 3, 'min_bin_size': 0.08},\n  {'max_n_bins': 4, 'min_bin_size': 0.05},\n  {'max_n_bins': 4, 'min_bin_size': 0.06},\n  {'max_n_bins': 4, 'min_bin_size': 0.07},\n  {'max_n_bins': 4, 'min_bin_size': 0.08},\n  {'max_n_bins': 5, 'min_bin_size': 0.05},\n  {'max_n_bins': 5, 'min_bin_size': 0.06},\n  {'max_n_bins': 5, 'min_bin_size': 0.07},\n  {'max_n_bins': 5, 'min_bin_size': 0.08},\n  {'max_n_bins': 6, 'min_bin_size': 0.05},\n  {'max_n_bins': 6, 'min_bin_size': 0.06},\n  {'max_n_bins': 6, 'min_bin_size': 0.07},\n  {'max_n_bins': 6, 'min_bin_size': 0.08}],\n 'split0_test_score': array([0.079, 0.079, 0.079, 0.079, 0.097, 0.097, 0.097, 0.097, 0.106,\n        0.106, 0.106, 0.106, 0.107, 0.107, 0.107, 0.107]),\n 'split1_test_score': array([4.491, 4.491, 4.491, 4.491, 4.308, 4.308, 4.308, 4.308, 4.19 ,\n        4.19 , 4.19 , 4.19 , 4.043, 4.043, 4.043, 4.043]),\n 'split2_test_score': array([4.442, 4.442, 4.442, 4.442, 4.305, 4.305, 4.305, 4.305, 4.07 ,\n        4.07 , 4.07 , 4.07 , 3.975, 3.975, 3.975, 3.975]),\n 'mean_test_score': array([3.004     , 3.004     , 3.004     , 3.004     , 2.90333333,\n        2.90333333, 2.90333333, 2.90333333, 2.78866667, 2.78866667,\n        2.78866667, 2.78866667, 2.70833333, 2.70833333, 2.70833333,\n        2.70833333]),\n 'std_test_score': array([2.06838407, 2.06838407, 2.06838407, 2.06838407, 1.98437771,\n        1.98437771, 1.98437771, 1.98437771, 1.89756429, 1.89756429,\n        1.89756429, 1.89756429, 1.83962991, 1.83962991, 1.83962991,\n        1.83962991]),\n 'rank_test_score': array([ 1,  1,  1,  1,  5,  5,  5,  5,  9,  9,  9,  9, 13, 13, 13, 13],\n       dtype=int32),\n 'split0_train_score': array([0.049, 0.049, 0.049, 0.049, 0.049, 0.049, 0.049, 0.049, 0.05 ,\n        0.05 , 0.05 , 0.05 , 0.05 , 0.05 , 0.05 , 0.05 ]),\n 'split1_train_score': array([0.102, 0.102, 0.102, 0.102, 0.112, 0.112, 0.112, 0.112, 0.116,\n        0.116, 0.116, 0.116, 0.119, 0.119, 0.119, 0.119]),\n 'split2_train_score': array([0.119, 0.119, 0.119, 0.119, 0.144, 0.144, 0.144, 0.144, 0.156,\n        0.156, 0.156, 0.156, 0.159, 0.159, 0.159, 0.159]),\n 'mean_train_score': array([0.09      , 0.09      , 0.09      , 0.09      , 0.10166667,\n        0.10166667, 0.10166667, 0.10166667, 0.10733333, 0.10733333,\n        0.10733333, 0.10733333, 0.10933333, 0.10933333, 0.10933333,\n        0.10933333]),\n 'std_train_score': array([0.02981051, 0.02981051, 0.02981051, 0.02981051, 0.03946588,\n        0.03946588, 0.03946588, 0.03946588, 0.04370609, 0.04370609,\n        0.04370609, 0.04370609, 0.04502098, 0.04502098, 0.04502098,\n        0.04502098])}</code>\n</pre> <pre><code>from skorecard.bucketers import DecisionTreeBucketer, OptimalBucketer\nfrom skorecard.pipeline import BucketingProcess\nfrom skorecard.linear_model import LogisticRegression\nfrom skorecard.preprocessing import WoeEncoder\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.pipeline import make_pipeline\nfrom scipy.stats import uniform\n\n\ndef get_pipeline():\n    bucketing_process = BucketingProcess(\n        prebucketing_pipeline=make_pipeline(\n            DecisionTreeBucketer(max_n_bins=100, min_bin_size=0.05),\n        ),\n        bucketing_pipeline=make_pipeline(\n            OptimalBucketer(max_n_bins=10, min_bin_size=0.04),\n        ),\n    )\n\n    return make_pipeline(\n        bucketing_process, WoeEncoder(), LogisticRegression(solver=\"liblinear\", C=1.7, max_iter=150, random_state=0)\n    )\n\n\npipe = get_pipeline()\n\n\nparam_grid = [\n    {\"logisticregression__C\": uniform(loc=0, scale=4), \"logisticregression__solver\": [\"liblinear\"]},\n]\n\nsearch_cv = RandomizedSearchCV(\n    pipe, param_distributions=param_grid, cv=5, verbose=True, scoring=\"roc_auc\", n_jobs=-1, random_state=0, refit=True\n)\nsearch_cv.fit(X, y)\n</code></pre> <pre>\n<code>Fitting 5 folds for each of 10 candidates, totalling 50 fits\n</code>\n</pre> <pre>\n<code>RandomizedSearchCV(cv=5,\n                   estimator=Pipeline(steps=[('bucketingprocess',\n                                              BucketingProcess(bucketing_pipeline=Pipeline(steps=[('optimalbucketer',\n                                                                                                   OptimalBucketer(min_bin_size=0.04))]),\n                                                               prebucketing_pipeline=Pipeline(steps=[('decisiontreebucketer',\n                                                                                                      DecisionTreeBucketer())]))),\n                                             ('woeencoder', WoeEncoder()),\n                                             ('logisticregression',\n                                              LogisticRegression(C=1.7,\n                                                                 max_iter=150,\n                                                                 random_state=0,\n                                                                 solver='liblinear'))]),\n                   n_jobs=-1,\n                   param_distributions=[{'logisticregression__C': &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x7f97de25c5d0&gt;,\n                                         'logisticregression__solver': ['liblinear']}],\n                   random_state=0, scoring='roc_auc', verbose=True)</code>\n</pre> <pre><code>search_cv.best_params_, search_cv.best_score_\n</code></pre> <pre>\n<code>({'logisticregression__C': 2.860757465489678,\n  'logisticregression__solver': 'liblinear'},\n 0.6187444445104318)</code>\n</pre>"},{"location":"howto/Optimizations/#optimizing-the-bucketing-process","title":"Optimizing the bucketing process","text":""},{"location":"howto/Optimizations/#finding-the-best-bucketing","title":"Finding the best bucketing","text":"<p>The art of building a good scorecard model lies in finding the best bucketing strategy. Good buckets improve the predicitve power of the model, as well as guarantee stability of the predictions.</p> <p>This is normally a very manual, labour intensive process (and for a good reason).</p> <p>A good bucketing strategy follows the following principles: - maximizes the Information Values, defined as </p> \\[IV = \\sum_{i}(\\%G_{i}-\\%B_{i}) \\cdot \\log(\\frac{\\%G_{i}}{\\%B_{i}})\\] <ul> <li>avoids buckets that contain a very large or very small fraction of the population wherever the business sense requires it, </li> </ul> <p>The <code>skorecard</code> package provides some tooling to automate part of the process, namely:</p> <ul> <li>Grid search the hyper-parameters of the bucketers in order to maximise the information value</li> <li>Run the optimal bucketer within the bucketing process</li> </ul>"},{"location":"howto/Optimizations/#grid-search-the-bucketers-to-maximise-the-information-value","title":"Grid search the bucketers to maximise the information value","text":"<p><code>skorecard</code> implements an <code>IV_scorer</code>, that can be used as a custom scoring function for grid searching. The following snippets of code show how to integrate it in the grid search. The DecisionTreeBucketer applied on numerical features is the best use case, as there are some hyper-parameters that influence the bucketing quality. </p>"},{"location":"howto/Optimizations/#randomizedsearchcv-to-maximise-auc","title":"RandomizedSearchCV to maximise AUC","text":"<p>As <code>Skorecard</code> is scikit-learn compatibile we can use scikit-learn methods such as RandomizedSearchCV to maximise the AUC of our model. Shown below is one such example</p>"},{"location":"howto/mix_with_other_packages/","title":"Mixed usage with other packages","text":"<pre><code>%%capture\n!pip install category_encoders\n</code></pre> <pre><code>%%capture\nfrom sklearn.pipeline import make_pipeline\nfrom skorecard.datasets import load_uci_credit_card\nfrom skorecard.bucketers import OrdinalCategoricalBucketer\n\nX, y = load_uci_credit_card(return_X_y=True)\n\nfrom category_encoders import TargetEncoder\n\npipe = make_pipeline(\n    TargetEncoder(cols=[\"EDUCATION\"]),  #  category_encoders.TargetEncoder passes through other columns\n    OrdinalCategoricalBucketer(variables=[\"MARRIAGE\"]),\n)\npipe.fit(X, y)\n</code></pre> <pre><code>pipe.transform(X).head(5)\n</code></pre> EDUCATION MARRIAGE LIMIT_BAL BILL_AMT1 0 0.0 2.0 400000.0 201800.0 1 1.0 2.0 80000.0 80610.0 2 0.0 2.0 500000.0 499452.0 3 0.0 1.0 140000.0 450.0 4 1.0 1.0 420000.0 56107.0 <p>Some packages do not return pandas DataFrames, like:</p> <ul> <li><code>sklearn.preprocessing.KBinsDiscretizer</code></li> </ul> <p>You can wrap the class in <code>skorecard.pipeline.KeepPandas</code> to use these transformers in a pipeline: </p> <pre><code>from sklearn.preprocessing import KBinsDiscretizer\nfrom skorecard.pipeline import KeepPandas\nfrom sklearn.compose import ColumnTransformer\n\nct = ColumnTransformer(\n    [(\"binner\", KBinsDiscretizer(n_bins=3, encode=\"ordinal\", strategy=\"uniform\"), [\"EDUCATION\"])],\n    remainder=\"passthrough\",\n)\npipe = make_pipeline(KeepPandas(ct), OrdinalCategoricalBucketer(variables=[\"MARRIAGE\"]))\npipe.fit_transform(X, y).head(5)\n</code></pre> <pre>\n<code>WARNING:root:sklearn.compose.ColumnTransformer can change the order of columns, be very careful when using with KeepPandas()\n</code>\n</pre> EDUCATION MARRIAGE LIMIT_BAL BILL_AMT1 0 0.0 2.0 400000.0 201800.0 1 1.0 2.0 80000.0 80610.0 2 0.0 2.0 500000.0 499452.0 3 0.0 1.0 140000.0 450.0 4 1.0 1.0 420000.0 56107.0"},{"location":"howto/mix_with_other_packages/#mixed-usage-with-other-packages","title":"Mixed usage with other packages","text":"<p>There are quite some excellent packages out there offering functionality around bucketing/binning/discretizing numerical variables and encoding categorical variables. Chances are you'd like to combine them in your <code>skorecard</code> pipelines.</p> <p>Here are some packages are are compatible with pandas dataframes:</p> <ul> <li><code>category_encoders</code> from scikit-learn-contrib</li> <li><code>feature-engine</code> categorical variable encoders</li> <li><code>feature-engine</code> variable discretisation</li> </ul>"},{"location":"howto/psi_and_iv/","title":"Assessing bucket quality","text":"<pre><code>from skorecard import datasets\nfrom sklearn.model_selection import train_test_split\nfrom skorecard.bucketers import DecisionTreeBucketer\n\nX, y = datasets.load_uci_credit_card(return_X_y=True)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n</code></pre> <p>By definition, the PSI acts on bucketed features.</p> <p>Failing to bucket the features would still yield a value of the PSI. However, in this case the PSI will be computed over all the unique values. For numerical features, this will return artifically high and meaningless values.</p> <pre><code>dbt = DecisionTreeBucketer()\n\nX_train_bins = dbt.fit_transform(X_train, y_train)\nX_test_bins = dbt.transform(X_test)\n</code></pre> <p>Calculating the PSI</p> <pre><code>from skorecard.reporting import psi\n\npsi_dict = psi(X_train_bins, X_test_bins)\npsi_dict\n</code></pre> <pre>\n<code>{'EDUCATION': 0.0005202506508081382,\n 'MARRIAGE': 0.0003497580712116056,\n 'LIMIT_BAL': 0.013577676978376134,\n 'BILL_AMT1': 0.017027519474734677}</code>\n</pre> <pre><code>dbt = DecisionTreeBucketer()\nX_bins = dbt.fit_transform(X, y)\n</code></pre> <p>To compute the iv, <code>skorecard</code> implements a handy function. The function consumes the (binned) feature set X, and the target y</p> <pre><code>from skorecard.reporting import iv\n\niv_result = iv(X_bins, y)\niv_result\n</code></pre> <pre>\n<code>{'EDUCATION': 0.036451028950383324,\n 'MARRIAGE': 0.009494315565036299,\n 'LIMIT_BAL': 0.17922043483265943,\n 'BILL_AMT1': 0.05239237644085838}</code>\n</pre>"},{"location":"howto/psi_and_iv/#measuring-bucketed-distribution-shifts","title":"Measuring bucketed distribution shifts.","text":""},{"location":"howto/psi_and_iv/#population-staibility-index-psi","title":"Population staibility index - PSI","text":"<p>The PSI (population stability index), is a common measure to evaluate how similar two univariate distributions are.</p> <p>It's given by the following formula</p> \\[PSI=\\sum_{i}^{N_{bins}} (\\%x_{i}^{actual} - \\%x_{i}^{expected}) log\\frac{\\%x_{i}^{actual}}{\\%x_{i}^{expected}}\\] <p>where the sum runs over all the buckets of the feature <code>x</code>.</p> <p><code>skorecard</code> implements a simple functionality to calculate the PSI between two datasets. As two datasets are needed, we split the X and y into a train and test set.</p>"},{"location":"howto/psi_and_iv/#univariate-predictive-power","title":"Univariate predictive power","text":""},{"location":"howto/psi_and_iv/#information-value-iv","title":"Information value (IV)","text":"<p>The information value is nothing else than the PSI, but it's computed between the features set defined by the target y=0 and y=1. </p> <p>In other words, it can be summarized by the formula.</p> \\[IV=\\sum_{i}^{N_{bins}} (\\%x_{i}^{y=0} - \\%x_{i}^{y=1}) log\\frac{\\%x_{i}^{y=0}}{\\%x_{i}^{y=1}}\\]"},{"location":"howto/save_buckets_to_file/","title":"Read/write buckets to file","text":"<pre><code>from skorecard.datasets import load_uci_credit_card\nfrom skorecard.bucketers import DecisionTreeBucketer, UserInputBucketer\n\nX, y = load_uci_credit_card(return_X_y=True)\n\nbucketer = DecisionTreeBucketer(max_n_bins=10)\nbucketer = bucketer.fit(X, y)\nbucketer.save_yml(\"bucketer.yml\")\n\nuib = UserInputBucketer(\"bucketer.yml\")\nuib.transform(X).head(4)\n</code></pre> EDUCATION MARRIAGE LIMIT_BAL BILL_AMT1 0 0 1 9 9 1 1 1 3 7 2 0 1 9 9 3 0 0 5 0 <pre><code>from skorecard.pipeline import BucketingProcess\nfrom skorecard.bucketers import EqualFrequencyBucketer, OptimalBucketer, AsIsCategoricalBucketer\nfrom sklearn.pipeline import make_pipeline\n\nnum_cols = [\"LIMIT_BAL\", \"BILL_AMT1\"]\ncat_cols = [\"EDUCATION\", \"MARRIAGE\"]\n\nbucketing_process = BucketingProcess(\n    prebucketing_pipeline=make_pipeline(\n        DecisionTreeBucketer(variables=num_cols, max_n_bins=100, min_bin_size=0.05),\n        AsIsCategoricalBucketer(variables=cat_cols),\n    ),\n    bucketing_pipeline=make_pipeline(\n        OptimalBucketer(variables=num_cols, max_n_bins=10, min_bin_size=0.05),\n        OptimalBucketer(variables=cat_cols, variables_type=\"categorical\", max_n_bins=10, min_bin_size=0.05),\n    ),\n)\n\nbucketing_process.fit(X, y)\nbucketing_process.save_yml(\"bucket_process.yml\")\n\nuib = UserInputBucketer(\"bucket_process.yml\")\nuib.transform(X).head(4)\n</code></pre> EDUCATION MARRIAGE LIMIT_BAL BILL_AMT1 0 0 0 8 5 1 2 0 3 4 2 0 0 8 5 3 0 1 4 0 <pre><code>from sklearn.pipeline import make_pipeline\nfrom skorecard.bucketers import EqualFrequencyBucketer\nfrom skorecard.pipeline.pipeline import to_skorecard_pipeline\n\npipe = make_pipeline(\n    EqualFrequencyBucketer(n_bins=10, variables=[\"BILL_AMT1\"]),\n    DecisionTreeBucketer(max_n_bins=5, variables=[\"LIMIT_BAL\"]),\n)\npipe.fit(X, y)\nsk_pipe = to_skorecard_pipeline(pipe)\nsk_pipe.save_yml(\"pipe.yml\")\n\nuib = UserInputBucketer(\"pipe.yml\")\nuib.transform(X).head(4)\n</code></pre> EDUCATION MARRIAGE LIMIT_BAL BILL_AMT1 0 1 2 4 9 1 2 2 2 7 2 1 2 4 9 3 1 1 3 1"},{"location":"howto/save_buckets_to_file/#saving-bucket-information-to-a-file","title":"Saving bucket information to a file","text":"<p>If you have a specific set of bucketing boundaries you are satisfied with, it's useful to save them to a file. You might want to save the bucketing information as configuration files along with your code.</p> <p>All <code>skorecard</code> bucketers, the BucketingProcess and <code>Skorecard</code> model support saving to yaml files with <code>save_yml()</code>. </p> <p>The special <code>UserInputBucketer</code> can read in these configuration files and can be used in the final model pipeline.</p>"},{"location":"howto/save_buckets_to_file/#example-with-a-bucketer","title":"Example with a bucketer","text":""},{"location":"howto/save_buckets_to_file/#example-with-bucketingprocess","title":"Example with BucketingProcess","text":"<p>A bucketing process works in exactly the same way. Because there is a prebucketing pipeline and a bucketing pipeline, <code>skorecard</code> makes sure that the buckets are the transformation from raw data to final bucket.</p>"},{"location":"howto/save_buckets_to_file/#example-with-scorecardpipelines","title":"Example with ScorecardPipelines","text":"<p><code>skorecard</code> supports converting <code>scikit-learn</code> pipelines to a <code>SkorecardPipeline</code> using <code>to_skorecard_pipeline</code>. This will add support for <code>.save_yml()</code>:</p>"},{"location":"howto/using_manually_defined_buckets/","title":"Manually defining buckets","text":"<pre><code>from skorecard.datasets import load_uci_credit_card, load_credit_card\nfrom skorecard.bucketers import DecisionTreeBucketer, UserInputBucketer\n\nX, y = load_uci_credit_card(return_X_y=True)\n\nbucketer = DecisionTreeBucketer(variables=[\"EDUCATION\"])\nbucketer.fit(X, y)  # can also be .fit_interactive()\nbucketer.features_bucket_mapping_\n</code></pre> <pre>\n<code>FeaturesBucketMapping([BucketMapping(feature_name='EDUCATION', type='numerical', missing_bucket=None, other_bucket=None, map=[1.5, 2.5], right=False, specials={})])</code>\n</pre> <pre><code>uib = UserInputBucketer(bucketer.features_bucket_mapping_)\nuib.transform(X).head(1)  # note uib does not require a .fit() step\n</code></pre> EDUCATION MARRIAGE LIMIT_BAL BILL_AMT1 0 0 2 400000.0 201800.0 <pre><code>bucket_maps = {\n    \"EDUCATION\": {\n        \"feature_name\": \"EDUCATION\",\n        \"type\": \"categorical\",\n        \"map\": {2: 0, 1: 1, 3: 2},\n        \"right\": True,\n        \"specials\": {},\n    },\n    \"LIMIT_BAL\": {\n        \"feature_name\": \"LIMIT_BAL\",\n        \"type\": \"numerical\",\n        \"map\": [25000.0, 55000.0, 105000.0, 225000.0, 275000.0, 325000.0],\n        \"right\": True,\n        \"specials\": {},\n    },\n    \"BILL_AMT1\": {\n        \"feature_name\": \"BILL_AMT1\",\n        \"type\": \"numerical\",\n        \"map\": [800.0, 12500, 50000, 77800, 195000.0],\n        \"right\": True,\n        \"specials\": {},\n    },\n}\n</code></pre> <p>You can create a bucketer using the input dictionary using <code>UserInputBucketer</code>:</p> <pre><code>from skorecard.bucketers import UserInputBucketer\n\nuib = UserInputBucketer(bucket_maps)\n</code></pre>"},{"location":"howto/using_manually_defined_buckets/#working-with-manually-defined-buckets","title":"Working with manually defined buckets","text":"<p>Often bucketing is tweaked manually to incorporate domain expertise. Skorecard offers good support for manually defining buckets.</p>"},{"location":"howto/using_manually_defined_buckets/#from-a-bucketer","title":"From a bucketer","text":"<p>If you've used <code>.fit_interactive()</code> (see interactive bucketing), you can choose to explicitly use the updated bucket mapping in a <code>UserInputBucketer</code>:</p>"},{"location":"howto/using_manually_defined_buckets/#from-a-dictionary","title":"From a dictionary","text":"<p>You can manually define the buckets in a python dictionary. For every feature, the following keys must be present.</p> <ul> <li><code>feature_name</code> (mandatory): must match the column name in the dataframe</li> <li><code>type</code> (mandatory): type of feature (categorical or numerical)</li> <li><code>map</code> (mandatory): contains the actual mapping for the bins.<ul> <li>categorical features: expect a dictionary <code>{value:bin_index}</code></li> <li>numerical features: expect a list of boundaries <code>[value, value]</code></li> </ul> </li> <li><code>right</code> (optional, defaults to <code>True</code>): flag that indicates if to include the upper bound (True) or lower bound (False) in the bucket definition. Applicable only to numerical bucketers</li> <li><code>specials</code> (optional, defaults to <code>{}</code>): dictionary of special values that will be put in their own bucket.</li> </ul> <p>Here's an example:</p>"},{"location":"howto/using_manually_defined_buckets/#from-a-file","title":"From a file","text":"<p>You can also work with manually defined buckets that have saved in a <code>.yml</code> file. See the how to on Read/write buckets to file.</p>"},{"location":"tutorials/1_bucketing/","title":"Bucketing features","text":"<pre><code>from skorecard.datasets import load_credit_card\n\ndata = load_credit_card(as_frame=True)\n</code></pre> <pre><code>data.head()\n</code></pre> x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 ... x15 x16 x17 x18 x19 x20 x21 x22 x23 y 0 20000.0 2.0 2.0 1.0 24.0 2.0 2.0 -1.0 -1.0 -2.0 ... 0.0 0.0 0.0 0.0 689.0 0.0 0.0 0.0 0.0 1 1 120000.0 2.0 2.0 2.0 26.0 -1.0 2.0 0.0 0.0 0.0 ... 3272.0 3455.0 3261.0 0.0 1000.0 1000.0 1000.0 0.0 2000.0 1 2 90000.0 2.0 2.0 2.0 34.0 0.0 0.0 0.0 0.0 0.0 ... 14331.0 14948.0 15549.0 1518.0 1500.0 1000.0 1000.0 1000.0 5000.0 0 3 50000.0 2.0 2.0 1.0 37.0 0.0 0.0 0.0 0.0 0.0 ... 28314.0 28959.0 29547.0 2000.0 2019.0 1200.0 1100.0 1069.0 1000.0 0 4 50000.0 1.0 2.0 1.0 57.0 -1.0 0.0 -1.0 0.0 0.0 ... 20940.0 19146.0 19131.0 2000.0 36681.0 10000.0 9000.0 689.0 679.0 0 <p>5 rows \u00d7 24 columns</p> <p>The dataset used contains 30K rows, 23 features and a binary target.</p> <p>Let's start by splitting the data in train and test sample, as per common practice when building a model.</p> <pre><code>from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    data.drop([\"y\"], axis=1), data[\"y\"], test_size=0.25, random_state=42\n)\n</code></pre> <pre><code>cat_cols = [\"x2\", \"x3\", \"x4\"]\nnum_cols = [col for col in X_train.columns if col not in cat_cols]\n\nprint(f\"Total categorical columns: {len(cat_cols)}\")\nprint(f\"Total numerical columns: {len(num_cols)}\")\n</code></pre> <pre>\n<code>Total categorical columns: 3\nTotal numerical columns: 20\n</code>\n</pre> <pre><code># Start with categorical features\nfrom skorecard.bucketers import OrdinalCategoricalBucketer\n</code></pre> <pre><code>cat_bucketer = OrdinalCategoricalBucketer(variables=cat_cols, tol=0.05, encoding_method=\"ordered\")\n\ncat_bucketer.fit(X_train, y_train)\n</code></pre> <pre>\n<code>OrdinalCategoricalBucketer(encoding_method='ordered',\n                           variables=['x2', 'x3', 'x4'])</code>\n</pre> <p>Example: Look up the feature 'x3'</p> <p>The feature 'x3' has a few categories that are sparsely-populated</p> <pre><code>X_train[\"x3\"].value_counts(normalize=True)\n</code></pre> <pre>\n<code>2.0    0.466311\n1.0    0.354089\n3.0    0.163911\n5.0    0.009333\n4.0    0.004089\n6.0    0.001822\n0.0    0.000444\nName: x3, dtype: float64</code>\n</pre> <p>The OrdinalCategorical bucketer populates the <code>other</code> category with sparse values</p> <pre><code>cat_bucketer.bucket_table(\"x3\")\n</code></pre> bucket label Count Count (%) Non-event Event Event Rate WoE IV 0 -2 Other 353.0 1.57 326.0 27.0 0.076487 1.239 0.016 1 -1 Missing 0.0 0.00 0.0 0.0 NaN 0.000 0.000 2 0 3.0 3688.0 16.39 2755.0 933.0 0.252983 -0.168 0.005 3 1 1.0 7967.0 35.41 6445.0 1522.0 0.191038 0.193 0.012 4 2 2.0 10492.0 46.63 7965.0 2527.0 0.240850 -0.102 0.005 <p>Note There might be a different strategy applied to different bucketers. This is addressed by defining a pipeline of bucketers (see the numerical features section for details)</p> <pre><code>X_train[\"x1\"].value_counts()\n</code></pre> <pre>\n<code>50000.0      2510\n20000.0      1470\n30000.0      1218\n80000.0      1173\n200000.0     1151\n             ... \n650000.0        1\n1000000.0       1\n730000.0        1\n690000.0        1\n327680.0        1\nName: x1, Length: 79, dtype: int64</code>\n</pre> <pre><code>from skorecard.bucketers import DecisionTreeBucketer\n\nspecials = {\"x1\": {\"special_demo\": [50000]}}\n\nnum_bucketer = DecisionTreeBucketer(\n    variables=num_cols,\n    max_n_bins=5,  # max number of bins allowed\n    min_bin_size=0.06,  # min fraction of data allowed in the bin\n    dt_kwargs={\n        \"criterion\": \"entropy\",\n        \"min_impurity_decrease\": 0.0005,  # as in sklearn. Helps to decide how to split the buckets\n    },\n    specials=specials,\n)\nnum_bucketer.fit(X_train, y_train)\n</code></pre> <pre>\n<code>DecisionTreeBucketer(dt_kwargs={'criterion': 'entropy',\n                                'min_impurity_decrease': 0.0005,\n                                'random_state': None},\n                     max_n_bins=5, min_bin_size=0.06,\n                     specials={'x1': {'special_demo': [50000]}},\n                     variables=['x1', 'x5', 'x6', 'x7', 'x8', 'x9', 'x10',\n                                'x11', 'x12', 'x13', 'x14', 'x15', 'x16', 'x17',\n                                'x18', 'x19', 'x20', 'x21', 'x22', 'x23'])</code>\n</pre> <pre><code>num_bucketer.bucket_table(\"x1\")\n</code></pre> bucket label Count Count (%) Non-event Event Event Rate WoE IV 0 -3 Special: special_demo 2510.0 11.16 1848.0 662.0 0.263745 -0.224 0.006 1 -1 Missing 0.0 0.00 0.0 0.0 NaN 0.000 0.000 2 0 [-inf, 75000.0) 4399.0 19.55 2880.0 1519.0 0.345306 -0.611 0.085 3 1 [75000.0, 145000.0) 4567.0 20.30 3494.0 1073.0 0.234946 -0.070 0.001 4 2 [145000.0, 375000.0) 9230.0 41.02 7685.0 1545.0 0.167389 0.354 0.046 5 3 [375000.0, inf) 1794.0 7.97 1584.0 210.0 0.117057 0.770 0.037 <pre><code>from IPython.display import display\n\nfor x in num_cols[:3]:\n    display(num_bucketer.fit(X_train, y_train).bucket_table(x))\n</code></pre> bucket label Count Count (%) Non-event Event Event Rate WoE IV 0 -3 Special: special_demo 2510.0 11.16 1848.0 662.0 0.263745 -0.224 0.006 1 -1 Missing 0.0 0.00 0.0 0.0 NaN 0.000 0.000 2 0 [-inf, 75000.0) 4399.0 19.55 2880.0 1519.0 0.345306 -0.611 0.085 3 1 [75000.0, 145000.0) 4567.0 20.30 3494.0 1073.0 0.234946 -0.070 0.001 4 2 [145000.0, 375000.0) 9230.0 41.02 7685.0 1545.0 0.167389 0.354 0.046 5 3 [375000.0, inf) 1794.0 7.97 1584.0 210.0 0.117057 0.770 0.037 bucket label Count Count (%) Non-event Event Event Rate WoE IV 0 -1 Missing 0.0 0.00 0.0 0.0 NaN 0.000 0.000 1 0 [-inf, 25.5) 2927.0 13.01 2143.0 784.0 0.267851 -0.245 0.008 2 1 [25.5, 35.5) 9664.0 42.95 7750.0 1914.0 0.198055 0.148 0.009 3 2 [35.5, inf) 9909.0 44.04 7598.0 2311.0 0.233222 -0.060 0.002 bucket label Count Count (%) Non-event Event Event Rate WoE IV 0 -1 Missing 0.0 0.00 0.0 0.0 NaN 0.000 0.000 1 0 [-inf, -0.5) 6370.0 28.31 5361.0 1009.0 0.158399 0.420 0.044 2 1 [-0.5, 0.5) 11032.0 49.03 9620.0 1412.0 0.127991 0.668 0.179 3 2 [0.5, 1.5) 2720.0 12.09 1796.0 924.0 0.339706 -0.586 0.048 4 3 [1.5, inf) 2378.0 10.57 714.0 1664.0 0.699748 -2.096 0.611 <pre><code>from sklearn.pipeline import make_pipeline\nfrom skorecard.bucketers import EqualFrequencyBucketer, DecisionTreeBucketer\n\npipe = make_pipeline(\n    EqualFrequencyBucketer(n_bins=5, variables=num_cols[:5]), DecisionTreeBucketer(max_n_bins=5, variables=num_cols[5:])\n)\npipe.fit(X_train, y_train)\n</code></pre> <pre>\n<code>/Users/iv58uq/Documents/open_source/skorecard/skorecard/bucketers/bucketers.py:502: ApproximationWarning: Approximated quantiles - too many unique values\n  warnings.warn(ApproximationWarning(\"Approximated quantiles - too many unique values\"))\n/Users/iv58uq/Documents/open_source/skorecard/skorecard/bucketers/bucketers.py:502: ApproximationWarning: Approximated quantiles - too many unique values\n  warnings.warn(ApproximationWarning(\"Approximated quantiles - too many unique values\"))\n/Users/iv58uq/Documents/open_source/skorecard/skorecard/bucketers/bucketers.py:502: ApproximationWarning: Approximated quantiles - too many unique values\n  warnings.warn(ApproximationWarning(\"Approximated quantiles - too many unique values\"))\n</code>\n</pre> <pre>\n<code>Pipeline(steps=[('equalfrequencybucketer',\n                 EqualFrequencyBucketer(variables=['x1', 'x5', 'x6', 'x7',\n                                                   'x8'])),\n                ('decisiontreebucketer',\n                 DecisionTreeBucketer(max_n_bins=5,\n                                      variables=['x9', 'x10', 'x11', 'x12',\n                                                 'x13', 'x14', 'x15', 'x16',\n                                                 'x17', 'x18', 'x19', 'x20',\n                                                 'x21', 'x22', 'x23']))])</code>\n</pre> <pre><code>pipe.transform(X_test)\n</code></pre> x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 ... x14 x15 x16 x17 x18 x19 x20 x21 x22 x23 2308 0 1.0 2.0 2.0 0 1 1 1 2 2 ... 2 1 3 3 1 2 1 1 1 1 22404 2 2.0 1.0 2.0 0 1 1 1 2 2 ... 4 4 4 4 2 2 2 3 2 2 23397 1 2.0 3.0 1.0 2 1 1 1 2 2 ... 3 4 4 4 1 2 2 2 1 2 25058 2 1.0 3.0 2.0 4 1 1 1 2 2 ... 2 2 2 1 1 2 3 1 3 2 2664 0 2.0 2.0 2.0 2 1 1 1 2 2 ... 3 2 3 3 1 1 1 2 0 1 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 3211 0 2.0 3.0 1.0 4 2 0 0 0 0 ... 0 0 0 0 0 0 0 0 1 1 9355 4 2.0 2.0 1.0 3 1 1 1 2 2 ... 2 3 3 4 3 3 3 3 3 2 28201 4 2.0 3.0 2.0 4 2 2 2 1 2 ... 0 0 1 0 1 0 1 1 1 0 19705 1 2.0 2.0 1.0 4 0 0 0 1 1 ... 0 1 0 0 3 0 3 0 0 0 28313 2 2.0 3.0 1.0 2 0 0 0 1 1 ... 0 1 0 0 1 1 1 0 1 0 <p>7500 rows \u00d7 23 columns</p> <pre><code>print(f\"\\nDecisionTreeBucketer, with IV: {num_bucketer.bucket_table(num_cols[0])['IV'].sum()}\")\ndisplay(num_bucketer.bucket_table(num_cols[0]))\n\nprint(f\"\\nEqualFrequencyBucketer, with IV: {pipe.steps[0][1].bucket_table(num_cols[0])['IV'].sum()}\")\ndisplay(pipe.steps[0][1].bucket_table(num_cols[0]))\n</code></pre> <pre>\n<code>\nDecisionTreeBucketer, with IV: 0.17500000000000002\n</code>\n</pre> bucket label Count Count (%) Non-event Event Event Rate WoE IV 0 -3 Special: special_demo 2510.0 11.16 1848.0 662.0 0.263745 -0.224 0.006 1 -1 Missing 0.0 0.00 0.0 0.0 NaN 0.000 0.000 2 0 [-inf, 75000.0) 4399.0 19.55 2880.0 1519.0 0.345306 -0.611 0.085 3 1 [75000.0, 145000.0) 4567.0 20.30 3494.0 1073.0 0.234946 -0.070 0.001 4 2 [145000.0, 375000.0) 9230.0 41.02 7685.0 1545.0 0.167389 0.354 0.046 5 3 [375000.0, inf) 1794.0 7.97 1584.0 210.0 0.117057 0.770 0.037 <pre>\n<code>\nEqualFrequencyBucketer, with IV: 0.159\n</code>\n</pre> bucket label Count Count (%) Non-event Event Event Rate WoE IV 0 -1 Missing 0.0 0.00 0.0 0.0 NaN 0.000 0.000 1 0 (-inf, 50000.0] 5741.0 25.52 3885.0 1856.0 0.323289 -0.512 0.076 2 1 (50000.0, 100000.0] 3613.0 16.06 2691.0 922.0 0.255190 -0.179 0.005 3 2 (100000.0, 180000.0] 4629.0 20.57 3711.0 918.0 0.198315 0.146 0.004 4 3 (180000.0, 270000.0] 4062.0 18.05 3362.0 700.0 0.172329 0.319 0.017 5 4 (270000.0, inf] 4455.0 19.80 3842.0 613.0 0.137598 0.585 0.057 <pre><code>bucketing_pipe = make_pipeline(num_bucketer, cat_bucketer)\nbucketing_pipe.fit(X_train, y_train)\n</code></pre> <pre>\n<code>Pipeline(steps=[('decisiontreebucketer',\n                 DecisionTreeBucketer(dt_kwargs={'criterion': 'entropy',\n                                                 'min_impurity_decrease': 0.0005,\n                                                 'random_state': None},\n                                      max_n_bins=5, min_bin_size=0.06,\n                                      specials={'x1': {'special_demo': [50000]}},\n                                      variables=['x1', 'x5', 'x6', 'x7', 'x8',\n                                                 'x9', 'x10', 'x11', 'x12',\n                                                 'x13', 'x14', 'x15', 'x16',\n                                                 'x17', 'x18', 'x19', 'x20',\n                                                 'x21', 'x22', 'x23'])),\n                ('ordinalcategoricalbucketer',\n                 OrdinalCategoricalBucketer(encoding_method='ordered',\n                                            variables=['x2', 'x3', 'x4']))])</code>\n</pre> <pre><code>from skorecard.pipeline import to_skorecard_pipeline\n\nto_skorecard_pipeline(bucketing_pipe).save_yml(open(\"buckets.yml\", \"w\"))\n</code></pre> <pre><code>from skorecard.bucketers import DecisionTreeBucketer, OrdinalCategoricalBucketer, OptimalBucketer\nfrom skorecard.pipeline import BucketingProcess\nfrom sklearn.pipeline import make_pipeline\n\n\nspecials = {\"x1\": {\"special_demo\": [50000]}}\n\n\nprebucketing_pipeline = make_pipeline(\n    DecisionTreeBucketer(variables=num_cols, max_n_bins=40, min_bin_size=0.03),  # loose requirements\n    OrdinalCategoricalBucketer(variables=cat_cols, tol=0.02),\n)\n\nbucketing_pipeline = make_pipeline(\n    OptimalBucketer(variables=num_cols, max_n_bins=6, min_bin_size=0.05),\n    OptimalBucketer(variables=cat_cols, variables_type=\"categorical\", max_n_bins=10, min_bin_size=0.05),\n)\n\n\nbucketing_process = BucketingProcess(\n    prebucketing_pipeline=prebucketing_pipeline,\n    bucketing_pipeline=bucketing_pipeline,\n    specials=specials,\n)\n\nbucketing_process = bucketing_process.fit(X_train, y_train)\n</code></pre> <pre><code>print(f\"Prebucketing step (fine classing), IV {bucketing_process.prebucket_table('x1')['IV'].sum()}\")\ndisplay(bucketing_process.prebucket_table(\"x1\"))\n\n\nprint(f\"\\nBucketing step (coarse classing), IV {bucketing_process.bucket_table('x1')['IV'].sum()}\")\ndisplay(bucketing_process.bucket_table(\"x1\"))\n</code></pre> <pre>\n<code>Prebucketing step (fine classing), IV 0.192\n</code>\n</pre> pre-bucket label Count Count (%) Non-event Event Event Rate WoE IV bucket 0 -3 Special: special_demo 2510.0 11.16 1848.0 662.0 0.263745 -0.224 0.006 -3 1 -1 Missing 0.0 0.00 0.0 0.0 NaN 0.000 0.000 -1 2 0 [-inf, 25000.0) 1830.0 8.13 1153.0 677.0 0.369945 -0.718 0.050 0 3 1 [25000.0, 50000.0) 1401.0 6.23 884.0 517.0 0.369022 -0.714 0.038 0 4 2 [50000.0, 75000.0) 1168.0 5.19 843.0 325.0 0.278253 -0.297 0.005 1 5 3 [75000.0, 85000.0) 1173.0 5.21 907.0 266.0 0.226769 -0.024 0.000 2 6 4 [85000.0, 105000.0) 1272.0 5.65 941.0 331.0 0.260220 -0.206 0.003 2 7 5 [105000.0, 125000.0) 995.0 4.42 770.0 225.0 0.226131 -0.020 0.000 2 8 6 [125000.0, 145000.0) 1127.0 5.01 876.0 251.0 0.222715 -0.001 0.000 2 9 7 [145000.0, 155000.0) 821.0 3.65 683.0 138.0 0.168088 0.349 0.004 3 10 8 [155000.0, 175000.0) 937.0 4.16 768.0 169.0 0.180363 0.263 0.003 3 11 9 [175000.0, 185000.0) 749.0 3.33 614.0 135.0 0.180240 0.264 0.002 3 12 10 [185000.0, 205000.0) 1321.0 5.87 1070.0 251.0 0.190008 0.199 0.002 3 13 11 [205000.0, 225000.0) 877.0 3.90 729.0 148.0 0.168757 0.344 0.004 3 14 12 [225000.0, 245000.0) 1037.0 4.61 859.0 178.0 0.171649 0.323 0.004 3 15 13 [245000.0, 285000.0) 1193.0 5.30 1018.0 175.0 0.146689 0.510 0.012 4 16 14 [285000.0, 305000.0) 680.0 3.02 573.0 107.0 0.157353 0.427 0.005 4 17 15 [305000.0, 355000.0) 908.0 4.04 791.0 117.0 0.128855 0.660 0.014 4 18 16 [355000.0, 375000.0) 707.0 3.14 580.0 127.0 0.179632 0.268 0.002 4 19 17 [375000.0, 495000.0) 1098.0 4.88 965.0 133.0 0.121129 0.731 0.021 5 20 18 [495000.0, inf) 696.0 3.09 619.0 77.0 0.110632 0.833 0.017 5 <pre>\n<code>\nBucketing step (coarse classing), IV 0.186\n</code>\n</pre> bucket label Count Count (%) Non-event Event Event Rate WoE IV 0 -3 Special: special_demo 2510.0 11.16 1848.0 662.0 0.263745 -0.224 0.006 1 -1 Missing 0.0 0.00 0.0 0.0 NaN 0.000 0.000 2 0 [-inf, 2.0) 3231.0 14.36 2037.0 1194.0 0.369545 -0.716 0.087 3 1 [2.0, 3.0) 1168.0 5.19 843.0 325.0 0.278253 -0.297 0.005 4 2 [3.0, 7.0) 4567.0 20.30 3494.0 1073.0 0.234946 -0.070 0.001 5 3 [7.0, 13.0) 5742.0 25.52 4723.0 1019.0 0.177464 0.283 0.019 6 4 [13.0, 17.0) 3488.0 15.50 2962.0 526.0 0.150803 0.478 0.031 7 5 [17.0, inf) 1794.0 7.97 1584.0 210.0 0.117057 0.770 0.037 <p>Notice in the first table there is an additional column that is the optimized buckets (those indexes are aggregated in the second table).  Because of the additive nature of the IV, by reducing the number of buckets, the IV will normally decrease.  The goal is to reduce the number of buckets as much as possible, by keeping a high IV value, and check for monotonicity wherever needed.</p> <pre><code>y_train.value_counts(normalize=True)\n</code></pre> <pre>\n<code>0    0.777378\n1    0.222622\nName: y, dtype: float64</code>\n</pre> <pre><code>bucketing_process.plot_prebucket(\"x1\", format=\"png\", scale=2, width=1050, height=525)\n</code></pre> <pre><code>bucketing_process.plot_bucket(\"x1\", format=\"png\", scale=2, width=1050, height=525)\n</code></pre> <pre><code>bucketing_process.summary()\n</code></pre> column num_prebuckets num_buckets IV_score dtype 0 x1 21 8 0.002778 float64 1 x2 4 4 0.001921 float64 2 x3 5 5 0.002564 float64 3 x4 4 4 0.001909 float64 4 x5 24 3 0.001914 float64 5 x6 6 5 0.002351 float64 6 x7 5 3 0.002112 float64 7 x8 5 4 0.002129 float64 8 x9 5 4 0.002809 float64 9 x10 5 4 0.001997 float64 10 x11 5 4 0.002106 float64 11 x12 26 7 0.002408 float64 12 x13 27 6 0.002798 float64 13 x14 25 5 0.002571 float64 14 x15 22 6 0.002321 float64 15 x16 24 4 0.002236 float64 16 x17 25 4 0.002109 float64 17 x18 21 7 0.003035 float64 18 x19 21 7 0.003631 float64 19 x20 21 7 0.001999 float64 20 x21 22 7 0.002747 float64 21 x22 19 7 0.002383 float64 22 x23 20 7 0.002647 float64 <pre><code>bucketing_process.save_yml(\"best_bucketing.yml\")\n</code></pre> <pre><code>bucketing_process.fit(X_train, y_train)\n</code></pre> <pre>\n<code>BucketingProcess(bucketing_pipeline=Pipeline(steps=[('optimalbucketer-1',\n                                                     OptimalBucketer(max_n_bins=6,\n                                                                     variables=['x1',\n                                                                                'x5',\n                                                                                'x6',\n                                                                                'x7',\n                                                                                'x8',\n                                                                                'x9',\n                                                                                'x10',\n                                                                                'x11',\n                                                                                'x12',\n                                                                                'x13',\n                                                                                'x14',\n                                                                                'x15',\n                                                                                'x16',\n                                                                                'x17',\n                                                                                'x18',\n                                                                                'x19',\n                                                                                'x20',\n                                                                                'x21',\n                                                                                'x22',\n                                                                                'x23'])),\n                                                    ('optimalbucketer-2',\n                                                     OptimalBucketer(variables=['x2',\n                                                                                'x3',\n                                                                                'x4'],\n                                                                     variables_type='categorical'))]),\n                 prebucketing_pipeline=P...steps=[('decisiontreebucketer',\n                                                        DecisionTreeBucketer(max_n_bins=40,\n                                                                             min_bin_size=0.03,\n                                                                             variables=['x1',\n                                                                                        'x5',\n                                                                                        'x6',\n                                                                                        'x7',\n                                                                                        'x8',\n                                                                                        'x9',\n                                                                                        'x10',\n                                                                                        'x11',\n                                                                                        'x12',\n                                                                                        'x13',\n                                                                                        'x14',\n                                                                                        'x15',\n                                                                                        'x16',\n                                                                                        'x17',\n                                                                                        'x18',\n                                                                                        'x19',\n                                                                                        'x20',\n                                                                                        'x21',\n                                                                                        'x22',\n                                                                                        'x23'])),\n                                                       ('ordinalcategoricalbucketer',\n                                                        OrdinalCategoricalBucketer(tol=0.02,\n                                                                                   variables=['x2',\n                                                                                              'x3',\n                                                                                              'x4']))]),\n                 specials={'x1': {'special_demo': [50000]}})</code>\n</pre>"},{"location":"tutorials/1_bucketing/#bucketing","title":"Bucketing","text":"<p>The core of a good skorecard model is to bucket the features.</p> <p>This section showcases how to use skorecard to achieve this.</p> <p>Let's start by loading the demo data</p>"},{"location":"tutorials/1_bucketing/#define-the-numerical-and-categorical-features","title":"Define the numerical and categorical features","text":"<p>The features <code>x2</code>, <code>x3</code> and <code>x4</code> are of a categorical nature</p>"},{"location":"tutorials/1_bucketing/#bucketing-features","title":"Bucketing features","text":"<p>Skorecard implements different bucketers, but they are not applicable to all the features.</p> <ul> <li> <p>Categorical features:  In most of the cases, there is no numerical relationship between categories. Therefore automatic bucketing is very difficult to perform.  <code>skorecard</code> implements the <code>OrdinalCategoricalBucketer</code> that orders the buckets either by the count or by the target rate.  It includes a tolerance (<code>tol</code>) input, which represents the lower bound of the fraction of data allowed to keep a category in the same bucket.</p> </li> <li> <p>Numerical features:  differently from categorical features, algorithmic bucketing can be applied to numerical features. <code>skorecard</code> implements different bucketers that can be used for numerical features:</p> <ul> <li><code>DecisionTreeBucketer</code>: fits a univariate decision tree to find the optimal splits (requires the target <code>y</code>)</li> <li><code>EqualWidthBucketer</code>: generates buckets with equal spacing in the bucket boundaries (i.e. historgrams)</li> <li><code>EqualFrequencyBucketer</code>: generates buckets with equal counts in the buckets (i.e. quantiles)</li> <li><code>AgglomerativeClusteringBucketer</code>: generates bucketes by applying AgglomerativeClustering (density-based bucketer)</li> </ul> </li> </ul>"},{"location":"tutorials/1_bucketing/#manual-correction","title":"Manual correction","text":"<p>Note: to build a high quality scorecard, it's highly recommended to manually assess every bucket. Algorithms implemented in <code>skorecard</code> are very helpful, but are obscure to the business sense. This is especially true for categorical features, where business sense should prevail.</p>"},{"location":"tutorials/1_bucketing/#default-bucketers","title":"Default bucketers","text":""},{"location":"tutorials/1_bucketing/#categorical-features","title":"Categorical features","text":"<p>Due to (generally speaking) no relationship between categories, it's not possible to implement an algorithmic way of bucketing the values (in the same way as it is possible for numerical features).  The only suitable bucketer for categorical features in <code>skorecard</code> is the <code>OrdinalCategoricalBucketer</code>, which groups together low-frequency categorical variables (all variables with a frequency below the <code>tol</code> threshold are put in the <code>other</code> bucket)</p> <p>Let's fix the tol to <code>5%</code>, as this is the recommended minimum.</p>"},{"location":"tutorials/1_bucketing/#numerical-features","title":"Numerical features","text":"<p>Numerical features allow for different bucketers (as described above).</p> <p>However, the recommended approach for bucketing is to use either the <code>DecisionTreeBucketer</code> or the <code>BucketingProcess</code>.</p>"},{"location":"tutorials/1_bucketing/#see-the-bucket-outputs-for-the-first-three-features","title":"See the bucket outputs (for the first three features)","text":""},{"location":"tutorials/1_bucketing/#with-different-bucketers-for-different-features-in-one-go","title":"With different bucketers for different features in one go","text":"<p>Note that below a warning is given to alert you that there are too many unique values in the numerical features. It is good to pay attention to these warnings, as the quantiles are approximate.</p>"},{"location":"tutorials/1_bucketing/#parenthesis-compare-the-buckets-from-two-different-algorithms","title":"Parenthesis: compare the buckets from two different algorithms","text":"<p>By comparing the <code>DecisionTreeBucketer</code> in the first example, and the <code>EqualFrequencyBucketer</code> from the pipeline example, here comes a quick preview on assessing the two bucketing results.</p> <p>The first case results in the higher IV, with less buckets, hence it's definetely a better result!</p>"},{"location":"tutorials/1_bucketing/#make-a-pipeline-for-all-the-features","title":"Make a pipeline for all the features","text":"<p>So far we have shown how to deal with bucketers for categoricals/numericals.</p> <p>The whole process can be put together as in a scikit-learn pipeline.</p>"},{"location":"tutorials/1_bucketing/#save-the-bucketers-to-file","title":"Save the bucketers to file","text":"<p>Once the buckets are satisfactory, save the ouputs to a yaml file</p>"},{"location":"tutorials/1_bucketing/#using-the-bucketing-process","title":"Using the bucketing process","text":"<p>The most common approach in bucketing is to perform what is known as <code>fine-coarse classing</code>.</p> <p>In less fancy words: - you start with very loose bucketing requirements (many buckets, where some minimal (hopefully significant) aggregations and statistics can be computed - this is known as <code>fine classing</code> - for numerical features, it starts by merging together adjacent buckets with similar default rate/WoE  - for categorical features, one should merge together categories with similar default rate/WoE, but only when it makes sense - the last two steps (or merging together buckets) is known as <code>coarse classing</code></p> <p>In skorecard, this process is known as <code>Bucketing Process</code>, as shown below:</p> <ol> <li> <p>The bucketing process starts by defining the loose (fine) buckets (prebucketing pipeline)</p> </li> <li> <p>It then runs an optimization algorithm, that merges the buckets together according to an optimization algorithm (bucketing pipeline)</p> </li> </ol>"},{"location":"tutorials/1_bucketing/#lets-see-the-output-of-this-optimization-step","title":"Let's see the output of this optimization step","text":""},{"location":"tutorials/1_bucketing/#manual-bucket-refinement","title":"Manual bucket refinement","text":"<p>Besides manually inspecting, it's often necessary to manually refine the buckets. <code>skorecard</code> implements a handy dash web-app that allows the user to redefine the bucket allocation.</p>"},{"location":"tutorials/1_bucketing/#up-next","title":"Up next","text":"<p>How to perform feature selection in a <code>skorecard</code> model</p>"},{"location":"tutorials/2_feature_selection/","title":"Selecting features","text":"<pre><code>import pandas as pd\nfrom skorecard.datasets import load_credit_card\nfrom sklearn.model_selection import train_test_split\n\ndata = load_credit_card(as_frame=True)\n\nX_train, X_test, y_train, y_test = train_test_split(\n    data.drop([\"y\"], axis=1), data[\"y\"], test_size=0.25, random_state=42\n)\n</code></pre> <pre><code>import yaml\n\nbuckets_dict = yaml.safe_load(open(\"buckets.yml\"))\n</code></pre> <p>Define the bucketer, using the <code>UserInputBucketer</code></p> <pre><code>from skorecard.bucketers import UserInputBucketer\n\nuib = UserInputBucketer(buckets_dict)\n</code></pre> <pre><code>X_train_bins = uib.fit_transform(X_train, y_train)\nX_test_bins = uib.transform(X_test)\n</code></pre> <pre><code>X_train_bins\n</code></pre> x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 ... x14 x15 x16 x17 x18 x19 x20 x21 x22 x23 21177 2 0 0 0 1 1 0 0 0 0 ... 0 0 3 3 1 1 1 2 2 2 23942 0 0 2 0 0 1 0 0 0 0 ... 0 0 2 2 1 1 1 3 2 1 1247 1 1 2 0 1 0 0 0 0 0 ... 0 0 0 0 1 0 0 0 0 0 23622 2 1 2 1 2 2 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 28454 0 1 2 0 2 1 0 0 0 0 ... 0 0 2 2 1 1 1 1 1 1 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 29802 -3 0 2 0 1 1 0 0 0 0 ... 0 0 2 1 1 1 1 1 1 3 5390 2 0 1 0 2 3 1 1 1 1 ... 0 0 3 3 2 2 0 3 2 2 860 -3 0 1 0 1 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 15795 0 1 2 0 0 1 0 0 0 1 ... 0 0 2 2 1 1 1 1 0 1 23654 2 1 2 1 2 0 0 0 0 0 ... 0 0 1 2 0 1 1 2 3 1 <p>22500 rows \u00d7 23 columns</p> <pre><code>uib.bucket_table(\"x1\")\n</code></pre> bucket label Count Count (%) Non-event Event Event Rate WoE IV 0 -3 Special: special_demo 2510.0 11.16 1848.0 662.0 0.263745 -0.224 0.006 1 -1 Missing 0.0 0.00 0.0 0.0 NaN 0.000 0.000 2 0 [-inf, 75000.0) 4399.0 19.55 2880.0 1519.0 0.345306 -0.611 0.085 3 1 [75000.0, 145000.0) 4567.0 20.30 3494.0 1073.0 0.234946 -0.070 0.001 4 2 [145000.0, 375000.0) 9230.0 41.02 7685.0 1545.0 0.167389 0.354 0.046 5 3 [375000.0, inf) 1794.0 7.97 1584.0 210.0 0.117057 0.770 0.037 <pre><code>from skorecard.reporting import iv\n\niv_dict = iv(X_train_bins, y_train)\n\niv_values = pd.Series(iv_dict).sort_values(ascending=False)  # sort them by predicting power\niv_values.head(5)\n</code></pre> <pre>\n<code>x19    0.003325\nx3     0.002968\nx13    0.002634\nx18    0.002503\nx1     0.002457\ndtype: float64</code>\n</pre> <p>As an abritrary threshold, we can select the features where the IV values are above 0.002</p> <pre><code>preselected_features = iv_values[iv_values &amp;gt; 0.002].index.tolist()\nprint(f\"Total selected features by IV: {len(preselected_features)}\")\n</code></pre> <pre>\n<code>Total selected features by IV: 15\n</code>\n</pre> <pre><code>from skorecard.reporting import psi\n\npsi_dict = psi(X_train_bins, X_test_bins)\n\npsi_values = pd.Series(psi_dict)\npsi_values.sort_values(ascending=False).head(5)\n</code></pre> <pre>\n<code>x6     0.000996\nx2     0.000702\nx12    0.000697\nx19    0.000443\nx21    0.000357\ndtype: float64</code>\n</pre> <p>In this particular case, all the features have a very low PSI, hence no instability is present and no feature selection is performed.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\n\n\ndef plot_correlations(corr):\n    plt.figure(figsize=(10, 8), constrained_layout=True)\n\n    cmap = plt.cm.get_cmap(\"RdBu\")\n    matrix = np.triu(corr)\n    sns.heatmap(corr, vmin=-1, vmax=1, annot=True, mask=matrix, cmap=cmap, annot_kws={\"fontsize\": 6})\n</code></pre> <pre><code>from skorecard.preprocessing import WoeEncoder\nfrom sklearn.pipeline import make_pipeline\n\nwoe_pipe = make_pipeline(uib, WoeEncoder())\n</code></pre> <pre><code>X_train_woe = woe_pipe.fit_transform(X_train, y_train)\n</code></pre> <pre><code>preselected_features\n</code></pre> <pre>\n<code>['x19',\n 'x3',\n 'x13',\n 'x18',\n 'x1',\n 'x6',\n 'x21',\n 'x12',\n 'x16',\n 'x23',\n 'x22',\n 'x7',\n 'x17',\n 'x11',\n 'x5']</code>\n</pre> <pre><code>X_train_corr = X_train_woe[preselected_features].corr()\nplot_correlations(X_train_corr)\n</code></pre> <p>As a rule of thumb, correlations above 0.6 can be considered problematic for the logisitc regression model (this threshold might depend heeavily on dataset and use case).</p> <p>The following code snippet illustrates a recursive feature elimination step, where features are sorted by their IV importance, and correlated features with lower IV importance are removed</p> <pre><code>corr_limit = 0.6  # correlation threshold\n\ndrop_feats = list()\n\n# keep_feats = list()\n\nfor ix, feature in enumerate(preselected_features):\n    if feature in drop_feats:\n        continue\n\n    remaining_features = [\n        feat\n        for feat in preselected_features[ix:]  # check the next feature in the preselected step\n        if feat not in drop_feats and feat != feature\n    ]\n    if len(remaining_features) == 0:\n        continue  # go to the next step if the features at step x have already been removeed\n\n    # find the correlated features with the remaining preselected features\n    # both positive and negative correlations matter, hence the abs()\n    corr_feats = X_train_corr.loc[remaining_features, feature].apply(lambda x: abs(x))\n\n    drop_at_step = corr_feats[corr_feats &amp;gt; corr_limit].index.tolist()\n\n    # append the new features to the list\n    drop_feats += drop_at_step\n\n# Select thee features with low correlations\ngood_feats = [feat for feat in preselected_features if feat not in drop_feats]\n\nprint(f\"Total preselected features: {len(preselected_features)}\")\nprint(f\"Total features dropped due too high correlations: {len(drop_feats)}\")\nprint(f\"Total selected features: {len(good_feats)}\")\n</code></pre> <pre>\n<code>Total preselected features: 15\nTotal features dropped due too high correlations: 2\nTotal selected features: 13\n</code>\n</pre> <p>Visualizing the correlation of the good features to verify that the RFE step worked</p> <pre><code>plot_correlations(X_train_woe[good_feats].corr())\n</code></pre> <p>and the final list of selected features is shown below</p> <pre><code>good_feats\n</code></pre> <pre>\n<code>['x19',\n 'x3',\n 'x13',\n 'x18',\n 'x1',\n 'x6',\n 'x21',\n 'x16',\n 'x23',\n 'x22',\n 'x17',\n 'x11',\n 'x5']</code>\n</pre>"},{"location":"tutorials/2_feature_selection/#once-the-buckets-are-defined-the-next-step-is-to-perform-the-feature-selection","title":"Once the buckets are defined, the next step is to perform the feature selection.","text":"<p>In building a skorecard model, there are a few recommended steps to felect the features.</p> <ul> <li>Calculate the information values (IV) to identify the very predictive features</li> <li>Calculate the Population Stability Index (PSI) to identify the unstable features</li> <li>Evaluate the multicollinearity of the features that pass the previous two steps</li> </ul>"},{"location":"tutorials/2_feature_selection/#the-information-value-iv","title":"The information value IV","text":"<p>The information value is defined by the following equation</p> \\[IV = \\sum_{i}(\\%n_{i}^{y=0} - \\%n_{i}^{y=1})\\frac{\\%n_{i}^{y=0}}{\\%n_{i}^{y=1}}\\] <p>where \\(i\\) is the bucket index, \\(\\%n_{i}^{y=0}\\) represents the fraction counts of target 0 in the bucket, while \\(\\%n_{i}^{y=1}\\) represents the fraction of the counts of target 1 in the bucket \\(i\\).</p> <p>The IV is a weighted sum of the Weight of Evidences (WoE) of every bin. The higher the value, the larger the separation between the classes: in other words the more predictive the feature is. As a rule of thumb: - IV &lt; 0.02 non-predictive feature - 0.02 &lt; IV &lt; 0.1 predictive feature - IV &gt; 0.1 very predictive feature</p>"},{"location":"tutorials/2_feature_selection/#the-population-stability-index-psi","title":"The Population Stability index (PSI)","text":"<p>The PSI measures the similarity between two samples. The PSI is defined as</p> \\[PSI = \\sum_{i}(\\%n_{i}^{X1} - \\%n_{i}^{X2})\\frac{\\%n_{i}^{X1}}{\\%n_{i}^{X2}}\\] <p>where \\(i\\) is the bucket index, \\(\\%n_{i}^{X1}\\) represents the fraction counts of the feature in the sample X1, while \\(\\%n_{i}^{y=1}\\) represents the fraction counts of the feature in the sample X2 in the bucket \\(i\\). It's the same definition as in the IV. However, here large values indicate a difference between two samples, therefore for the selection we look at the lower values: - IV &lt; 0.02 stable feature - 0.02 &lt; IV &lt; 0.1 unstable, but acceptable, feature - IV &gt; 0.1 unstable feature</p> <p>Let's load the data as in the previous tutorials and split it into train and test.</p>"},{"location":"tutorials/2_feature_selection/#load-the-saved-buckets","title":"Load the saved buckets","text":""},{"location":"tutorials/2_feature_selection/#calculating-the-information-value","title":"Calculating the Information Value","text":"<p>The information value can be calculated by the <code>iv()</code> function in the reporting module.</p>"},{"location":"tutorials/2_feature_selection/#calculate-the-psi","title":"Calculate the PSI","text":"<p>Similar to the IV, by using the <code>psi</code> function in the report module.</p>"},{"location":"tutorials/2_feature_selection/#removing-multi-collinear-features","title":"Removing multi-collinear features","text":"<p>A skorecard model is based on a logistic regression algorithm. Logistic regression suffers from multi-collinearity (i.e. highly correlated features) by design.</p> <p>There are multiple ways of measuring it, such as VIF and correlations </p> <p>Here we are showing the approach with correlations.</p> <p>First, let's build an easy visualization function</p>"},{"location":"tutorials/2_feature_selection/#adding-the-woe-encoder","title":"Adding the WoE Encoder","text":"<p>A default scorecard model is defined by the following steps: - bucketing - encoder (a default one is a WoE encoder) - logistic regression model</p> <p>As the input of the logistic regression model is the dataset transformed to its WoE, first let's build the pipline with the first two steps and transform the dataset</p>"},{"location":"tutorials/2_feature_selection/#up-next","title":"Up next","text":"<p>After performing the feature selection, it's time to build the final <code>Skorecard</code> model.</p>"},{"location":"tutorials/3_skorecard_model/","title":"Scorecard model","text":"<pre><code>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n\nfrom skorecard.datasets import load_credit_card\nfrom sklearn.model_selection import train_test_split\n\ndata = load_credit_card(as_frame=True)\n\n\nX_train, X_test, y_train, y_test = train_test_split(\n    data.drop([\"y\"], axis=1), data[\"y\"], test_size=0.25, random_state=42\n)\n</code></pre> <p>Load the buckets and the selected features that were created in the previous tutorials.</p> <pre><code>import yaml\n\nbuckets_dict = yaml.safe_load(open(\"buckets.yml\"))\nselected_features = [\"x6\", \"x8\", \"x10\", \"x18\", \"x1\", \"x19\", \"x20\", \"x21\", \"x23\", \"x22\", \"x3\", \"x17\", \"x16\"]\n</code></pre> <pre><code>from skorecard import Skorecard\nfrom skorecard.bucketers import UserInputBucketer\n\nscorecard = Skorecard(bucketing=UserInputBucketer(buckets_dict), variables=selected_features, calculate_stats=True)\nscorecard = scorecard.fit(X_train, y_train)\n</code></pre> <p>The <code>get_stats</code> method returns the coefficients with their standard error and p-values</p> <pre><code>scorecard.get_stats()\n</code></pre> Coef. Std.Err z P&gt;|z| const -1.242955 0.018238 -68.152266 0.000000e+00 x6 0.765416 0.020622 37.116970 1.495993e-301 x8 0.276995 0.033657 8.229831 1.874781e-16 x10 0.323640 0.036329 8.908558 5.170055e-19 x18 0.226277 0.050398 4.489820 7.128339e-06 x1 0.394922 0.048176 8.197563 2.453087e-16 x19 0.165650 0.055950 2.960659 3.069817e-03 x20 0.254235 0.062924 4.040349 5.337170e-05 x21 0.097257 0.072504 1.341396 1.797919e-01 x23 0.176958 0.074045 2.389866 1.685452e-02 x22 0.110976 0.076718 1.446541 1.480256e-01 x3 0.443555 0.096009 4.619955 3.838229e-06 x17 0.203349 0.133504 1.523175 1.277148e-01 x16 -0.166103 0.142547 -1.165254 2.439163e-01 <p>Retrieve the model performance like in any sklearn classifier</p> <pre><code>from sklearn.metrics import roc_auc_score, classification_report\n\nproba_train = scorecard.predict_proba(X_train)[:, 1]\nproba_test = scorecard.predict_proba(X_test)[:, 1]\n\nprint(f\"AUC train:{round(roc_auc_score(y_train, proba_train),4)}\")\nprint(f\"AUC test :{round(roc_auc_score(y_test, proba_test),4)}\\n\")\n\nprint(classification_report(y_test, scorecard.predict(X_test)))\n</code></pre> <pre>\n<code>AUC train:0.7714\nAUC test :0.7642\n\n              precision    recall  f1-score   support\n\n           0       0.84      0.95      0.89      5873\n           1       0.66      0.34      0.45      1627\n\n    accuracy                           0.82      7500\n   macro avg       0.75      0.65      0.67      7500\nweighted avg       0.80      0.82      0.80      7500\n\n</code>\n</pre> <pre><code>from IPython.display import display\n\nnew_feats = [feat for feat in selected_features if feat not in [\"x21\", \"x16\", \"x17\", \"x22\"]]\n\nscorecard = Skorecard(UserInputBucketer(buckets_dict), variables=new_feats, calculate_stats=True)\n\nscorecard = scorecard.fit(X_train, y_train)\n\nmodel_stats = scorecard.get_stats()\n\nmodel_stats.index = [\"Const\"] + new_feats\ndisplay(model_stats)\n\nproba_train = scorecard.predict_proba(X_train)[:, 1]\nproba_test = scorecard.predict_proba(X_test)[:, 1]\n\nprint(f\"AUC train:{round(roc_auc_score(y_train, proba_train),4)}\")\nprint(f\"AUC test :{round(roc_auc_score(y_test, proba_test),4)}\")\n</code></pre> Coef. Std.Err z P&gt;|z| Const -1.242598 0.018229 -68.166667 0.000000e+00 x6 0.763946 0.020596 37.092613 3.695794e-301 x8 0.269057 0.033234 8.095809 5.688515e-16 x10 0.339016 0.035156 9.643180 5.253488e-22 x18 0.241832 0.049687 4.867076 1.132612e-06 x1 0.409354 0.046618 8.781019 1.619998e-18 x19 0.191910 0.053865 3.562812 3.669030e-04 x20 0.282166 0.060879 4.634866 3.571691e-06 x23 0.227794 0.069624 3.271788 1.068698e-03 x3 0.441695 0.095994 4.601264 4.199341e-06 <pre>\n<code>AUC train:0.7712\nAUC test :0.7648\n</code>\n</pre> <pre><code>from IPython.display import display\n\nprint(\"Top 5 rows and the transformed buckets\")\ndisplay(scorecard.bucket_transform(X_test)[new_feats].head())\n\nprint(\"\\nTop 5 rows and the transformed WoEs\")\ndisplay(scorecard.woe_transform(X_test)[new_feats].head())\n</code></pre> <pre>\n<code>Top 5 rows and the transformed buckets\n</code>\n</pre> x6 x8 x10 x18 x1 x19 x20 x23 x3 2308 1 0 0 1 0 1 1 1 2 22404 1 0 0 1 2 1 1 2 1 23397 1 0 0 1 0 1 1 2 0 25058 1 0 0 1 1 1 2 2 0 2664 1 0 0 1 -3 1 1 1 2 <pre>\n<code>\nTop 5 rows and the transformed WoEs\n</code>\n</pre> x6 x8 x10 x18 x1 x19 x20 x23 x3 2308 -0.668068 -0.321161 -0.230059 -0.029168 0.610738 0.000397 -0.015444 0.105318 0.102411 22404 -0.668068 -0.321161 -0.230059 -0.029168 -0.353564 0.000397 -0.015444 -0.256887 -0.192612 23397 -0.668068 -0.321161 -0.230059 -0.029168 0.610738 0.000397 -0.015444 -0.256887 0.168106 25058 -0.668068 -0.321161 -0.230059 -0.029168 0.070222 0.000397 -0.405293 -0.256887 0.168106 2664 -0.668068 -0.321161 -0.230059 -0.029168 0.224540 0.000397 -0.015444 0.105318 0.102411 <p>In order to talk of feature importance, we should consider both the coefficients and the IV of the single feature. The importance cab be approximated as the product of the two numbers.</p> <pre><code>from skorecard.reporting import iv\n\nX_train_bins = scorecard.bucket_transform(X_train)\niv_dict = iv(X_train_bins, y_train)\n\niv_values = pd.Series(iv_dict).sort_values(ascending=False)\niv_values.name = \"IV\"\n\nfeat_importance = model_stats[[\"Coef.\"]].join(iv_values)\nfeat_importance[\"importance\"] = -1.0 * feat_importance[\"Coef.\"] * feat_importance[\"IV\"]\nfeat_importance.sort_values(by=\"importance\", ascending=False)\n</code></pre> Coef. IV importance x23 0.227794 0.002257 -0.000514 x8 0.269057 0.001924 -0.000518 x20 0.282166 0.001998 -0.000564 x18 0.241832 0.002503 -0.000605 x19 0.191910 0.003325 -0.000638 x10 0.339016 0.001917 -0.000650 x1 0.409354 0.002457 -0.001006 x3 0.441695 0.002968 -0.001311 x6 0.763946 0.002430 -0.001857 Const -1.242598 NaN NaN <pre><code>from skorecard.rescale import calibrate_to_master_scale\n\nproba_train = pd.Series(proba_train, index=y_train.index).sort_values()  # sorting for visualization purposes\nscores = calibrate_to_master_scale(proba_train, pdo=25, ref_score=400, ref_odds=20)\n</code></pre> <pre><code>fig, (ax1, ax2, ax3) = plt.subplots(3, sharex=True, figsize=(8, 12), gridspec_kw={\"hspace\": 0})\nax1.plot(scores.values, proba_train.values)\n\nax1.set_ylabel(\"Predicted probability\")\nax1.set_title(\"Rescaled scores and probabilities\")\nax1.grid()\n\n\nax2.plot(scores.values, proba_train.apply(lambda x: (1 - x) / x).values)\nax2.set_ylabel(\"Odds\")\nax2.grid()\n\n\nax3.plot(\n    scores.values,\n    proba_train.apply(lambda x: np.log(1 - x) - np.log(x)).values,\n)\nax3.set_ylabel(\"log-odds\")\nax3.grid()\nax3.set_xlabel(\"Rescaled scores\")\n\nplt.show()\n</code></pre> <pre><code>from skorecard.rescale import ScoreCardPoints\n\n# ensure that pdo, ref_score and ref_odds are consistent\nscp = ScoreCardPoints(skorecard_model=scorecard, pdo=25, ref_score=400, ref_odds=20)\n</code></pre> <p>one can extract the final scorecard as follows</p> <pre><code>scp.get_scorecard_points()\n</code></pre> bin_index map woe feature coef contribution Points 0 -2 NaN 0.000000 x6 0.763946 0.000000 37 1 -1 Missing 0.000000 x6 0.763946 0.000000 37 3 1 [-0.5, 0.5) -0.668068 x6 0.763946 -0.510368 56 4 2 [0.5, 1.5) -0.419221 x6 0.763946 -0.320262 49 5 3 [1.5, inf) 0.586085 x6 0.763946 0.447737 21 6 4 NaN 2.095463 x6 0.763946 1.600820 -20 7 -2 NaN 0.000000 x8 0.269057 0.000000 37 8 -1 Missing 0.000000 x8 0.269057 0.000000 37 10 1 [1.5, inf) -0.321161 x8 0.269057 -0.086411 41 11 2 NaN 1.392671 x8 0.269057 0.374708 24 12 -2 NaN 0.000000 x10 0.339016 0.000000 37 13 -1 Missing 0.000000 x10 0.339016 0.000000 37 15 1 [1.0, inf) -0.230059 x10 0.339016 -0.077994 40 16 2 NaN 1.491617 x10 0.339016 0.505681 19 17 -2 NaN 0.000000 x18 0.241832 0.000000 37 18 -1 Missing 0.000000 x18 0.241832 0.000000 37 20 1 [21.0, 4552.5) -0.029168 x18 0.241832 -0.007054 38 21 2 [4552.5, 15001.5) 0.692001 x18 0.241832 0.167348 31 22 3 [15001.5, inf) -0.457263 x18 0.241832 -0.110581 41 23 4 NaN -0.860845 x18 0.241832 -0.208180 45 25 -2 NaN 0.000000 x1 0.409354 0.000000 37 26 -1 Missing 0.000000 x1 0.409354 0.000000 37 28 1 [75000.0, 145000.0) -0.353564 x1 0.409354 -0.144733 43 29 2 [145000.0, 375000.0) 0.610738 x1 0.409354 0.250008 28 30 3 [375000.0, inf) 0.070222 x1 0.409354 0.028746 36 31 4 NaN -0.766316 x1 0.409354 -0.313695 49 32 5 NaN 0.224540 x1 0.409354 0.091916 34 33 -2 NaN 0.000000 x19 0.191910 0.000000 37 34 -1 Missing 0.000000 x19 0.191910 0.000000 37 36 1 [131.5, 4970.5) 0.000397 x19 0.191910 0.000076 37 37 2 [4970.5, 15001.0) 0.576300 x19 0.191910 0.110598 33 38 3 [15001.0, inf) -0.403868 x19 0.191910 -0.077506 40 39 4 NaN -1.162523 x19 0.191910 -0.223100 45 40 -2 NaN 0.000000 x20 0.282166 0.000000 37 41 -1 Missing 0.000000 x20 0.282166 0.000000 37 43 1 [16.5, 4513.5) -0.015444 x20 0.282166 -0.004358 38 44 2 [4513.5, 12490.5) 0.509437 x20 0.282166 0.143746 32 45 3 [12490.5, inf) -0.405293 x20 0.282166 -0.114360 42 46 4 NaN -0.832020 x20 0.282166 -0.234768 46 47 -2 NaN 0.000000 x23 0.227794 0.000000 37 48 -1 Missing 0.000000 x23 0.227794 0.000000 37 50 1 [1.5, 2000.5) -0.256887 x23 0.227794 -0.058517 40 51 2 [2000.5, 9849.5) 0.105318 x23 0.227794 0.023991 37 52 3 [9849.5, inf) 0.350867 x23 0.227794 0.079925 35 53 4 NaN -0.701982 x23 0.227794 -0.159907 43 54 -2 Other 0.000000 x3 0.441695 0.000000 37 55 -1 Missing 0.000000 x3 0.441695 0.000000 37 57 1 1.0 0.168106 x3 0.441695 0.074252 35 58 2 2.0 0.102411 x3 0.441695 0.045235 36 59 3 NaN -0.192612 x3 0.441695 -0.085076 40 60 4 NaN -1.207590 x3 0.441695 -0.533387 57 61 0 0 0.000000 Intercept -1.242598 -0.000000 0 <p>Or one can apply the transformation directly on the data, by calling the <code>transform</code> method, in order to map each feature to its actual points.</p> <pre><code>proba_train = pd.Series(\n    proba_train, index=y_train.index\n)  # convert to pandas and correct index in order to be able to perform the diff\nscores = calibrate_to_master_scale(proba_train, pdo=25, ref_score=400, ref_odds=20)\n\n# Check the distribution of the differences\n(scores - scp.transform(X_train).sum(axis=1)).value_counts()\n</code></pre> <pre>\n<code> 111.0    878\n 105.0    708\n 146.0    560\n 125.0    516\n 130.0    442\n         ... \n 284.0      1\n 292.0      1\n-54.0       1\n 261.0      1\n-80.0       1\nLength: 373, dtype: int64</code>\n</pre>"},{"location":"tutorials/3_skorecard_model/#building-a-scorecard-model","title":"Building a scorecard model","text":"<p>This tutorial shows how to build a <code>skorecard</code> model.</p> <p>Start by loading the data and performiing the train test split:</p>"},{"location":"tutorials/3_skorecard_model/#define-the-scorecard-model","title":"Define the scorecard model","text":"<p>A Skorecard class has two main components: - the bucketer - the list of selected features (if <code>None</code> is passed, it uses all the features defined in the bucketer)</p> <p>It behaves like a scikit-learn model</p>"},{"location":"tutorials/3_skorecard_model/#removing-features-based-on-their-statistical-properties","title":"Removing features based on their statistical properties","text":"<p>Features can be further removed. In a scorecard model, the coefficients are expected to be between 0 and -1. Coefficients smaller than -1 indicate that the model relies heavily on features (likely to overfit), while positive coefficients show an inverted trend.</p> <p>Additionally, p-values of the coefficients should be smaller that 0.05. (or 0.01).</p> <p>Looking at the stats table above, this would suggest removing the following features from the list <code>['x21','x16','x17','x22']</code>.</p> <p>Note that feature removal should be done carefully, as every time the feature is removed, the coefficients might converge elsewhere, and would hence give a different model with a different interpretation.</p>"},{"location":"tutorials/3_skorecard_model/#retrieving-the-transformed-data","title":"Retrieving the transformed data","text":"<p>Buckets and WoE transformations are available directly in a fitted <code>skorecard</code> model</p>"},{"location":"tutorials/3_skorecard_model/#getting-the-feature-importance-to-be-integrated-in-the-skorecard-class","title":"Getting the feature importance (to be integrated in the skorecard class)","text":""},{"location":"tutorials/3_skorecard_model/#scaling-the-scores","title":"Scaling the scores","text":"<p>The last step of building skorecard models is the rescaling of the predictions. This is a very common practice within the Credit Risk domain, where scorecard models are widely used.</p> <p>Rescaling scorecards  has no impact on the model performance, but rather returns the predictions on an arbitrary scale (normally from 0-1000) which are more meaningful for risk managers and underwriters in a bank than probabilities.</p> <p>The rescaling is a linear transfromation performed on the log-odds of the predicted probability \\(p\\),</p> \\[ log(\\frac{1-p}{p}) \\] <p>Where the odds are defined as:</p> \\[ \\frac{1-p}{p} \\] <p>The reference for the linear transformation are commonly defined by the following values:</p> <ul> <li><code>ref_score</code>: reference score, that should match a given reference odds (ref_odds)</li> <li><code>ref_odds</code>: reference odds that should match a giver reference score</li> <li><code>pdo</code>: points to double the odds, number of points to add where the odds double.</li> </ul> <p>An example: with the following settings:</p> <ul> <li><code>ref_score = 400</code> </li> <li><code>ref_odds = 20</code> </li> <li><code>pdo = 25</code> </li> </ul> <p>A score of <code>400</code> corresponds to the odds <code>20:1</code> of being a \"good client\" (<code>y=0</code>). This means that the predicted probability for <code>y=1</code> is in this case <code>~4.76%</code>, which you can get by rearranging the equation for the odds, above. When the score increases to <code>425</code>, the odds double to <code>40:1</code> (predicted probability to be <code>y=1</code> is <code>~2,43%</code>). When the score decreases to <code>375</code>, the odds are reduced by a factor 2, ie, <code>10:1</code> (predicted probability to be <code>y=1</code> is <code>~9,09%</code>).</p> <p>In <code>skorecard</code>, one can use the <code>calibrate_to_master_scale</code> function.</p>"},{"location":"tutorials/3_skorecard_model/#visualize-the-score-dependencies","title":"Visualize the score dependencies","text":""},{"location":"tutorials/3_skorecard_model/#assigning-points-to-every-feature","title":"Assigning points to every feature","text":"<p>The last step of a scorecard development is to convert all the features into the rescaled model.</p> <p>A scorecard model is a logisitic regression fitted on the WoE values of every single bucketed feature. In other words, the following equations holds:</p> \\[ log(odds) = log(\\frac{1-p}{p}) = \\beta_{0} + \\sum_{i} \\beta_{i} \\cdot WOE(X_{i}) \\] <p>As the rescaling performed earlier is linear in the predicted <code>log-odds</code>, this means that the every feature-bucket contribution can be rescaled to an integer value (by rescaling directly the $ \\beta_{i} \\cdot WOE(X_{i})$ factors with the same calculations. </p> <p>This returns the final scorecard, that can be easily implemented.</p> <p>The functionality in <code>skorecard</code> to rescale the features is as follows</p>"},{"location":"tutorials/3_skorecard_model/#validate-the-rescaling","title":"Validate the rescaling","text":"<p>As the last step, in order to ensure that the rescaling was successfull, one can verify that the sum of the points of each row in the dataset matches the rescaled scores. The rescaling steps has some integer rounding, therefore small discrepancies of 1-2 points might occur due to the rounding error</p>"},{"location":"tutorials/categoricals/","title":"Categoricals","text":"<pre><code>from skorecard.bucketers import OptimalBucketer\nimport random\nfrom skorecard.datasets import load_uci_credit_card\n\nX, y = load_uci_credit_card(return_X_y=True)\n\n# Add a categorical feature\npets = [\"no pets\"] * 3000 + [\"cat lover\"] * 1500 + [\"dog lover\"] * 1000 + [\"rabbit\"] * 498 + [\"gold fish\"] * 2\nrandom.Random(42).shuffle(pets)\nX[\"pet_ownership\"] = pets\n\nbucketer = OptimalBucketer(max_n_bins=3, variables=[\"pet_ownership\"], variables_type=\"categorical\", cat_cutoff=None)\nbucketer.fit_transform(X, y)[\"pet_ownership\"].value_counts().sort_index()\n</code></pre> <pre>\n<code>0    1998\n1    3000\n2    1002\nName: pet_ownership, dtype: int64</code>\n</pre>"},{"location":"tutorials/categoricals/#categoricals","title":"Categoricals","text":"<p><code>skorecard</code> also has bucketers that support categorical features (such as OptimalBucketer and OrdinalCategoricalBucketer). If you have a categorical feature, you can bucket them directly:</p>"},{"location":"tutorials/interactive_bucketing/","title":"Interactive","text":"<pre><code>from skorecard.datasets import load_uci_credit_card\nfrom skorecard.bucketers import DecisionTreeBucketer\n\nX, y = load_uci_credit_card(return_X_y=True)\nbucketer = DecisionTreeBucketer(max_n_bins=10)\n# bucketer.fit_interactive(X, y) # not run\n</code></pre> <p>This should look like:</p> <p></p> <p>This also works for categorical features:</p> <pre><code>from skorecard.bucketers import OrdinalCategoricalBucketer\nimport random\n\npets = [\"no pets\"] * 3000 + [\"cat lover\"] * 1500 + [\"dog lover\"] * 1000 + [\"rabbit\"] * 498 + [\"gold fish\"] * 2\nrandom.Random(42).shuffle(pets)\nX[\"pet_ownership\"] = pets\n\nbucketer = OrdinalCategoricalBucketer(variables=[\"pet_ownership\"])\n# bucketer.fit_interactive(X, y) # not run\n</code></pre> <pre>\n<code>Dash app running on http://127.0.0.1:8050/\n</code>\n</pre> <p>Which should look like:</p> <p></p> <pre><code>from skorecard.bucketers import OrdinalCategoricalBucketer\nfrom skorecard.pipeline import to_skorecard_pipeline\nfrom sklearn.pipeline import make_pipeline\n\npipe = make_pipeline(\n    OrdinalCategoricalBucketer(variables=[\"EDUCATION\", \"MARRIAGE\"]),\n    DecisionTreeBucketer(max_n_bins=10, variables=[\"LIMIT_BAL\", \"BILL_AMT1\"]),\n)\n\n# Make this a skorecard pipeline, which adds some convenience methods\npipe = to_skorecard_pipeline(pipe)\n\n# pipe.fit_interactive(X, y) # not run\n</code></pre> <pre>\n<code>Dash app running on http://127.0.0.1:8050/\n</code>\n</pre> <pre><code>from skorecard import Skorecard\nfrom skorecard.datasets import load_uci_credit_card\n\nmodel = Skorecard(variables=[\"EDUCATION\", \"MARRIAGE\", \"LIMIT_BAL\", \"BILL_AMT1\"])\n\n# model.fit_interactive(X, y) # not run\n</code></pre> <pre>\n<code>Dash app running on http://127.0.0.1:8050/\n</code>\n</pre>"},{"location":"tutorials/interactive_bucketing/#interactive-bucketing","title":"Interactive bucketing","text":"<p>You might want to manually edit the bucketing boundaries, for example to incorporate specific domain knowledge. You can manually define buckets, but you could also use to interactive explore and update the buckets. All <code>skorecard.bucketers</code> have a method called <code>.fit_interactive()</code>, which will call <code>.fit()</code> if the bucketer is not yet fitted, and then launch a dash webapp.</p> <p>Make sure to have the up to date <code>dash</code> dependencies by running <code>pip install --upgrade skorecard[dashboard]</code>.</p>"},{"location":"tutorials/interactive_bucketing/#pipelines","title":"Pipelines","text":"<p>You can also run <code>.fit_interactive()</code> on a pipeline of bucketers. You'll need to convert to a <code>SkorecardPipeline</code> in order to have access to the method:</p>"},{"location":"tutorials/interactive_bucketing/#bucketingprocess-and-skorecard-models","title":"BucketingProcess and Skorecard models","text":"<p>Interactively setting pre-bucketing and bucketing per column is also possible on <code>BucketingProcess</code> and <code>Skorecard</code> models </p>"},{"location":"tutorials/methods/","title":"Methods","text":"<pre><code>from skorecard.datasets import load_uci_credit_card\nfrom skorecard.bucketers import DecisionTreeBucketer\n\nX, y = load_uci_credit_card(return_X_y=True)\n\nspecials = {\"LIMIT_BAL\": {\"=50000\": [50000], \"in [20000,30000]\": [20000, 30000]}}\n\ndt_bucketer = DecisionTreeBucketer(variables=[\"LIMIT_BAL\"], specials=specials)\ndt_bucketer.fit(X, y)\n\ndt_bucketer.fit_transform(X, y).head()\n</code></pre> EDUCATION MARRIAGE LIMIT_BAL BILL_AMT1 0 1 2 9 201800.0 1 2 2 1 80610.0 2 1 2 9 499452.0 3 1 1 3 450.0 4 2 1 9 56107.0 <pre><code>dt_bucketer.summary()\n</code></pre> column num_prebuckets num_buckets IV_score dtype 0 EDUCATION not_prebucketed not_bucketed 0.057606 int64 1 MARRIAGE not_prebucketed not_bucketed 0.016267 int64 2 LIMIT_BAL not_prebucketed 13 0.178036 float64 3 BILL_AMT1 not_prebucketed not_bucketed 2.915613 float64 <pre><code>dt_bucketer.bucket_table(\"LIMIT_BAL\")\n</code></pre> bucket label Count Count (%) Non-event Event Event Rate WoE IV 0 -4 Special: in [20000,30000] 723.0 12.05 453.0 270.0 0.373444 -0.724 -0.075 1 -3 Special: =50000 676.0 11.27 518.0 158.0 0.233728 -0.054 -0.000 2 -1 Missing 0.0 0.00 0.0 0.0 NaN 0.000 0.000 3 0 [-inf, 75000.0) 462.0 7.70 313.0 149.0 0.322511 -0.499 -0.022 4 1 [75000.0, 85000.0) 319.0 5.32 243.0 76.0 0.238245 -0.079 -0.000 5 2 [85000.0, 105000.0) 330.0 5.50 241.0 89.0 0.269697 -0.245 -0.004 6 3 [105000.0, 145000.0) 566.0 9.43 436.0 130.0 0.229682 -0.031 -0.000 7 4 [145000.0, 175000.0) 449.0 7.48 380.0 69.0 0.153675 0.464 -0.014 8 5 [175000.0, 225000.0) 769.0 12.82 630.0 139.0 0.180754 0.270 -0.009 9 6 [225000.0, 275000.0) 501.0 8.35 419.0 82.0 0.163673 0.390 -0.011 10 7 [275000.0, 325000.0) 379.0 6.32 326.0 53.0 0.139842 0.575 -0.018 11 8 [325000.0, 385000.0) 350.0 5.83 287.0 63.0 0.180000 0.275 -0.004 12 9 [385000.0, inf) 476.0 7.93 409.0 67.0 0.140756 0.567 -0.022 <pre><code>dt_bucketer.plot_bucket(\n    \"LIMIT_BAL\", format=\"png\", scale=2, width=1050, height=525\n)  # remove format argument for an interactive plotly plot.)\n</code></pre> <pre><code>dt_bucketer.save_yml(\"my_output.yml\")\n</code></pre>"},{"location":"tutorials/methods/#methods","title":"Methods","text":"<p>The bucketers of <code>skorecard</code> come with a handy list of methods for you to peek under the hood of the bucketer</p>"},{"location":"tutorials/methods/#summary","title":".summary()","text":"<p>This gives the user a simple table of the columns and number of (pre)buckets generated by the bucketer. The information value and dtypes are also given</p>"},{"location":"tutorials/methods/#bucket_table","title":".bucket_table()","text":"<p>To look at the buckets in a more granular level, the <code>bucket_table()</code> method outputs, among others, a table containing the counts in each bin, the percentages, and the event rate.</p>"},{"location":"tutorials/methods/#plot_bucket","title":".plot_bucket()","text":"<p>We have already seen that we can plot the above bucket table for a better visualisation of the buckets</p>"},{"location":"tutorials/methods/#save_yml","title":".save_yml()","text":"<p>We can save the generated bucket to a yaml file. This yaml file can later be used to generate a bucketer as we show in the <code>create_bucketer_from_file</code> tutorial</p>"},{"location":"tutorials/methods/#bucket-mapping","title":"Bucket mapping","text":"<p>If you're interested into digging into the internals of the buckets, you can access the fitted attribute     <code>features_bucket_mapping_</code>. For example:</p> <pre><code>```python\nbucketer.features_bucket_mapping_.get('pet_ownership').labels\n# {0: 'cat lover, rabbit',\n# 1: 'no pets',\n# 2: 'dog lover',\n# 3: 'gold fish',\n# 4: 'other',\n# 5: 'Missing'}\n```\n</code></pre>"},{"location":"tutorials/missing_values/","title":"Missing Values","text":"<pre><code>import numpy as np\nimport pandas as pd\nfrom skorecard.bucketers import EqualFrequencyBucketer\n\ndf = pd.DataFrame({\"counts\": [1, 2, 2, 1, 4, 2, np.nan, 1, 3]})\nEqualFrequencyBucketer(n_bins=2).fit_transform(df).value_counts()\n</code></pre> <pre>\n<code>counts\n 0        6\n 1        2\n-1        1\ndtype: int64</code>\n</pre> <pre><code>EqualFrequencyBucketer(n_bins=2, missing_treatment={\"counts\": 1}).fit_transform(df).value_counts()\n</code></pre> <pre>\n<code>counts\n0         6\n1         3\ndtype: int64</code>\n</pre> <pre><code>EqualFrequencyBucketer(n_bins=2, missing_treatment=\"passthrough\").fit_transform(df)\n</code></pre> counts 0 0.0 1 0.0 2 0.0 3 0.0 4 1.0 5 0.0 6 NaN 7 0.0 8 1.0 <pre><code>EqualFrequencyBucketer(n_bins=2, missing_treatment=\"most_frequent\").fit_transform(df)\n</code></pre> counts 0 0 1 0 2 0 3 0 4 1 5 0 6 0 7 0 8 1 <pre><code>X = pd.DataFrame({\"counts\": [1, 2, 2, 1, 4, 2, np.nan, 1, 3]})\ny = pd.DataFrame({\"target\": [0, 0, 1, 0, 1, 0, 1, 0, 1]})\nEqualFrequencyBucketer(n_bins=2, missing_treatment=\"neutral\").fit_transform(X, y)\n</code></pre> counts 0 0 1 0 2 0 3 0 4 1 5 0 6 0 7 0 8 1 <pre><code>EqualFrequencyBucketer(n_bins=2, missing_treatment=\"similar\").fit_transform(X, y)\n</code></pre> counts 0 0 1 0 2 0 3 0 4 1 5 0 6 1 7 0 8 1 <pre><code>a = EqualFrequencyBucketer(n_bins=2, missing_treatment=\"least_risky\")  # .fit_transform(X, y)\na.fit_transform(X, y)\n</code></pre> counts 0 0 1 0 2 0 3 0 4 1 5 0 6 0 7 0 8 1 <pre><code>EqualFrequencyBucketer(n_bins=2, missing_treatment=\"least_risky\").fit_transform(X, y)\n</code></pre> counts 0 0 1 0 2 0 3 0 4 1 5 0 6 0 7 0 8 1 <pre><code>EqualFrequencyBucketer(n_bins=2, missing_treatment=\"most_risky\").fit_transform(X, y)\n</code></pre> counts 0 0 1 0 2 0 3 0 4 1 5 0 6 1 7 0 8 1"},{"location":"tutorials/missing_values/#missing-values","title":"Missing Values","text":"<p><code>skorecard</code> bucketers offer native support for missing values and will put them in a separate bucket by default. </p> <p>In the example below, you can see that the single missing value is put into a new bucket '-1'.</p>"},{"location":"tutorials/missing_values/#specific","title":"Specific","text":"<p>Alternatively, the user can give a specific bucket for the missing values.</p> <p>In the example below, you can see we put the missing value into bucket 1</p>"},{"location":"tutorials/missing_values/#passthrough","title":"Passthrough","text":"<p>If the user wishes the missing values to be left untouched, they can specify this with the <code>passthrough</code> argument</p>"},{"location":"tutorials/missing_values/#most-frequent","title":"Most frequent","text":"<p>It's also possible to put the missing values into the most common bucket. Below, we see that the missing values are put into the '0' bucket</p>"},{"location":"tutorials/missing_values/#using-the-target-to-bucket","title":"Using the target to bucket","text":"<p>It's also possible to use the target to decide which bucket to use for the missing values. In the below examples, we use <code>y</code> as the target.</p>"},{"location":"tutorials/missing_values/#neutral","title":"Neutral","text":"<p>Here the missing values are placed into the bucket that has a Weight of Evidence closest to 0</p>"},{"location":"tutorials/missing_values/#similar","title":"Similar","text":"<p>We can also put the missing values into the bucket that has a Weight of Evidence closest to the bucket containing only missing values</p>"},{"location":"tutorials/missing_values/#least-risky","title":"Least risky","text":"<p>Missing values are put into the bucket containing the largest percentage of Class 0.</p>"},{"location":"tutorials/missing_values/#most-risky","title":"Most risky","text":"<p>Missing values are put into the bucket containing the largest percentage of Class 1.</p>"},{"location":"tutorials/reporting/","title":"Reporting","text":"<pre><code>%matplotlib inline\nfrom skorecard.datasets import load_uci_credit_card\n\nX, y = load_uci_credit_card(return_X_y=True)\nX.head(4)\n</code></pre> EDUCATION MARRIAGE LIMIT_BAL BILL_AMT1 0 1 2 400000.0 201800.0 1 2 2 80000.0 80610.0 2 1 2 500000.0 499452.0 3 1 1 140000.0 450.0 <pre><code>from skorecard.bucketers import DecisionTreeBucketer\n\nbucketer = DecisionTreeBucketer(max_n_bins=10)\nX_transformed = bucketer.fit_transform(X, y)\nX_transformed.head(4)\n</code></pre> EDUCATION MARRIAGE LIMIT_BAL BILL_AMT1 0 0 1 9 9 1 1 1 3 7 2 0 1 9 9 3 0 0 5 0 <p>Retrieve the bucket summary table</p> <pre><code>bucketer.bucket_table(column=\"LIMIT_BAL\")\n</code></pre> bucket label Count Count (%) Non-event Event Event Rate WoE IV 0 -1 Missing 0.0 0.00 0.0 0.0 NaN 0.000 0.000 1 0 [-inf, 45000.0) 849.0 14.15 533.0 316.0 0.372203 -0.719 0.087 2 1 [45000.0, 55000.0) 676.0 11.27 518.0 158.0 0.233728 -0.054 0.000 3 2 [55000.0, 75000.0) 336.0 5.60 233.0 103.0 0.306548 -0.425 0.011 4 3 [75000.0, 85000.0) 319.0 5.32 243.0 76.0 0.238245 -0.079 0.000 5 4 [85000.0, 105000.0) 330.0 5.50 241.0 89.0 0.269697 -0.245 0.004 6 5 [105000.0, 145000.0) 566.0 9.43 436.0 130.0 0.229682 -0.031 0.000 7 6 [145000.0, 275000.0) 1719.0 28.65 1429.0 290.0 0.168703 0.353 0.032 8 7 [275000.0, 325000.0) 379.0 6.32 326.0 53.0 0.139842 0.575 0.018 9 8 [325000.0, 385000.0) 350.0 5.83 287.0 63.0 0.180000 0.275 0.004 10 9 [385000.0, inf) 476.0 7.93 409.0 67.0 0.140756 0.567 0.022 <p>Plotting the buckets</p> <pre><code>bucketer.plot_bucket(column=\"LIMIT_BAL\", format=\"png\", scale=2, width=1050, height=525)\n</code></pre> <pre><code>from skorecard.datasets import load_uci_credit_card\nfrom skorecard.bucketers import EqualFrequencyBucketer\nfrom skorecard.linear_model import LogisticRegression\nfrom skorecard.reporting import weight_plot\nfrom sklearn.pipeline import Pipeline\n\nX, y = load_uci_credit_card(return_X_y=True)\npipeline = Pipeline(\n    [(\"bucketer\", EqualFrequencyBucketer(n_bins=10)), (\"clf\", LogisticRegression(calculate_stats=True))]\n)\npipeline.fit(X, y)\nstats = pipeline.named_steps[\"clf\"].get_stats()\n\nstats\n</code></pre> <pre>\n<code>/Users/iv58uq/Documents/open_source/skorecard/skorecard/bucketers/bucketers.py:502: ApproximationWarning:\n\nApproximated quantiles - too many unique values\n\n/Users/iv58uq/Documents/open_source/skorecard/skorecard/bucketers/bucketers.py:502: ApproximationWarning:\n\nApproximated quantiles - too many unique values\n\n</code>\n</pre> Coef. Std.Err z P&gt;|z| const -0.537571 0.096108 -5.593394 2.226735e-08 EDUCATION 0.010091 0.044874 0.224876 8.220757e-01 MARRIAGE -0.255608 0.062513 -4.088864 4.334903e-05 LIMIT_BAL -0.136681 0.011587 -11.796145 4.086051e-32 BILL_AMT1 -0.006634 0.011454 -0.579160 5.624809e-01 <pre><code>weight_plot(stats, format=\"png\", scale=2, width=1050, height=525)\n</code></pre> <pre><code>from skorecard import datasets\nfrom skorecard.bucketers import DecisionTreeBucketer, OptimalBucketer, AsIsCategoricalBucketer\nfrom skorecard.pipeline import BucketingProcess\nfrom sklearn.pipeline import make_pipeline\n\ndf = datasets.load_uci_credit_card(as_frame=True)\ny = df[\"default\"]\nX = df.drop(columns=[\"default\"])\n\nnum_cols = [\"LIMIT_BAL\", \"BILL_AMT1\"]\ncat_cols = [\"EDUCATION\", \"MARRIAGE\"]\n\nbucketing_process = BucketingProcess(\n    prebucketing_pipeline=make_pipeline(\n        DecisionTreeBucketer(variables=num_cols, max_n_bins=100, min_bin_size=0.05),\n        AsIsCategoricalBucketer(variables=cat_cols),\n    ),\n    bucketing_pipeline=make_pipeline(\n        OptimalBucketer(variables=num_cols, max_n_bins=10, min_bin_size=0.05),\n        OptimalBucketer(variables=cat_cols, variables_type=\"categorical\", max_n_bins=10, min_bin_size=0.05),\n    ),\n)\n\n_ = bucketing_process.fit(X, y)\n</code></pre> <pre><code>bucketing_process.prebucket_table(\"LIMIT_BAL\")\n</code></pre> pre-bucket label Count Count (%) Non-event Event Event Rate WoE IV bucket 0 -1 Missing 0.0 0.00 0.0 0.0 NaN 0.000 0.000 -1 1 0 [-inf, 25000.0) 479.0 7.98 300.0 179.0 0.373695 -0.725 0.050 0 2 1 [25000.0, 45000.0) 370.0 6.17 233.0 137.0 0.370270 -0.710 0.037 1 3 2 [45000.0, 55000.0) 676.0 11.27 518.0 158.0 0.233728 -0.054 0.000 2 4 3 [55000.0, 75000.0) 336.0 5.60 233.0 103.0 0.306548 -0.425 0.011 2 5 4 [75000.0, 85000.0) 319.0 5.32 243.0 76.0 0.238245 -0.079 0.000 3 6 5 [85000.0, 105000.0) 330.0 5.50 241.0 89.0 0.269697 -0.245 0.004 3 7 6 [105000.0, 145000.0) 566.0 9.43 436.0 130.0 0.229682 -0.031 0.000 4 8 7 [145000.0, 175000.0) 449.0 7.48 380.0 69.0 0.153675 0.464 0.014 5 9 8 [175000.0, 225000.0) 769.0 12.82 630.0 139.0 0.180754 0.270 0.009 5 10 9 [225000.0, 275000.0) 501.0 8.35 419.0 82.0 0.163673 0.390 0.011 6 11 10 [275000.0, 325000.0) 379.0 6.32 326.0 53.0 0.139842 0.575 0.018 7 12 11 [325000.0, 385000.0) 350.0 5.83 287.0 63.0 0.180000 0.275 0.004 7 13 12 [385000.0, inf) 476.0 7.93 409.0 67.0 0.140756 0.567 0.022 8 <p>Visualizing the bucketing</p> <pre><code>bucketing_process.plot_prebucket(\"LIMIT_BAL\", format=\"png\", scale=2, width=1050, height=525)\n</code></pre> <pre><code>bucketing_process.bucket_table(\"LIMIT_BAL\")\n</code></pre> bucket label Count Count (%) Non-event Event Event Rate WoE IV 0 -1 Missing 0.0 0.00 0.0 0.0 NaN 0.000 0.000 1 0 [-inf, 1.0) 479.0 7.98 300.0 179.0 0.373695 -0.725 0.050 2 1 [1.0, 2.0) 370.0 6.17 233.0 137.0 0.370270 -0.710 0.037 3 2 [2.0, 4.0) 1012.0 16.87 751.0 261.0 0.257905 -0.185 0.006 4 3 [4.0, 6.0) 649.0 10.82 484.0 165.0 0.254237 -0.165 0.003 5 4 [6.0, 7.0) 566.0 9.43 436.0 130.0 0.229682 -0.031 0.000 6 5 [7.0, 9.0) 1218.0 20.30 1010.0 208.0 0.170772 0.339 0.021 7 6 [9.0, 10.0) 501.0 8.35 419.0 82.0 0.163673 0.390 0.011 8 7 [10.0, 12.0) 729.0 12.15 613.0 116.0 0.159122 0.423 0.019 9 8 [12.0, inf) 476.0 7.93 409.0 67.0 0.140756 0.567 0.022 <p>and the same applies to plotting the bucketing step</p> <pre><code>bucketing_process.plot_bucket(\"LIMIT_BAL\", format=\"png\", scale=2, width=1050, height=525)\n</code></pre>"},{"location":"tutorials/reporting/#reporting","title":"Reporting","text":"<p>Reporting plays a crucial role in building scorecard models.</p> <p>Skorecard bucketers include a reporting module, and this tutorial shows how to extract it</p>"},{"location":"tutorials/reporting/#reporting-in-bucketers","title":"Reporting in bucketers","text":"<p>Once a bucketer is fitted, the reporting module is incorporated directly in the bucketer object</p>"},{"location":"tutorials/reporting/#statistical-significance","title":"Statistical Significance","text":"<p>You will often want to report and analyse the statistical significance of the coefficients generated by the Logistic Regression Model. We do this by calculating the <code>p-values</code> of the coefficients. Typically, any coefficient with <code>p-value &gt; 0.05</code> is regarded as insignificant, and hence should not be reported as a contributing feature.</p> <p>Below, we show an example of how to get the summary statistics including the <code>p-values</code> using the <code>.get_stats()</code> function. As can be seen from the resulting dataframe, there are 2 features - EDUCATION and BILL_AMT1 - with \"unreliable\" <code>p-values</code>. </p> <p>The coefficients can be further analysed using the <code>weight_plot()</code> function. The 2-sigma confidence interval is plotted. Assuming a Gaussian distribution, 95% of data exists within this spread. The plot corroboartes the <code>p-values</code>: we can see that there is a significant chance the coefficients of EDUCATION and BILL_AMT1 are 0.</p>"},{"location":"tutorials/reporting/#reporting-in-bucketing-process","title":"Reporting in Bucketing Process","text":"<p>The Bucketing Process module incorporates two bucketing steps: - the prebucketing step - bucketing step</p> <p>Let's first fit a bucketing process step</p>"},{"location":"tutorials/reporting/#prebucketing-step","title":"Prebucketing step","text":"<p>Retrieve the bucketing report of the prebucketing step by calling the <code>prebucket_table</code>.</p> <p>In addition to the statstics, the prebucket_table returns also the recommended bucket for the merging.</p>"},{"location":"tutorials/reporting/#bucketing-step","title":"Bucketing step","text":"<p>Retreving the bucketing table from the second step is the same like in every bucketer, ie</p>"},{"location":"tutorials/specials/","title":"Special Values","text":"<pre><code>from skorecard.bucketers import EqualWidthBucketer\nfrom skorecard.datasets import load_uci_credit_card\n\nX, y = load_uci_credit_card(return_X_y=True)\n\nspecials = {\n    \"LIMIT_BAL\": {\"=50000\": [50000], \"in [20001,30000]\": [20000, 30000]},\n    \"EDUCATION\": {\"=High School, Graduate School\": [1, 3]},\n}\n\ncols = [\"LIMIT_BAL\", \"EDUCATION\"]\nX_transformed = EqualWidthBucketer(n_bins=3, specials=specials, variables=cols).fit_transform(X, y)\nX_transformed[\"EDUCATION\"].value_counts()\n</code></pre> <pre>\n<code>-3    3199\n 0    2726\n 2      62\n 1      13\nName: EDUCATION, dtype: int64</code>\n</pre>"},{"location":"tutorials/specials/#special-values","title":"Special values","text":"<p>You might have some features with values that you need to have in a separate bucket. You can define a dictionary with the buckets you want, and pass them to the bucketer. </p> <p>In the example below, the special values for the variable \"EDUCATION\" are put into a separate bucket, -3. Note that this bucket is not included in the <code>n_bins</code> parameter</p>"},{"location":"tutorials/the_basics/","title":"The Basics","text":"<pre><code>from skorecard.datasets import load_uci_credit_card\n\nX, y = load_uci_credit_card(return_X_y=True)\nX.head(4)\n</code></pre> EDUCATION MARRIAGE LIMIT_BAL BILL_AMT1 0 1 2 400000.0 201800.0 1 2 2 80000.0 80610.0 2 1 2 500000.0 499452.0 3 1 1 140000.0 450.0 <pre><code>from skorecard.bucketers import DecisionTreeBucketer\n\nbucketer = DecisionTreeBucketer(max_n_bins=10)\nX_transformed = bucketer.fit_transform(X, y)\nX_transformed.head(4)\n</code></pre> EDUCATION MARRIAGE LIMIT_BAL BILL_AMT1 0 0 1 9 9 1 1 1 3 7 2 0 1 9 9 3 0 0 5 0 <pre><code>X_transformed[\"BILL_AMT1\"].value_counts().sort_index()\n</code></pre> <pre>\n<code>0    1343\n1     404\n2     574\n3     462\n4     400\n5     359\n6     857\n7     789\n8     500\n9     312\nName: BILL_AMT1, dtype: int64</code>\n</pre> <pre><code>bucketer = DecisionTreeBucketer(max_n_bins=10, variables=[\"BILL_AMT1\"])\nbucketer.fit_transform(X, y).head(4)\n</code></pre> EDUCATION MARRIAGE LIMIT_BAL BILL_AMT1 0 1 2 400000.0 9 1 2 2 80000.0 7 2 1 2 500000.0 9 3 1 1 140000.0 0 <pre><code>from skorecard.bucketers import EqualWidthBucketer\n\nbucketer = EqualWidthBucketer(n_bins=5, variables=[\"BILL_AMT1\"])\nbucketer.fit(X, y)\nbucketer.bucket_table(\"BILL_AMT1\")\n</code></pre> bucket label Count Count (%) Non-event Event Event Rate WoE IV 0 -1 Missing 0.0 0.00 0.0 0.0 NaN 0.000 0.000 1 0 (-inf, -10319.399999999994] 3.0 0.05 3.0 0.0 0.000000 4.181 -0.003 2 1 (-10319.399999999994, 144941.2] 5408.0 90.13 4188.0 1220.0 0.225592 -0.008 -0.000 3 2 (144941.2, 300201.80000000005] 490.0 8.17 395.0 95.0 0.193878 0.183 -0.003 4 3 (300201.80000000005, 455462.4] 75.0 1.25 55.0 20.0 0.266667 -0.230 -0.001 5 4 (455462.4, inf] 24.0 0.40 14.0 10.0 0.416667 -0.903 -0.004 <pre><code>bucketer.plot_bucket(\n    \"BILL_AMT1\", format=\"png\", scale=2, width=1050, height=525\n)  # remove format argument for an interactive plotly plot.\n</code></pre>"},{"location":"tutorials/the_basics/#the-basics","title":"The Basics","text":""},{"location":"tutorials/the_basics/#dummy-dataset","title":"Dummy dataset","text":"<p>Let's start first with a dummy dataset based on the UCI credit card dataset.</p>"},{"location":"tutorials/the_basics/#a-basic-bucketer","title":"A basic bucketer","text":"<p><code>skorecard</code> offers a set of bucketers that have a scikit-learn compatible interface. By default they will bucket all variables into <code>n_bins</code> buckets.</p> <p>Some bucketers like OptimalBucketer and DecisionTreeBucketer are supervised and can use information from <code>y</code> to find good buckets. You can control the numbers of buckets using <code>max_n_bins</code> instead of <code>n_bins</code>. </p>"},{"location":"tutorials/the_basics/#bucketing-specific-variables","title":"Bucketing specific variables","text":"<p>Instead of applying a bucketer on all features, you'll likely want to apply it only to specific features. You can use the <code>variables</code> parameter for that:</p>"},{"location":"tutorials/the_basics/#inspecting-bucketing-results","title":"Inspecting bucketing results","text":"<p><code>skorecard</code> bucketers have some methods to help you inspect the result of the bucketing process:</p>"},{"location":"tutorials/using-bucketing-process/","title":"BucketingProcess","text":"<pre><code>from skorecard import datasets\nfrom skorecard.bucketers import DecisionTreeBucketer, OptimalBucketer, AsIsCategoricalBucketer\nfrom skorecard.pipeline import BucketingProcess\n\nfrom sklearn.pipeline import make_pipeline\n\ndf = datasets.load_uci_credit_card(as_frame=True)\ny = df[\"default\"]\nX = df.drop(columns=[\"default\"])\n\nnum_cols = [\"LIMIT_BAL\", \"BILL_AMT1\"]\ncat_cols = [\"EDUCATION\", \"MARRIAGE\"]\nspecials = {\"EDUCATION\": {\"Is 1\": [1]}}\n\nbucketing_process = BucketingProcess(\n    prebucketing_pipeline=make_pipeline(\n        DecisionTreeBucketer(variables=num_cols, max_n_bins=100, min_bin_size=0.05),\n        AsIsCategoricalBucketer(variables=cat_cols),\n    ),\n    bucketing_pipeline=make_pipeline(\n        OptimalBucketer(variables=num_cols, max_n_bins=10, min_bin_size=0.05),\n        OptimalBucketer(variables=cat_cols, variables_type=\"categorical\", max_n_bins=10, min_bin_size=0.05),\n    ),\n    specials=specials,\n)\n\nbucketing_process.fit_transform(X, y).head()\n</code></pre> EDUCATION MARRIAGE LIMIT_BAL BILL_AMT1 0 -3 0 8 5 1 1 0 3 4 2 -3 0 8 5 3 -3 1 4 0 4 1 1 8 3 <pre><code>bucketing_process.summary()\n</code></pre> column num_prebuckets num_buckets IV_score dtype 0 EDUCATION 9 5 0.036308 int64 1 MARRIAGE 6 4 0.013054 int64 2 LIMIT_BAL 14 10 0.168862 float64 3 BILL_AMT1 15 7 0.005823 float64 <pre><code>bucketing_process.prebucket_table(\"MARRIAGE\")\n</code></pre> pre-bucket label Count Count (%) Non-event Event Event Rate WoE IV bucket 0 -2 Other 0.0 0.00 0.0 0.0 NaN 0.000 0.000 -2 1 -1 Missing 0.0 0.00 0.0 0.0 NaN 0.000 0.000 -1 2 0 2 3138.0 52.30 2493.0 645.0 0.205545 0.110 0.006 0 3 1 1 2784.0 46.40 2108.0 676.0 0.242816 -0.104 0.005 1 4 2 3 64.0 1.07 42.0 22.0 0.343750 -0.594 0.004 1 5 3 0 14.0 0.23 12.0 2.0 0.142857 0.547 0.001 0 <pre><code>bucketing_process.bucket_table(\"MARRIAGE\")\n</code></pre> bucket label Count Count (%) Non-event Event Event Rate WoE IV 0 -2 Other 0.0 0.00 0.0 0.0 NaN 0.000 0.000 1 -1 Missing 0.0 0.00 0.0 0.0 NaN 0.000 0.000 2 0 0, 3 3152.0 52.53 2505.0 647.0 0.205266 0.112 0.006 3 1 1, 2 2848.0 47.47 2150.0 698.0 0.245084 -0.117 0.007 <pre><code>bucketing_process.plot_prebucket(\"LIMIT_BAL\", format=\"png\", scale=2, width=1050, height=525)\n</code></pre> <pre><code>bucketing_process.pre_pipeline_.features_bucket_mapping_.get(\"MARRIAGE\").labels\n</code></pre> <pre>\n<code>{3: '0', 1: '1', 0: '2', 2: '3', -1: 'Missing', -2: 'Other'}</code>\n</pre> <pre><code>bucketing_process.pipeline_.features_bucket_mapping_.get(\"EDUCATION\")\n</code></pre> <pre>\n<code>BucketMapping(feature_name='EDUCATION', type='categorical', missing_bucket=None, other_bucket=None, map={1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 0: 1}, right=False, specials={'Is 1': [-3]})</code>\n</pre> <pre><code>bucketing_process.features_bucket_mapping_.get(\"EDUCATION\")\n</code></pre> <pre>\n<code>BucketMapping(feature_name='EDUCATION', type='categorical', missing_bucket=None, other_bucket=None, map={0: 0, 3: 0, 4: 0, 5: 0, 6: 0, 2: 1}, right=True, specials={'Is 1': [1]})</code>\n</pre> <pre><code># bucketing_process.fit_interactive(X, y) # not run\n</code></pre>"},{"location":"tutorials/using-bucketing-process/#using-the-bucketingprocess","title":"Using the BucketingProcess","text":"<p>The <code>BucketingProcess</code> enables a two-step bucketing approach, where a feature is first pre-bucketed to e.g. 100 pre-buckets, and then bucketed. </p> <p>This is a common practice - it reduces the complexity of finding exact boundaries to the problem of finding which of 100 buckets to merge together.</p>"},{"location":"tutorials/using-bucketing-process/#define-the-bucketingprocess","title":"Define the BucketingProcess","text":"<p>The bucketing process incorporates a pre-bucketing pipeline and a bucketing pipeline. You can also pass <code>specials</code> or <code>variables</code> and <code>BucketingProcess</code> will pass those settings on to the bucketers in the pipelines.</p> <p>In the example below, we prebucket numerical features to max 100 bins, and prebucket categorical columns as-is (each unique value is a category and new categories end up in the other bucket).</p>"},{"location":"tutorials/using-bucketing-process/#methods-and-attributes","title":"Methods and Attributes","text":"<p>A <code>BucketingProcess</code> instance has all the similar methods &amp; attributes of a bucketer:</p> <ul> <li><code>.summary()</code></li> <li><code>.bucket_table(column)</code></li> <li><code>.plot_bucket(column)</code></li> <li><code>.features_bucket_mapping</code></li> <li><code>.save_to_yaml()</code></li> <li><code>.fit_interactive()</code></li> </ul> <p>but also adds a few unique ones:</p> <ul> <li><code>.prebucket_table(column)</code></li> <li><code>.plot_prebucket(column)</code></li> </ul>"},{"location":"tutorials/using-bucketing-process/#the-features_bucket_mapping-attribute","title":"The <code>.features_bucket_mapping</code> attribute","text":"<p>All skorecard bucketing classes have a <code>.features_bucket_mapping</code> attribute to access the stored bucketing information to go from an input feature to a bucketed feature. In the case of <code>BucketingProcess</code>, because there is a prebucketing and bucketing step, this means the bucket mapping reflects the net effect of merging both steps into one. This is demonstrated below:</p>"},{"location":"tutorials/using-bucketing-process/#the-fit_interactive-method","title":"The <code>.fit_interactive()</code> method","text":"<p>All skorecard bucketing classes have a <code>.fit_interactive()</code> method. In the case of <code>BucketingProcess</code> this will launch a slightly different app that shows the pre-buckets and the buckets, and allows you to edit the prebucketing as well.</p>"}]}