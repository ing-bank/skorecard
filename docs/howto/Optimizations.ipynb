{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "faced-scholarship",
   "metadata": {},
   "source": [
    "# Optimizing the bucketing process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "meaning-addition",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>x10</th>\n",
       "      <th>...</th>\n",
       "      <th>x15</th>\n",
       "      <th>x16</th>\n",
       "      <th>x17</th>\n",
       "      <th>x18</th>\n",
       "      <th>x19</th>\n",
       "      <th>x20</th>\n",
       "      <th>x21</th>\n",
       "      <th>x22</th>\n",
       "      <th>x23</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20000.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>689.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>120000.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3272.0</td>\n",
       "      <td>3455.0</td>\n",
       "      <td>3261.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90000.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>14331.0</td>\n",
       "      <td>14948.0</td>\n",
       "      <td>15549.0</td>\n",
       "      <td>1518.0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50000.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>28314.0</td>\n",
       "      <td>28959.0</td>\n",
       "      <td>29547.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>1069.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         x1   x2   x3   x4    x5   x6   x7   x8   x9  x10  ...      x15  \\\n",
       "0   20000.0  2.0  2.0  1.0  24.0  2.0  2.0 -1.0 -1.0 -2.0  ...      0.0   \n",
       "1  120000.0  2.0  2.0  2.0  26.0 -1.0  2.0  0.0  0.0  0.0  ...   3272.0   \n",
       "2   90000.0  2.0  2.0  2.0  34.0  0.0  0.0  0.0  0.0  0.0  ...  14331.0   \n",
       "3   50000.0  2.0  2.0  1.0  37.0  0.0  0.0  0.0  0.0  0.0  ...  28314.0   \n",
       "\n",
       "       x16      x17     x18     x19     x20     x21     x22     x23  y  \n",
       "0      0.0      0.0     0.0   689.0     0.0     0.0     0.0     0.0  1  \n",
       "1   3455.0   3261.0     0.0  1000.0  1000.0  1000.0     0.0  2000.0  1  \n",
       "2  14948.0  15549.0  1518.0  1500.0  1000.0  1000.0  1000.0  5000.0  0  \n",
       "3  28959.0  29547.0  2000.0  2019.0  1200.0  1100.0  1069.0  1000.0  0  \n",
       "\n",
       "[4 rows x 24 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "from skorecard.datasets import load_credit_card\n",
    "\n",
    "df = load_credit_card(as_frame=True)\n",
    "\n",
    "# Show\n",
    "display(df.head(4))\n",
    "\n",
    "num_feats = ['x1','x15','x16']\n",
    "\n",
    "X = df[num_feats]\n",
    "y = df['y']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "adult-shareware",
   "metadata": {},
   "source": [
    "# Finding the best bucketing\n",
    "\n",
    "The art of building a good scorecard model lies in finding the best bucketing strategy.<br>\n",
    "Good buckets improve the predicitve power of the model, as well as guarantee stability of the predictions.<br>\n",
    "\n",
    "This is normally a very manual, labour intensive process (and for a good reason).<br>\n",
    "\n",
    "A good bucketing strategy follows the following principles:\n",
    "- maximizes the Information Values, defined as \n",
    "\n",
    "$$IV = \\sum_{i}(\\%G_{i}-\\%B_{i}) \\cdot \\log(\\frac{\\%G_{i}}{\\%B_{i}})$$\n",
    "\n",
    "- avoids buckets that contain a very large or very small fraction of the population wherever the business sense requires it, \n",
    "\n",
    "The `skorecard` package provides some tooling to automate part of the process, namely:\n",
    "\n",
    "- Grid search the hyper-parameters of the bucketers in order to maximise the information value\n",
    "- Run the optimal bucketer within the bucketing process\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eligible-train",
   "metadata": {},
   "source": [
    "## Grid search the bucketers to maximise the information value\n",
    "\n",
    "`skorecard` implements an `IV_scorer`, that can be used as a custom scoring function for grid searching.<br>\n",
    "The following snippets of code show how to integrate it in the grid search.<br>\n",
    "The DecisionTreeBucketer applied on numerical features is the best use case, as there are some hyper-parameters that influence the bucketing quality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fitted-perspective",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorecard.metrics import IV_scorer\n",
    "from skorecard.bucketers import DecisionTreeBucketer\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "raised-trinity",
   "metadata": {},
   "source": [
    "The DecisionTreeBucketer has two main hyperparameters to grid-search:\n",
    "- `max_n_bins`, maximum number of bins allowed for the bucketing\n",
    "- `min_bin_size` minimum fraction of data in the buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "occupational-framing",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_params = {\n",
    "   \"max_n_bins\": [3, 4, 5, 6],\n",
    "   \"min_bin_size\": [0.05, 0.06, 0.07, 0.08], #, 0.12]\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "integrated-butter",
   "metadata": {},
   "source": [
    "The optimization has to be done for every feature indipendently, therefore we need a loop, and all the parameters are best stored in a data collector, like a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "strange-highland",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the specials\n",
    "best_params = dict()\n",
    "max_iv = dict()\n",
    "cv_results = dict()\n",
    "\n",
    "# Add a special for demo purposes\n",
    "specials = {'x1':{'special 0':['50000.0']}}\n",
    "\n",
    "for feat in num_feats:\n",
    "\n",
    "    # This snippet illustrates what to do with special values\n",
    "    if feat in specials.keys():\n",
    "        # This construct is needed to remap the specials, because skorecard validates that the key \n",
    "        # of the dictionary is present in the variables\n",
    "        special = {feat: specials[feat]}\n",
    "    else:\n",
    "        special = {}\n",
    "    bucketer = DecisionTreeBucketer(variables=[feat], specials=special)\n",
    "    gs = GridSearchCV(bucketer, gs_params, scoring=IV_scorer, cv=3, return_train_score=True)\n",
    "    gs.fit(X[[feat]], y)\n",
    "\n",
    "    best_params[feat] = gs.best_params_\n",
    "    max_iv[feat] = gs.best_score_\n",
    "    cv_results[feat] = gs.cv_results_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "other-macro",
   "metadata": {},
   "source": [
    "Checking the best parameters per feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "brilliant-smith",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x1': {'max_n_bins': 3, 'min_bin_size': 0.05},\n",
       " 'x15': {'max_n_bins': 3, 'min_bin_size': 0.05},\n",
       " 'x16': {'max_n_bins': 3, 'min_bin_size': 0.05}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bound-singer",
   "metadata": {},
   "source": [
    "Because of its additive nature, IV is likely to be maximal for the highest `max_n_bins`. \n",
    "Therefore it is worth looking analysing the CV results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "hired-mozambique",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.14118997, 0.13273303, 0.13474902, 0.15843304, 0.17114846,\n",
       "        0.1259594 , 0.12854441, 0.13791513, 0.14939396, 0.12906257,\n",
       "        0.15454125, 0.11709793, 0.1234947 , 0.11326059, 0.11524073,\n",
       "        0.11928709]),\n",
       " 'std_fit_time': array([0.01615798, 0.00538481, 0.00918157, 0.02513249, 0.02130305,\n",
       "        0.0088008 , 0.0078898 , 0.00226504, 0.01736914, 0.00537724,\n",
       "        0.04489044, 0.00418452, 0.00750423, 0.00055744, 0.00241629,\n",
       "        0.01126566]),\n",
       " 'mean_score_time': array([0.03244432, 0.03500628, 0.03295326, 0.04452038, 0.04895496,\n",
       "        0.03155041, 0.03200722, 0.03328069, 0.0405368 , 0.03386513,\n",
       "        0.02966809, 0.03014151, 0.03117593, 0.02836776, 0.02895562,\n",
       "        0.02856787]),\n",
       " 'std_score_time': array([0.00520814, 0.00130717, 0.00347241, 0.00365442, 0.01206228,\n",
       "        0.00173939, 0.00279055, 0.0009404 , 0.01539335, 0.00089022,\n",
       "        0.00115331, 0.00201206, 0.00159311, 0.00142928, 0.0013334 ,\n",
       "        0.00086222]),\n",
       " 'param_max_n_bins': masked_array(data=[3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 6, 6, 6, 6],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_min_bin_size': masked_array(data=[0.05, 0.06, 0.07, 0.08, 0.05, 0.06, 0.07, 0.08, 0.05,\n",
       "                    0.06, 0.07, 0.08, 0.05, 0.06, 0.07, 0.08],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'max_n_bins': 3, 'min_bin_size': 0.05},\n",
       "  {'max_n_bins': 3, 'min_bin_size': 0.06},\n",
       "  {'max_n_bins': 3, 'min_bin_size': 0.07},\n",
       "  {'max_n_bins': 3, 'min_bin_size': 0.08},\n",
       "  {'max_n_bins': 4, 'min_bin_size': 0.05},\n",
       "  {'max_n_bins': 4, 'min_bin_size': 0.06},\n",
       "  {'max_n_bins': 4, 'min_bin_size': 0.07},\n",
       "  {'max_n_bins': 4, 'min_bin_size': 0.08},\n",
       "  {'max_n_bins': 5, 'min_bin_size': 0.05},\n",
       "  {'max_n_bins': 5, 'min_bin_size': 0.06},\n",
       "  {'max_n_bins': 5, 'min_bin_size': 0.07},\n",
       "  {'max_n_bins': 5, 'min_bin_size': 0.08},\n",
       "  {'max_n_bins': 6, 'min_bin_size': 0.05},\n",
       "  {'max_n_bins': 6, 'min_bin_size': 0.06},\n",
       "  {'max_n_bins': 6, 'min_bin_size': 0.07},\n",
       "  {'max_n_bins': 6, 'min_bin_size': 0.08}],\n",
       " 'split0_test_score': array([0.079, 0.079, 0.079, 0.079, 0.097, 0.097, 0.097, 0.097, 0.106,\n",
       "        0.106, 0.106, 0.106, 0.107, 0.107, 0.107, 0.107]),\n",
       " 'split1_test_score': array([4.491, 4.491, 4.491, 4.491, 4.308, 4.308, 4.308, 4.308, 4.19 ,\n",
       "        4.19 , 4.19 , 4.19 , 4.043, 4.043, 4.043, 4.043]),\n",
       " 'split2_test_score': array([4.442, 4.442, 4.442, 4.442, 4.305, 4.305, 4.305, 4.305, 4.07 ,\n",
       "        4.07 , 4.07 , 4.07 , 3.975, 3.975, 3.975, 3.975]),\n",
       " 'mean_test_score': array([3.004     , 3.004     , 3.004     , 3.004     , 2.90333333,\n",
       "        2.90333333, 2.90333333, 2.90333333, 2.78866667, 2.78866667,\n",
       "        2.78866667, 2.78866667, 2.70833333, 2.70833333, 2.70833333,\n",
       "        2.70833333]),\n",
       " 'std_test_score': array([2.06838407, 2.06838407, 2.06838407, 2.06838407, 1.98437771,\n",
       "        1.98437771, 1.98437771, 1.98437771, 1.89756429, 1.89756429,\n",
       "        1.89756429, 1.89756429, 1.83962991, 1.83962991, 1.83962991,\n",
       "        1.83962991]),\n",
       " 'rank_test_score': array([ 1,  1,  1,  1,  5,  5,  5,  5,  9,  9,  9,  9, 13, 13, 13, 13],\n",
       "       dtype=int32),\n",
       " 'split0_train_score': array([0.049, 0.049, 0.049, 0.049, 0.049, 0.049, 0.049, 0.049, 0.05 ,\n",
       "        0.05 , 0.05 , 0.05 , 0.05 , 0.05 , 0.05 , 0.05 ]),\n",
       " 'split1_train_score': array([0.102, 0.102, 0.102, 0.102, 0.112, 0.112, 0.112, 0.112, 0.116,\n",
       "        0.116, 0.116, 0.116, 0.119, 0.119, 0.119, 0.119]),\n",
       " 'split2_train_score': array([0.119, 0.119, 0.119, 0.119, 0.144, 0.144, 0.144, 0.144, 0.156,\n",
       "        0.156, 0.156, 0.156, 0.159, 0.159, 0.159, 0.159]),\n",
       " 'mean_train_score': array([0.09      , 0.09      , 0.09      , 0.09      , 0.10166667,\n",
       "        0.10166667, 0.10166667, 0.10166667, 0.10733333, 0.10733333,\n",
       "        0.10733333, 0.10733333, 0.10933333, 0.10933333, 0.10933333,\n",
       "        0.10933333]),\n",
       " 'std_train_score': array([0.02981051, 0.02981051, 0.02981051, 0.02981051, 0.03946588,\n",
       "        0.03946588, 0.03946588, 0.03946588, 0.04370609, 0.04370609,\n",
       "        0.04370609, 0.04370609, 0.04502098, 0.04502098, 0.04502098,\n",
       "        0.04502098])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results['x1']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "internal-jurisdiction",
   "metadata": {},
   "source": [
    "## RandomizedSearchCV to maximise AUC\n",
    "As `Skorecard` is scikit-learn compatibile we can use scikit-learn methods such as RandomizedSearchCV to maximise the AUC of our model. Shown below is one such example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "boring-latino",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5,\n",
       "                   estimator=Pipeline(steps=[('bucketingprocess',\n",
       "                                              BucketingProcess(bucketing_pipeline=Pipeline(steps=[('optimalbucketer',\n",
       "                                                                                                   OptimalBucketer(min_bin_size=0.04))]),\n",
       "                                                               prebucketing_pipeline=Pipeline(steps=[('decisiontreebucketer',\n",
       "                                                                                                      DecisionTreeBucketer())]))),\n",
       "                                             ('woeencoder', WoeEncoder()),\n",
       "                                             ('logisticregression',\n",
       "                                              LogisticRegression(C=1.7,\n",
       "                                                                 max_iter=150,\n",
       "                                                                 random_state=0,\n",
       "                                                                 solver='liblinear'))]),\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions=[{'logisticregression__C': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f97de25c5d0>,\n",
       "                                         'logisticregression__solver': ['liblinear']}],\n",
       "                   random_state=0, scoring='roc_auc', verbose=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from skorecard.bucketers import DecisionTreeBucketer, OptimalBucketer\n",
    "from skorecard.pipeline import BucketingProcess\n",
    "from skorecard.linear_model import LogisticRegression\n",
    "from skorecard.preprocessing import WoeEncoder\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from scipy.stats import uniform\n",
    "\n",
    "\n",
    "def get_pipeline():\n",
    "    \n",
    "    bucketing_process = BucketingProcess(\n",
    "        prebucketing_pipeline=make_pipeline(\n",
    "                DecisionTreeBucketer(max_n_bins=100, min_bin_size=0.05),\n",
    "        ),\n",
    "        bucketing_pipeline=make_pipeline(\n",
    "                OptimalBucketer(max_n_bins=10, min_bin_size=0.04),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return make_pipeline(\n",
    "        bucketing_process,\n",
    "        WoeEncoder(),\n",
    "        LogisticRegression(solver=\"liblinear\", C=1.7, max_iter=150, random_state=0) \n",
    "    )\n",
    "\n",
    "\n",
    "pipe = get_pipeline()\n",
    "\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'logisticregression__C' : uniform(loc=0, scale=4),\n",
    "        'logisticregression__solver' : ['liblinear']\n",
    "    },\n",
    "]\n",
    "\n",
    "search_cv = RandomizedSearchCV(pipe, param_distributions = param_grid, cv = 5, verbose=True, scoring='roc_auc', n_jobs=-1, random_state=0, refit=True)\n",
    "search_cv.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "clean-means",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'logisticregression__C': 2.860757465489678,\n",
       "  'logisticregression__solver': 'liblinear'},\n",
       " 0.6187444445104318)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_cv.best_params_, search_cv.best_score_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dancard_py37] *",
   "language": "python",
   "name": "conda-env-dancard_py37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
